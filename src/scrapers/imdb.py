"""
IMDB ç½‘é¡µçˆ¬è™«
"""
import aiohttp
import asyncio
import re
import json
from datetime import datetime, date
from typing import List, Optional, Dict, Any
from bs4 import BeautifulSoup
from loguru import logger

from .base import WebScrapingBasedScraper, ScraperFactory
from ..models.anime import AnimeInfo, RatingData, WebsiteName, AnimeType, AnimeStatus, Season
from ..models.config import WebsiteConfig
from ..utils.season_utils import get_season_from_date


class IMDBScraper(WebScrapingBasedScraper):
    """IMDB ç½‘é¡µçˆ¬è™«"""
    
    def __init__(self, website_name: WebsiteName, config: WebsiteConfig):
        super().__init__(website_name, config)
        self.base_url = config.base_url or "https://www.imdb.com"
        
    def _parse_anime_type(self, imdb_type: str) -> Optional[AnimeType]:
        """è§£æIMDBç±»å‹"""
        if 'TV Series' in imdb_type or 'TV Mini Series' in imdb_type:
            return AnimeType.TV
        elif 'Movie' in imdb_type:
            return AnimeType.MOVIE
        elif 'TV Special' in imdb_type:
            return AnimeType.SPECIAL
        else:
            return AnimeType.TV
    
    def _parse_date(self, date_str: str) -> Optional[date]:
        """è§£æIMDBæ—¥æœŸ"""
        if not date_str:
            return None
        
        try:
            # IMDBæ—¥æœŸæ ¼å¼é€šå¸¸æ˜¯ "15 January 2024"
            return datetime.strptime(date_str.strip(), "%d %B %Y").date()
        except ValueError:
            try:
                # å°è¯•å…¶ä»–æ ¼å¼
                return datetime.strptime(date_str.strip(), "%Y").date().replace(month=1, day=1)
            except ValueError:
                logger.warning(f"Failed to parse IMDB date: {date_str}")
                return None
    
    def _extract_json_ld_data(self, html: str) -> Optional[Dict[str, Any]]:
        """æå–é¡µé¢ä¸­çš„JSON-LDç»“æ„åŒ–æ•°æ®"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾JSON-LDè„šæœ¬æ ‡ç­¾
        json_scripts = soup.find_all('script', type='application/ld+json')
        
        for script in json_scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict) and data.get('@type') in ['Movie', 'TVSeries']:
                    return data
                elif isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict) and item.get('@type') in ['Movie', 'TVSeries']:
                            return item
            except json.JSONDecodeError:
                continue
        
        return None
    
    def _extract_rating_from_page(self, html: str) -> Optional[Dict[str, Any]]:
        """ä»IMDBé¡µé¢æå–è¯„åˆ†ä¿¡æ¯"""
        # é¦–å…ˆå°è¯•ä»JSON-LDç»“æ„åŒ–æ•°æ®ä¸­æå–
        json_data = self._extract_json_ld_data(html)
        if json_data and 'aggregateRating' in json_data:
            rating_info = json_data['aggregateRating']
            try:
                raw_score = float(rating_info.get('ratingValue', 0))
                vote_count = int(rating_info.get('ratingCount', 0))

                if raw_score > 0 and vote_count > 0:
                    logger.debug(f"ä»JSON-LDæå–è¯„åˆ†: {raw_score}, æŠ•ç¥¨æ•°: {vote_count}")
                    return {
                        'score': raw_score,
                        'vote_count': vote_count,
                        'score_distribution': {}  # IMDBä¸æä¾›è¯¦ç»†åˆ†å¸ƒ
                    }
            except (ValueError, TypeError) as e:
                logger.warning(f"è§£æJSON-LDè¯„åˆ†æ•°æ®å¤±è´¥: {e}")

        # å¦‚æœJSON-LDå¤±è´¥ï¼Œå°è¯•ä»HTMLå…ƒç´ ä¸­æå–
        soup = BeautifulSoup(html, 'html.parser')

        # æŸ¥æ‰¾è¯„åˆ† - å°è¯•å¤šç§é€‰æ‹©å™¨
        rating_selectors = [
            'span[data-testid="hero-rating-bar__aggregate-rating__score"]',
            'span.sc-bde20123-1',
            '.rating-bar__base-button .ipc-button__text',
            '.AggregateRatingButton__RatingScore',
            '.ratingValue strong span'
        ]

        raw_score = None
        for selector in rating_selectors:
            rating_element = soup.select_one(selector)
            if rating_element:
                try:
                    rating_text = rating_element.text.strip()
                    raw_score = float(rating_text.split('/')[0])
                    logger.debug(f"ä»HTMLå…ƒç´ æå–è¯„åˆ†: {raw_score} (é€‰æ‹©å™¨: {selector})")
                    break
                except (ValueError, IndexError):
                    continue

        if raw_score is None:
            logger.warning("æœªèƒ½ä»é¡µé¢æå–è¯„åˆ†ä¿¡æ¯")
            return None

        # æŸ¥æ‰¾æŠ•ç¥¨æ•°
        vote_count = 0
        vote_selectors = [
            'div[data-testid="hero-rating-bar__aggregate-rating__vote-count"]',
            'div.sc-bde20123-3',
            '.rating-bar__base-button .ipc-button__text',
            '.AggregateRatingButton__TotalRatingAmount'
        ]

        for selector in vote_selectors:
            vote_element = soup.select_one(selector)
            if vote_element:
                vote_text = vote_element.text
                # æå–æ•°å­—ï¼Œå¯èƒ½åŒ…å«Kã€Mç­‰å•ä½
                vote_match = re.search(r'([\d.]+)([KM]?)', vote_text)
                if vote_match:
                    number = float(vote_match.group(1))
                    unit = vote_match.group(2)
                    if unit == 'K':
                        vote_count = int(number * 1000)
                    elif unit == 'M':
                        vote_count = int(number * 1000000)
                    else:
                        vote_count = int(number)
                    logger.debug(f"ä»HTMLå…ƒç´ æå–æŠ•ç¥¨æ•°: {vote_count} (é€‰æ‹©å™¨: {selector})")
                    break

        return {
            'score': raw_score,
            'vote_count': vote_count,
            'score_distribution': {}  # IMDBä¸æä¾›è¯¦ç»†åˆ†å¸ƒ
        }
    
    def _extract_anime_info_from_page(self, html: str, imdb_id: str) -> Optional[AnimeInfo]:
        """ä»IMDBé¡µé¢æå–åŠ¨æ¼«ä¿¡æ¯"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # å°è¯•ä»JSON-LDè·å–ç»“æ„åŒ–æ•°æ®
        json_data = self._extract_json_ld_data(html)
        
        # æ ‡é¢˜
        title = ''
        if json_data:
            title = json_data.get('name', '')
        
        if not title:
            title_element = soup.find('h1', {'data-testid': 'hero__pageTitle'})
            if title_element:
                title = title_element.text.strip()
        
        # ç±»å‹
        anime_type = None
        if json_data:
            imdb_type = json_data.get('@type', '')
            anime_type = self._parse_anime_type(imdb_type)
        
        # æ—¥æœŸ
        start_date = None
        if json_data and 'datePublished' in json_data:
            start_date = self._parse_date(json_data['datePublished'])
        
        # è§£æå­£åº¦
        season = None
        year = None
        if start_date:
            season, year = get_season_from_date(start_date)
        
        # ç±»å‹æ ‡ç­¾
        genres = []
        if json_data and 'genre' in json_data:
            if isinstance(json_data['genre'], list):
                genres = json_data['genre']
            elif isinstance(json_data['genre'], str):
                genres = [json_data['genre']]
        
        # ç®€ä»‹
        synopsis = ''
        if json_data and 'description' in json_data:
            synopsis = json_data['description']
        
        anime_info = AnimeInfo(
            title=title,
            anime_type=anime_type,
            start_date=start_date,
            season=season,
            year=year,
            genres=genres,
            external_ids={WebsiteName.IMDB: imdb_id},
            synopsis=synopsis
        )
        
        return anime_info
    
    async def search_anime(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """æœç´¢åŠ¨æ¼«"""
        logger.info(f"ğŸ” IMDBæœç´¢åŠ¨æ¼«: {title}")

        search_url = f"{self.base_url}/find"
        params = {
            'q': title,
            's': 'tt',  # æœç´¢æ ‡é¢˜
            'ttype': 'tv'  # é™åˆ¶ä¸ºTVå†…å®¹
        }

        response = await self._make_request(
            session, search_url, params=params, headers=self._get_default_headers()
        )

        if not response or 'text' not in response:
            logger.warning("IMDBæœç´¢è¯·æ±‚å¤±è´¥")
            return []

        soup = BeautifulSoup(response['text'], 'html.parser')
        results = []

        # ä½¿ç”¨æ­£ç¡®çš„é€‰æ‹©å™¨è§£ææœç´¢ç»“æœ
        result_selectors = [
            '.find-title-result',
            '.ipc-metadata-list-summary-item',
            'tr.findResult'
        ]

        result_items = []
        for selector in result_selectors:
            items = soup.select(selector)
            if items:
                logger.debug(f"ä½¿ç”¨é€‰æ‹©å™¨ {selector} æ‰¾åˆ° {len(items)} ä¸ªç»“æœ")
                result_items = items
                break

        if not result_items:
            logger.warning("æœªæ‰¾åˆ°æœç´¢ç»“æœ")
            return []

        for item in result_items[:5]:  # é™åˆ¶ç»“æœæ•°é‡
            try:
                # æŸ¥æ‰¾é“¾æ¥
                link_element = item.find('a')
                if not link_element:
                    continue

                href = link_element.get('href', '')
                imdb_id_match = re.search(r'/title/(tt\d+)/', href)
                if not imdb_id_match:
                    continue

                imdb_id = imdb_id_match.group(1)
                logger.debug(f"æ‰¾åˆ°IMDB ID: {imdb_id}")

                # è·å–è¯¦ç»†ä¿¡æ¯
                anime_info = await self.get_anime_details(session, imdb_id)
                if anime_info:
                    results.append(anime_info)
                    logger.debug(f"æˆåŠŸè·å–åŠ¨æ¼«ä¿¡æ¯: {anime_info.title}")

                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(1)

            except Exception as e:
                logger.warning(f"è§£æIMDBæœç´¢ç»“æœå¤±è´¥: {e}")
                continue

        logger.info(f"IMDBæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªæœ‰æ•ˆç»“æœ")
        return results
    
    async def get_anime_details(self, session: aiohttp.ClientSession, anime_id: str) -> Optional[AnimeInfo]:
        """è·å–åŠ¨æ¼«è¯¦ç»†ä¿¡æ¯"""
        url = f"{self.base_url}/title/{anime_id}/"
        
        response = await self._make_request(
            session, url, headers=self._get_default_headers()
        )
        
        if not response or 'text' not in response:
            return None
        
        try:
            return self._extract_anime_info_from_page(response['text'], anime_id)
        except Exception as e:
            logger.error(f"Failed to parse IMDB anime details for {anime_id}: {e}")
            return None
    
    async def get_anime_rating(self, session: aiohttp.ClientSession, anime_id: str) -> Optional[RatingData]:
        """è·å–åŠ¨æ¼«è¯„åˆ†æ•°æ®"""
        url = f"{self.base_url}/title/{anime_id}/"
        
        response = await self._make_request(
            session, url, headers=self._get_default_headers()
        )
        
        if not response or 'text' not in response:
            return None
        
        rating_data = self._extract_rating_from_page(response['text'])
        if not rating_data:
            return None
        
        rating = RatingData(
            website=WebsiteName.IMDB,
            raw_score=rating_data['score'],
            vote_count=rating_data['vote_count'],
            score_distribution=rating_data['score_distribution'],
            site_mean=None,  # éœ€è¦ä»ç½‘ç«™ç»Ÿè®¡ä¸­è·å–
            site_std=None,   # éœ€è¦è®¡ç®—
            last_updated=datetime.now(),
            url=url
        )
        
        return rating
    
    async def get_seasonal_anime(self, session: aiohttp.ClientSession, year: int, season: str) -> List[AnimeInfo]:
        """è·å–å­£åº¦åŠ¨æ¼«åˆ—è¡¨"""
        # IMDBæ²¡æœ‰ç›´æ¥çš„å­£åº¦åŠ¨æ¼«API
        logger.info(f"IMDB does not provide seasonal anime API for {season} {year}")
        return []
    
    async def get_site_statistics(self, session: aiohttp.ClientSession) -> Optional[Dict[str, float]]:
        """è·å–ç½‘ç«™ç»Ÿè®¡æ•°æ®"""
        logger.info("Getting IMDB site statistics...")
        
        # IMDBè¯„åˆ†ç‰¹ç‚¹çš„ä¼°ç®—å€¼
        return {
            'mean': 7.0,  # IMDBå¹³å‡åˆ†ç›¸å¯¹è¾ƒä½
            'std': 1.2    # æ ‡å‡†å·®è¾ƒå¤§
        }


# æ³¨å†ŒIMDBçˆ¬è™«
ScraperFactory.register_scraper(WebsiteName.IMDB, IMDBScraper)
