"""
è±†ç“£ç½‘é¡µçˆ¬è™«
"""
import aiohttp
import asyncio
import re
import json
from datetime import datetime, date
from typing import List, Optional, Dict, Any
from bs4 import BeautifulSoup
from loguru import logger

from .base import WebScrapingBasedScraper, ScraperFactory
from ..models.anime import AnimeInfo, RatingData, WebsiteName, AnimeType, AnimeStatus, Season
from ..models.config import WebsiteConfig
from ..utils.season_utils import get_season_from_date


class DoubanScraper(WebScrapingBasedScraper):
    """è±†ç“£ç½‘é¡µçˆ¬è™«"""
    
    def __init__(self, website_name: WebsiteName, config: WebsiteConfig):
        super().__init__(website_name, config)
        self.base_url = config.base_url or "https://movie.douban.com"
        
    def _parse_anime_type(self, douban_type: str) -> Optional[AnimeType]:
        """è§£æè±†ç“£åŠ¨æ¼«ç±»å‹"""
        if 'ç”µè§†' in douban_type or 'TV' in douban_type:
            return AnimeType.TV
        elif 'ç”µå½±' in douban_type or 'å‰§åœºç‰ˆ' in douban_type:
            return AnimeType.MOVIE
        elif 'OVA' in douban_type:
            return AnimeType.OVA
        elif 'ONA' in douban_type:
            return AnimeType.ONA
        elif 'ç‰¹åˆ«ç¯‡' in douban_type or 'ç‰¹å…¸' in douban_type:
            return AnimeType.SPECIAL
        else:
            return AnimeType.TV  # é»˜è®¤ä¸ºTV
    
    def _parse_date(self, date_str: str) -> Optional[date]:
        """è§£æè±†ç“£æ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return None
        
        # è±†ç“£æ—¥æœŸæ ¼å¼å¤šæ ·ï¼Œå°è¯•å¤šç§è§£ææ–¹å¼
        date_patterns = [
            r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2024-01-15
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 2024å¹´1æœˆ15æ—¥
            r'(\d{4})å¹´(\d{1,2})æœˆ',  # 2024å¹´1æœˆ
            r'(\d{4})',  # 2024
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, date_str)
            if match:
                try:
                    groups = match.groups()
                    year = int(groups[0])
                    month = int(groups[1]) if len(groups) > 1 else 1
                    day = int(groups[2]) if len(groups) > 2 else 1
                    return date(year, month, day)
                except ValueError:
                    continue
        
        logger.warning(f"Failed to parse Douban date: {date_str}")
        return None
    
    def _extract_rating_from_page(self, html: str) -> Optional[Dict[str, Any]]:
        """ä»è±†ç“£é¡µé¢æå–è¯„åˆ†ä¿¡æ¯"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾è¯„åˆ†
        rating_element = soup.find('strong', class_='ll rating_num')
        if not rating_element:
            return None
        
        try:
            raw_score = float(rating_element.text.strip())
        except ValueError:
            return None
        
        # æŸ¥æ‰¾è¯„åˆ†äººæ•°
        vote_count = 0
        rating_people = soup.find('a', class_='rating_people')
        if rating_people:
            vote_text = rating_people.text
            vote_match = re.search(r'(\d+)', vote_text)
            if vote_match:
                vote_count = int(vote_match.group(1))
        
        # æŸ¥æ‰¾è¯„åˆ†åˆ†å¸ƒ
        score_distribution = {}
        rating_per = soup.find_all('span', class_='rating_per')
        if len(rating_per) == 5:
            # è±†ç“£æ˜¯5æ˜Ÿåˆ¶ï¼Œè½¬æ¢ä¸º10åˆ†åˆ¶
            for i, per in enumerate(rating_per):
                score = 10 - i * 2  # 5æ˜Ÿ=10åˆ†, 4æ˜Ÿ=8åˆ†, ..., 1æ˜Ÿ=2åˆ†
                percent_text = per.text.strip().replace('%', '')
                try:
                    percent = float(percent_text)
                    count = int(vote_count * percent / 100)
                    score_distribution[str(score)] = count
                except ValueError:
                    continue
        
        return {
            'score': raw_score,
            'vote_count': vote_count,
            'score_distribution': score_distribution
        }
    
    def _extract_anime_info_from_page(self, html: str, douban_id: str) -> Optional[AnimeInfo]:
        """ä»è±†ç“£é¡µé¢æå–åŠ¨æ¼«ä¿¡æ¯"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # æ ‡é¢˜
        title_element = soup.find('span', property='v:itemreviewed')
        title = title_element.text.strip() if title_element else ''
        
        # åŸºæœ¬ä¿¡æ¯
        info_element = soup.find('div', id='info')
        if not info_element:
            return None
        
        info_text = info_element.get_text()
        
        # è§£æåŸºæœ¬ä¿¡æ¯
        anime_type = None
        episodes = None
        start_date = None
        studios = []
        genres = []
        
        # æŸ¥æ‰¾ç±»å‹
        type_match = re.search(r'ç±»å‹:\s*([^\n]+)', info_text)
        if type_match:
            type_str = type_match.group(1).strip()
            anime_type = self._parse_anime_type(type_str)
            genres = [g.strip() for g in type_str.split('/') if g.strip()]
        
        # æŸ¥æ‰¾é›†æ•°
        episodes_match = re.search(r'é›†æ•°:\s*(\d+)', info_text)
        if episodes_match:
            episodes = int(episodes_match.group(1))
        
        # æŸ¥æ‰¾é¦–æ’­æ—¥æœŸ
        date_match = re.search(r'é¦–æ’­:\s*([^\n]+)', info_text)
        if date_match:
            start_date = self._parse_date(date_match.group(1).strip())
        
        # æŸ¥æ‰¾åˆ¶ä½œå…¬å¸
        studio_match = re.search(r'åˆ¶ç‰‡å›½å®¶/åœ°åŒº:\s*([^\n]+)', info_text)
        if studio_match:
            studios = [s.strip() for s in studio_match.group(1).split('/') if s.strip()]
        
        # è§£æå­£åº¦
        season = None
        year = None
        if start_date:
            season, year = get_season_from_date(start_date)
        
        # ç®€ä»‹
        synopsis = ''
        summary_element = soup.find('span', property='v:summary')
        if summary_element:
            synopsis = summary_element.get_text().strip()
        
        anime_info = AnimeInfo(
            title=title,
            anime_type=anime_type,
            episodes=episodes,
            start_date=start_date,
            season=season,
            year=year,
            studios=studios,
            genres=genres,
            external_ids={WebsiteName.DOUBAN: douban_id},
            synopsis=synopsis
        )
        
        return anime_info
    
    async def search_anime(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """æœç´¢åŠ¨æ¼«"""
        logger.info(f"ğŸ” è±†ç“£æœç´¢åŠ¨æ¼«: {title}")

        # ä½¿ç”¨æ­£ç¡®çš„æœç´¢URL
        search_url = "https://search.douban.com/movie/subject_search"
        params = {
            'search_text': title,
            'cat': '1002'  # åŠ¨æ¼«åˆ†ç±»
        }

        response = await self._make_request(
            session, search_url, params=params, headers=self._get_default_headers()
        )

        if not response or 'text' not in response:
            logger.warning("è±†ç“£æœç´¢è¯·æ±‚å¤±è´¥")
            return []

        # ä»JavaScriptæ•°æ®ä¸­æå–æœç´¢ç»“æœ
        html = response['text']
        results = []

        try:
            # æŸ¥æ‰¾ window.__DATA__ ä¸­çš„æ•°æ®
            data_match = re.search(r'window\.__DATA__\s*=\s*({.*?});', html, re.DOTALL)
            if data_match:
                import json
                data = json.loads(data_match.group(1))
                items = data.get('items', [])

                logger.debug(f"è±†ç“£æœç´¢æ‰¾åˆ° {len(items)} ä¸ªç»“æœ")

                for item in items[:5]:  # é™åˆ¶ç»“æœæ•°é‡
                    try:
                        douban_id = str(item.get('id', ''))
                        if not douban_id:
                            continue

                        logger.debug(f"æ‰¾åˆ°è±†ç“£ID: {douban_id}")

                        # è·å–è¯¦ç»†ä¿¡æ¯
                        anime_info = await self.get_anime_details(session, douban_id)
                        if anime_info:
                            results.append(anime_info)
                            logger.debug(f"æˆåŠŸè·å–åŠ¨æ¼«ä¿¡æ¯: {anime_info.title}")

                        # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                        await asyncio.sleep(1)

                    except Exception as e:
                        logger.warning(f"è§£æè±†ç“£æœç´¢ç»“æœå¤±è´¥: {e}")
                        continue
            else:
                logger.warning("æœªæ‰¾åˆ°è±†ç“£æœç´¢æ•°æ®")

        except Exception as e:
            logger.error(f"è§£æè±†ç“£æœç´¢å“åº”å¤±è´¥: {e}")

        logger.info(f"è±†ç“£æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªæœ‰æ•ˆç»“æœ")
        return results
    
    async def get_anime_details(self, session: aiohttp.ClientSession, anime_id: str) -> Optional[AnimeInfo]:
        """è·å–åŠ¨æ¼«è¯¦ç»†ä¿¡æ¯"""
        url = f"{self.base_url}/subject/{anime_id}/"
        
        response = await self._make_request(
            session, url, headers=self._get_default_headers()
        )
        
        if not response or 'text' not in response:
            return None
        
        try:
            return self._extract_anime_info_from_page(response['text'], anime_id)
        except Exception as e:
            logger.error(f"Failed to parse Douban anime details for {anime_id}: {e}")
            return None
    
    async def get_anime_rating(self, session: aiohttp.ClientSession, anime_id: str) -> Optional[RatingData]:
        """è·å–åŠ¨æ¼«è¯„åˆ†æ•°æ®"""
        url = f"{self.base_url}/subject/{anime_id}/"
        
        response = await self._make_request(
            session, url, headers=self._get_default_headers()
        )
        
        if not response or 'text' not in response:
            return None
        
        rating_data = self._extract_rating_from_page(response['text'])
        if not rating_data:
            return None
        
        rating = RatingData(
            website=WebsiteName.DOUBAN,
            raw_score=rating_data['score'],
            vote_count=rating_data['vote_count'],
            score_distribution=rating_data['score_distribution'],
            site_mean=None,  # éœ€è¦ä»ç½‘ç«™ç»Ÿè®¡ä¸­è·å–
            site_std=None,   # éœ€è¦è®¡ç®—
            last_updated=datetime.now(),
            url=url
        )
        
        return rating
    
    async def get_seasonal_anime(self, session: aiohttp.ClientSession, year: int, season: str) -> List[AnimeInfo]:
        """è·å–å­£åº¦åŠ¨æ¼«åˆ—è¡¨"""
        # è±†ç“£æ²¡æœ‰ç›´æ¥çš„å­£åº¦åŠ¨æ¼«APIï¼Œè¿™é‡Œè¿”å›ç©ºåˆ—è¡¨
        # å®é™…ä½¿ç”¨ä¸­å¯èƒ½éœ€è¦é€šè¿‡å…¶ä»–æ–¹å¼è·å–å­£åº¦åŠ¨æ¼«åˆ—è¡¨
        logger.info(f"Douban does not provide seasonal anime API for {season} {year}")
        return []
    
    async def get_site_statistics(self, session: aiohttp.ClientSession) -> Optional[Dict[str, float]]:
        """è·å–ç½‘ç«™ç»Ÿè®¡æ•°æ®"""
        logger.info("Getting Douban site statistics...")
        
        # è±†ç“£åŠ¨æ¼«è¯„åˆ†ç‰¹ç‚¹çš„ä¼°ç®—å€¼
        return {
            'mean': 8.0,  # è±†ç“£è¯„åˆ†é€šå¸¸è¾ƒé«˜
            'std': 0.7    # æ ‡å‡†å·®ä¸­ç­‰
        }


# æ³¨å†Œè±†ç“£çˆ¬è™«
ScraperFactory.register_scraper(WebsiteName.DOUBAN, DoubanScraper)
