#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆè±†ç“£çˆ¬è™« - ååçˆ¬è™«ç‰ˆæœ¬
ä½¿ç”¨å¤šç§æŠ€æœ¯ç»•è¿‡è±†ç“£çš„åçˆ¬è™«æœºåˆ¶
"""
import asyncio
import aiohttp
import random
import time
import json
import re
from typing import Dict, List, Optional, Any
from datetime import datetime, date
from bs4 import BeautifulSoup

from ..models.anime import AnimeInfo, RatingData, WebsiteName, AnimeType
from ..models.config import WebsiteConfig
from .base import WebScrapingBasedScraper
from loguru import logger


class DoubanEnhancedScraper(WebScrapingBasedScraper):
    """å¢å¼ºç‰ˆè±†ç“£çˆ¬è™« - ååçˆ¬è™«ç‰ˆæœ¬"""
    
    def __init__(self, website_name: WebsiteName, config: WebsiteConfig):
        super().__init__(website_name, config)
        self.base_url = config.base_url or "https://movie.douban.com"
        self.session_cookies = {}
        self.last_request_time = 0
        
        # ç”¨æˆ·ä»£ç†æ± 
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
        ]
    
    def _get_random_headers(self, referer: Optional[str] = None) -> Dict[str, str]:
        """è·å–éšæœºåŒ–çš„è¯·æ±‚å¤´"""
        headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',  # ç§»é™¤bré¿å…Brotlié—®é¢˜
            'Cache-Control': 'max-age=0',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        if referer:
            headers['Referer'] = referer
            headers['Sec-Fetch-Site'] = 'same-origin'
        
        return headers
    
    async def _smart_delay(self):
        """æ™ºèƒ½å»¶è¿Ÿ - æ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        # æœ€å°é—´éš”3-8ç§’ï¼Œæ¨¡æ‹Ÿäººç±»æµè§ˆ
        min_delay = random.uniform(3.0, 8.0)
        
        if time_since_last < min_delay:
            delay = min_delay - time_since_last
            await asyncio.sleep(delay)
        
        self.last_request_time = time.time()

    async def _handle_security_redirect(self, session: aiohttp.ClientSession, response) -> Optional[Dict[str, Any]]:
        """å¤„ç†è±†ç“£å®‰å…¨éªŒè¯é‡å®šå‘"""
        response_url = str(response.url)

        if 'sec.douban.com' in response_url:
            logger.warning("æ£€æµ‹åˆ°è±†ç“£å®‰å…¨éªŒè¯é¡µé¢ï¼Œå°è¯•å¤„ç†...")

            try:
                html = await response.text()

                # æŸ¥æ‰¾è‡ªåŠ¨è·³è½¬çš„ç›®æ ‡URL
                redirect_match = re.search(r'location\.href\s*=\s*["\']([^"\']+)["\']', html)
                if redirect_match:
                    target_url = redirect_match.group(1)
                    logger.info(f"å‘ç°è‡ªåŠ¨è·³è½¬ç›®æ ‡: {target_url}")

                    # ç­‰å¾…ä¸€æ®µæ—¶é—´æ¨¡æ‹Ÿäººç±»è¡Œä¸º
                    await asyncio.sleep(random.uniform(2, 5))

                    # è®¿é—®è·³è½¬ç›®æ ‡
                    return await self._make_enhanced_request(
                        session, target_url, referer=response_url
                    )

                # æŸ¥æ‰¾è¡¨å•æäº¤
                form_match = re.search(r'<form[^>]*action=["\']([^"\']+)["\'][^>]*>', html)
                if form_match:
                    form_action = form_match.group(1)
                    logger.info(f"å‘ç°è¡¨å•æäº¤: {form_action}")

                    # è¿™é‡Œå¯ä»¥æ·»åŠ è¡¨å•å¤„ç†é€»è¾‘
                    # æš‚æ—¶è¿”å›Noneï¼Œè¡¨ç¤ºæ— æ³•å¤„ç†

                logger.warning("æ— æ³•è‡ªåŠ¨å¤„ç†å®‰å…¨éªŒè¯é¡µé¢")
                return None

            except Exception as e:
                logger.error(f"å¤„ç†å®‰å…¨éªŒè¯é¡µé¢æ—¶å‡ºé”™: {e}")
                return None

        # ä¸æ˜¯å®‰å…¨éªŒè¯é¡µé¢ï¼Œæ­£å¸¸è¿”å›
        if response.status == 200:
            text = await response.text()
            return {"text": text, "status": response.status}

        return None
    
    async def _make_enhanced_request(self, session: aiohttp.ClientSession, 
                                   url: str, method: str = "GET",
                                   headers: Optional[Dict[str, str]] = None,
                                   params: Optional[Dict[str, Any]] = None,
                                   referer: Optional[str] = None,
                                   max_retries: int = 3) -> Optional[Dict[str, Any]]:
        """å¢å¼ºç‰ˆHTTPè¯·æ±‚ - åŒ…å«ååçˆ¬è™«æœºåˆ¶"""
        
        for attempt in range(max_retries):
            try:
                # æ™ºèƒ½å»¶è¿Ÿ
                await self._smart_delay()
                
                # è·å–éšæœºåŒ–è¯·æ±‚å¤´
                request_headers = headers or self._get_random_headers(referer)
                
                # æ·»åŠ ä¼šè¯cookies
                cookies = self.session_cookies.copy()
                
                timeout = aiohttp.ClientTimeout(total=30)
                
                logger.debug(f"å°è¯•è¯·æ±‚ {url} (ç¬¬{attempt+1}æ¬¡)")
                
                async with session.request(
                    method=method,
                    url=url,
                    headers=request_headers,
                    params=params,
                    cookies=cookies,
                    timeout=timeout,
                    ssl=False  # å¿½ç•¥SSLéªŒè¯
                ) as response:
                    
                    # æ›´æ–°cookies
                    if response.cookies:
                        try:
                            for cookie in response.cookies:
                                if hasattr(cookie, 'key') and hasattr(cookie, 'value'):
                                    self.session_cookies[cookie.key] = cookie.value
                                else:
                                    # å¤„ç†ä¸åŒçš„cookieæ ¼å¼
                                    self.session_cookies[str(cookie)] = str(cookie)
                        except Exception as e:
                            logger.debug(f"Cookieå¤„ç†é”™è¯¯: {e}")
                    
                    logger.debug(f"å“åº”çŠ¶æ€ç : {response.status}")

                    if response.status == 200:
                        # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°å®‰å…¨éªŒè¯é¡µé¢
                        security_result = await self._handle_security_redirect(session, response)
                        if security_result:
                            return security_result

                        # æ­£å¸¸å¤„ç†å“åº”
                        if 'application/json' in response.headers.get('content-type', ''):
                            return await response.json()
                        else:
                            text = await response.text()
                            return {"text": text, "status": response.status}
                    
                    elif response.status == 403:
                        logger.warning(f"403é”™è¯¯ï¼Œå¯èƒ½è¢«åçˆ¬è™«æ£€æµ‹åˆ°ï¼Œå°è¯•{attempt+1}/{max_retries}")
                        if attempt < max_retries - 1:
                            # å¢åŠ å»¶è¿Ÿæ—¶é—´
                            await asyncio.sleep(random.uniform(10, 20))
                            continue
                        else:
                            logger.error("å¤šæ¬¡403é”™è¯¯ï¼Œå¯èƒ½éœ€è¦æ›´æ¢ç­–ç•¥")
                            return None
                    
                    elif response.status == 429:
                        logger.warning(f"è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œç­‰å¾…åé‡è¯•")
                        await asyncio.sleep(random.uniform(30, 60))
                        continue
                    
                    else:
                        logger.warning(f"è¯·æ±‚å¤±è´¥: {response.status} - {url}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(random.uniform(5, 10))
                            continue
                        return None
                        
            except asyncio.TimeoutError:
                logger.warning(f"è¯·æ±‚è¶…æ—¶: {url}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(random.uniform(5, 10))
                    continue
                return None
                
            except Exception as e:
                logger.error(f"è¯·æ±‚å¼‚å¸¸: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(random.uniform(5, 10))
                    continue
                return None
        
        return None
    
    async def _init_session(self, session: aiohttp.ClientSession) -> bool:
        """åˆå§‹åŒ–ä¼šè¯ - è·å–å¿…è¦çš„cookies"""
        logger.info("ğŸ”§ åˆå§‹åŒ–è±†ç“£ä¼šè¯...")
        
        # é¦–å…ˆè®¿é—®è±†ç“£é¦–é¡µè·å–åŸºç¡€cookies
        home_response = await self._make_enhanced_request(
            session, "https://www.douban.com", referer=None
        )
        
        if not home_response:
            logger.warning("æ— æ³•è®¿é—®è±†ç“£é¦–é¡µ")
            return False
        
        # è®¿é—®ç”µå½±é¦–é¡µ
        movie_response = await self._make_enhanced_request(
            session, "https://movie.douban.com", referer="https://www.douban.com"
        )
        
        if not movie_response:
            logger.warning("æ— æ³•è®¿é—®è±†ç“£ç”µå½±é¦–é¡µ")
            return False
        
        logger.info("âœ… è±†ç“£ä¼šè¯åˆå§‹åŒ–æˆåŠŸ")
        return True
    
    async def search_anime_with_mobile_api(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """ä½¿ç”¨ç§»åŠ¨ç«¯APIæœç´¢åŠ¨æ¼«"""
        logger.info(f"ğŸ“± ä½¿ç”¨ç§»åŠ¨ç«¯APIæœç´¢: {title}")
        
        # ç§»åŠ¨ç«¯APIé€šå¸¸åçˆ¬è™«è¾ƒå¼±
        mobile_headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Referer': 'https://m.douban.com/',
        }
        
        # å°è¯•ç§»åŠ¨ç«¯æœç´¢API
        mobile_search_url = "https://m.douban.com/rexxar/api/v2/search"
        params = {
            'q': title,
            'type': 'movie',
            'count': 10
        }
        
        response = await self._make_enhanced_request(
            session, mobile_search_url, headers=mobile_headers, params=params,
            referer="https://m.douban.com/"
        )
        
        if response and 'items' in response:
            logger.info(f"ç§»åŠ¨ç«¯APIæœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(response['items'])} ä¸ªç»“æœ")
            # è¿™é‡Œéœ€è¦è§£æç§»åŠ¨ç«¯APIçš„å“åº”æ ¼å¼
            return []  # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œéœ€è¦æ ¹æ®å®é™…APIå“åº”æ ¼å¼å®ç°
        
        logger.warning("ç§»åŠ¨ç«¯APIæœç´¢å¤±è´¥")
        return []
    
    async def search_anime_with_proxy(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """ä½¿ç”¨ä»£ç†æœç´¢åŠ¨æ¼«"""
        logger.info(f"ğŸŒ ä½¿ç”¨ä»£ç†æœç´¢: {title}")
        
        # è¿™é‡Œå¯ä»¥é…ç½®ä»£ç†æœåŠ¡å™¨
        # æ³¨æ„ï¼šéœ€è¦æœ‰æ•ˆçš„ä»£ç†æœåŠ¡å™¨é…ç½®
        proxy_url = None  # "http://proxy-server:port"
        
        if not proxy_url:
            logger.warning("æœªé…ç½®ä»£ç†æœåŠ¡å™¨")
            return []
        
        # ä½¿ç”¨ä»£ç†çš„è¯·æ±‚é€»è¾‘
        # å®é™…å®ç°éœ€è¦æ ¹æ®ä»£ç†æœåŠ¡å™¨é…ç½®
        return []

    async def search_anime_with_selenium(self, title: str) -> List[AnimeInfo]:
        """ä½¿ç”¨Seleniumæ¨¡æ‹Ÿæµè§ˆå™¨æœç´¢"""
        logger.info(f"ğŸ¤– ä½¿ç”¨Seleniumæœç´¢: {title}")

        try:
            from selenium import webdriver
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from selenium.webdriver.chrome.options import Options

            # é…ç½®Chromeé€‰é¡¹
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)

            # éšæœºUser-Agent
            chrome_options.add_argument(f'--user-agent={random.choice(self.user_agents)}')

            driver = webdriver.Chrome(options=chrome_options)
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            try:
                # è®¿é—®è±†ç“£æœç´¢é¡µé¢
                search_url = f"https://movie.douban.com/search?q={title}"
                driver.get(search_url)

                # ç­‰å¾…é¡µé¢åŠ è½½
                wait = WebDriverWait(driver, 10)

                # æŸ¥æ‰¾æœç´¢ç»“æœ
                results = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '.result')))

                anime_list = []
                for result in results[:5]:  # é™åˆ¶ç»“æœæ•°é‡
                    try:
                        # æå–åŠ¨æ¼«ä¿¡æ¯
                        title_element = result.find_element(By.CSS_SELECTOR, '.title a')
                        anime_title = title_element.text
                        anime_url = title_element.get_attribute('href')

                        # æå–è±†ç“£ID
                        douban_id = re.search(r'/subject/(\d+)/', anime_url)
                        if douban_id:
                            douban_id = douban_id.group(1)

                            # åˆ›å»ºAnimeInfoå¯¹è±¡
                            anime_info = AnimeInfo(
                                title=anime_title,
                                external_ids={WebsiteName.DOUBAN: douban_id}
                            )
                            anime_list.append(anime_info)

                    except Exception as e:
                        logger.warning(f"è§£ææœç´¢ç»“æœå¤±è´¥: {e}")
                        continue

                return anime_list

            finally:
                driver.quit()

        except ImportError:
            logger.warning("Seleniumæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æµè§ˆå™¨æ¨¡æ‹Ÿ")
            return []
        except Exception as e:
            logger.error(f"Seleniumæœç´¢å¤±è´¥: {e}")
            return []

    async def search_anime_alternative_sites(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """ä½¿ç”¨æ›¿ä»£ç½‘ç«™æœç´¢è±†ç“£ID"""
        logger.info(f"ğŸ”„ ä½¿ç”¨æ›¿ä»£ç½‘ç«™æœç´¢è±†ç“£ID: {title}")

        # å¯ä»¥ä½¿ç”¨å…¶ä»–ç½‘ç«™çš„è±†ç“£é“¾æ¥æ¥è·å–è±†ç“£ID
        # ä¾‹å¦‚ï¼šæŸäº›å½±è¯„ç½‘ç«™ã€èšåˆç½‘ç«™ç­‰

        # ç¤ºä¾‹ï¼šä½¿ç”¨ç™¾åº¦æœç´¢è±†ç“£é“¾æ¥
        baidu_search_url = "https://www.baidu.com/s"
        params = {
            'wd': f'site:movie.douban.com {title}',
            'rn': 10
        }

        headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9',
        }

        response = await self._make_enhanced_request(
            session, baidu_search_url, params=params, headers=headers
        )

        if response and 'text' in response:
            # ä»ç™¾åº¦æœç´¢ç»“æœä¸­æå–è±†ç“£é“¾æ¥
            douban_links = re.findall(r'https://movie\.douban\.com/subject/(\d+)/', response['text'])

            anime_list = []
            for douban_id in douban_links[:3]:  # é™åˆ¶æ•°é‡
                anime_info = AnimeInfo(
                    title=title,  # ä½¿ç”¨æœç´¢å…³é”®è¯ä½œä¸ºä¸´æ—¶æ ‡é¢˜
                    external_ids={WebsiteName.DOUBAN: douban_id}
                )
                anime_list.append(anime_info)

            if anime_list:
                logger.info(f"é€šè¿‡æ›¿ä»£ç½‘ç«™æ‰¾åˆ° {len(anime_list)} ä¸ªè±†ç“£ID")
                return anime_list

        logger.warning("æ›¿ä»£ç½‘ç«™æœç´¢å¤±è´¥")
        return []

    async def search_anime(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """æœç´¢åŠ¨æ¼« - å¤šç­–ç•¥å°è¯•"""
        logger.info(f"ğŸ” è±†ç“£å¢å¼ºæœç´¢: {title}")

        # åˆå§‹åŒ–ä¼šè¯
        if not await self._init_session(session):
            logger.warning("ä¼šè¯åˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•")

        # ç­–ç•¥1: å°è¯•åŸå§‹æœç´¢æ–¹æ³•
        try:
            results = await self._try_original_search(session, title)
            if results:
                logger.info(f"åŸå§‹æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                return results
        except Exception as e:
            logger.warning(f"åŸå§‹æœç´¢å¤±è´¥: {e}")

        # ç­–ç•¥2: ç§»åŠ¨ç«¯API
        try:
            results = await self.search_anime_with_mobile_api(session, title)
            if results:
                logger.info(f"ç§»åŠ¨ç«¯APIæœç´¢æˆåŠŸ")
                return results
        except Exception as e:
            logger.warning(f"ç§»åŠ¨ç«¯APIæœç´¢å¤±è´¥: {e}")

        # ç­–ç•¥3: æ›¿ä»£ç½‘ç«™æœç´¢
        try:
            results = await self.search_anime_alternative_sites(session, title)
            if results:
                logger.info(f"æ›¿ä»£ç½‘ç«™æœç´¢æˆåŠŸ")
                return results
        except Exception as e:
            logger.warning(f"æ›¿ä»£ç½‘ç«™æœç´¢å¤±è´¥: {e}")

        # ç­–ç•¥4: Selenium (æœ€åæ‰‹æ®µ)
        try:
            results = await self.search_anime_with_selenium(title)
            if results:
                logger.info(f"Seleniumæœç´¢æˆåŠŸ")
                return results
        except Exception as e:
            logger.warning(f"Seleniumæœç´¢å¤±è´¥: {e}")

        logger.error(f"æ‰€æœ‰æœç´¢ç­–ç•¥éƒ½å¤±è´¥äº†: {title}")
        return []

    async def _try_original_search(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """å°è¯•åŸå§‹æœç´¢æ–¹æ³•"""
        search_url = "https://search.douban.com/movie/subject_search"
        params = {
            'search_text': title,
            'cat': '1002'  # åŠ¨æ¼«åˆ†ç±»
        }

        response = await self._make_enhanced_request(
            session, search_url, params=params,
            referer="https://movie.douban.com"
        )

        if not response or 'text' not in response:
            return []

        # è§£ææœç´¢ç»“æœ
        html = response['text']
        results = []

        try:
            # æŸ¥æ‰¾ window.__DATA__ ä¸­çš„æ•°æ®
            data_match = re.search(r'window\.__DATA__\s*=\s*({.*?});', html, re.DOTALL)
            if data_match:
                data = json.loads(data_match.group(1))
                items = data.get('items', [])

                for item in items[:5]:
                    try:
                        douban_id = str(item.get('id', ''))
                        if douban_id:
                            anime_info = AnimeInfo(
                                title=item.get('title', title),
                                external_ids={WebsiteName.DOUBAN: douban_id}
                            )
                            results.append(anime_info)
                    except Exception as e:
                        logger.warning(f"è§£ææœç´¢é¡¹å¤±è´¥: {e}")
                        continue

            return results

        except Exception as e:
            logger.error(f"è§£ææœç´¢å“åº”å¤±è´¥: {e}")
            return []

    async def get_anime_rating(self, session: aiohttp.ClientSession, anime_id: str) -> Optional[RatingData]:
        """è·å–åŠ¨æ¼«è¯„åˆ†æ•°æ®"""
        url = f"{self.base_url}/subject/{anime_id}/"

        response = await self._make_enhanced_request(
            session, url, referer=f"{self.base_url}"
        )

        if not response or 'text' not in response:
            return None

        try:
            rating_data = self._extract_rating_from_page(response['text'])
            if not rating_data:
                return None

            rating = RatingData(
                website=WebsiteName.DOUBAN,
                raw_score=rating_data['score'],
                vote_count=rating_data['vote_count'],
                score_distribution=rating_data['score_distribution'],
                site_mean=None,
                site_std=None,
                last_updated=datetime.now(),
                url=url
            )

            return rating

        except Exception as e:
            logger.error(f"è·å–è±†ç“£è¯„åˆ†å¤±è´¥ {anime_id}: {e}")
            return None

    def _extract_rating_from_page(self, html: str) -> Optional[Dict[str, Any]]:
        """ä»è±†ç“£é¡µé¢æå–è¯„åˆ†ä¿¡æ¯"""
        soup = BeautifulSoup(html, 'html.parser')

        # æŸ¥æ‰¾è¯„åˆ†
        rating_element = soup.find('strong', class_='ll rating_num')
        if not rating_element:
            return None

        try:
            raw_score = float(rating_element.text.strip())
        except ValueError:
            return None

        # æŸ¥æ‰¾è¯„åˆ†äººæ•°
        vote_count = 0
        rating_people = soup.find('a', class_='rating_people')
        if rating_people:
            vote_text = rating_people.text
            vote_match = re.search(r'(\d+)', vote_text)
            if vote_match:
                vote_count = int(vote_match.group(1))

        return {
            'score': raw_score,
            'vote_count': vote_count,
            'score_distribution': {}
        }

    async def get_anime_details(self, session: aiohttp.ClientSession, anime_id: str) -> Optional[AnimeInfo]:
        """è·å–åŠ¨æ¼«è¯¦ç»†ä¿¡æ¯"""
        url = f"{self.base_url}/subject/{anime_id}/"

        response = await self._make_enhanced_request(
            session, url, referer=f"{self.base_url}"
        )

        if not response or 'text' not in response:
            return None

        try:
            return self._extract_anime_info_from_page(response['text'], anime_id)
        except Exception as e:
            logger.error(f"è·å–è±†ç“£åŠ¨æ¼«è¯¦æƒ…å¤±è´¥ {anime_id}: {e}")
            return None

    def _extract_anime_info_from_page(self, html: str, douban_id: str) -> Optional[AnimeInfo]:
        """ä»è±†ç“£é¡µé¢æå–åŠ¨æ¼«ä¿¡æ¯"""
        soup = BeautifulSoup(html, 'html.parser')

        # æ ‡é¢˜
        title_element = soup.find('span', property='v:itemreviewed')
        title = title_element.text.strip() if title_element else ''

        # åŸºæœ¬ä¿¡æ¯
        info_element = soup.find('div', id='info')
        if not info_element:
            return AnimeInfo(
                title=title,
                external_ids={WebsiteName.DOUBAN: douban_id}
            )

        info_text = info_element.get_text()

        # è§£æåŸºæœ¬ä¿¡æ¯
        anime_type = None
        episodes = None
        start_date = None
        studios = []
        genres = []

        # æŸ¥æ‰¾ç±»å‹
        type_match = re.search(r'ç±»å‹:\s*([^\n]+)', info_text)
        if type_match:
            type_str = type_match.group(1).strip()
            anime_type = self._parse_anime_type(type_str)
            genres = [g.strip() for g in type_str.split('/') if g.strip()]

        # æŸ¥æ‰¾é›†æ•°
        episodes_match = re.search(r'é›†æ•°:\s*(\d+)', info_text)
        if episodes_match:
            episodes = int(episodes_match.group(1))

        # æŸ¥æ‰¾é¦–æ’­æ—¥æœŸ
        date_match = re.search(r'é¦–æ’­:\s*([^\n]+)', info_text)
        if date_match:
            start_date = self._parse_date(date_match.group(1).strip())

        # æŸ¥æ‰¾åˆ¶ä½œå…¬å¸
        studio_match = re.search(r'åˆ¶ç‰‡å›½å®¶/åœ°åŒº:\s*([^\n]+)', info_text)
        if studio_match:
            studios = [s.strip() for s in studio_match.group(1).split('/') if s.strip()]

        anime_info = AnimeInfo(
            title=title,
            anime_type=anime_type,
            episodes=episodes,
            start_date=start_date,
            studios=studios,
            genres=genres,
            external_ids={WebsiteName.DOUBAN: douban_id}
        )

        return anime_info

    def _parse_anime_type(self, douban_type: str) -> Optional[AnimeType]:
        """è§£æè±†ç“£åŠ¨æ¼«ç±»å‹"""
        if 'ç”µè§†' in douban_type or 'TV' in douban_type:
            return AnimeType.TV
        elif 'ç”µå½±' in douban_type or 'å‰§åœºç‰ˆ' in douban_type:
            return AnimeType.MOVIE
        elif 'OVA' in douban_type:
            return AnimeType.OVA
        elif 'ONA' in douban_type:
            return AnimeType.ONA
        elif 'ç‰¹åˆ«ç¯‡' in douban_type or 'ç‰¹å…¸' in douban_type:
            return AnimeType.SPECIAL
        else:
            return AnimeType.TV  # é»˜è®¤ä¸ºTV

    def _parse_date(self, date_str: str) -> Optional[date]:
        """è§£æè±†ç“£æ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return None

        # è±†ç“£æ—¥æœŸæ ¼å¼å¤šæ ·ï¼Œå°è¯•å¤šç§è§£ææ–¹å¼
        date_patterns = [
            r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2024-01-15
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 2024å¹´1æœˆ15æ—¥
            r'(\d{4})å¹´(\d{1,2})æœˆ',  # 2024å¹´1æœˆ
            r'(\d{4})',  # 2024
        ]

        for pattern in date_patterns:
            match = re.search(pattern, date_str)
            if match:
                try:
                    groups = match.groups()
                    year = int(groups[0])
                    month = int(groups[1]) if len(groups) > 1 else 1
                    day = int(groups[2]) if len(groups) > 2 else 1
                    return date(year, month, day)
                except ValueError:
                    continue

        logger.warning(f"Failed to parse Douban date: {date_str}")
        return None

    async def get_seasonal_anime(self, session: aiohttp.ClientSession, season: str, year: int) -> List[AnimeInfo]:
        """è·å–å­£åº¦åŠ¨æ¼«åˆ—è¡¨ - è±†ç“£ä¸æä¾›æ­¤åŠŸèƒ½"""
        logger.warning("è±†ç“£ä¸æä¾›å­£åº¦åŠ¨æ¼«åˆ—è¡¨åŠŸèƒ½")
        return []

    async def get_site_statistics(self, session: aiohttp.ClientSession) -> Dict[str, Any]:
        """è·å–ç½‘ç«™ç»Ÿè®¡ä¿¡æ¯ - è±†ç“£ä¸æä¾›æ­¤åŠŸèƒ½"""
        logger.warning("è±†ç“£ä¸æä¾›ç½‘ç«™ç»Ÿè®¡ä¿¡æ¯")
        return {}
