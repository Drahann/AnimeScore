#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆè±†ç“£çˆ¬è™« - ç»ˆæååçˆ¬è™«ç‰ˆæœ¬
ä½¿ç”¨å¤šç§å…ˆè¿›æŠ€æœ¯ç»•è¿‡è±†ç“£çš„åçˆ¬è™«æœºåˆ¶
"""
import asyncio
import aiohttp
import random
import time
import json
import re
import hashlib
import base64
from typing import Dict, List, Optional, Any, Tuple, TYPE_CHECKING
from datetime import datetime, date
from bs4 import BeautifulSoup
from urllib.parse import urlencode, quote, unquote

from ..models.anime import AnimeInfo, RatingData, WebsiteName, AnimeType
from ..models.config import WebsiteConfig
from .base import WebScrapingBasedScraper
from loguru import logger

class DoubanEnhancedScraper(WebScrapingBasedScraper):
    """å¢å¼ºç‰ˆè±†ç“£çˆ¬è™« - ç»ˆæååçˆ¬è™«ç‰ˆæœ¬"""

    def __init__(self, website_name: WebsiteName, config: WebsiteConfig):
        super().__init__(website_name, config)
        self.base_url = config.base_url or "https://movie.douban.com"
        self.session_cookies = {}
        self.last_request_time = 0
        self.session_id = self._generate_session_id()
        self.request_count = 0
        self.failed_attempts = 0
        self.current_user_agent = None
        self.current_browser_type = None
        self.session_start_time = time.time()

        # æ›´çœŸå®çš„ç”¨æˆ·ä»£ç†æ±  - 2025å¹´æœ€æ–°ç‰ˆæœ¬ï¼ŒåŒ…å«æ›´å¤šå˜ä½“
        self.user_agents = [
            # Chrome Windows
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',

            # Chrome macOS
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',

            # Firefox Windows
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
            'Mozilla/5.0 (Windows NT 11.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0',

            # Safari macOS
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Safari/605.1.15',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 14_2_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',

            # Edge Windows
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0',
            'Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0',

            # Chrome Linux
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:123.0) Gecko/20100101 Firefox/123.0',
        ]

        # ä»£ç†æ± é…ç½®
        self.proxy_pool = []
        self.current_proxy_index = 0
        self.proxy_failure_count = {}

        # TLSæŒ‡çº¹ä¼ªè£…
        self.tls_versions = ['TLSv1.2', 'TLSv1.3']

        # å¢å¼ºçš„æµè§ˆå™¨æŒ‡çº¹ä¼ªè£…
        self.browser_fingerprints = {
            'chrome': {
                'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'accept_encoding': 'gzip, deflate, br, zstd',
                'accept_language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
                'sec_ch_ua': '"Chromium";v="122", "Not(A:Brand";v="24", "Google Chrome";v="122"',
                'sec_ch_ua_mobile': '?0',
                'sec_ch_ua_platform': '"Windows"',
                'sec_ch_ua_platform_version': '"15.0.0"',
                'sec_fetch_dest': 'document',
                'sec_fetch_mode': 'navigate',
                'sec_fetch_site': 'none',
                'sec_fetch_user': '?1',
                'upgrade_insecure_requests': '1',
                'cache_control': 'max-age=0'
            },
            'firefox': {
                'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'accept_encoding': 'gzip, deflate, br',
                'accept_language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
                'upgrade_insecure_requests': '1',
                'te': 'trailers'
            },
            'safari': {
                'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'accept_encoding': 'gzip, deflate, br',
                'accept_language': 'zh-CN,zh-Hans;q=0.9,en;q=0.8',
                'upgrade_insecure_requests': '1'
            },
            'edge': {
                'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'accept_encoding': 'gzip, deflate, br',
                'accept_language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
                'sec_ch_ua': '"Microsoft Edge";v="122", "Chromium";v="122", "Not(A:Brand";v="24"',
                'sec_ch_ua_mobile': '?0',
                'sec_ch_ua_platform': '"Windows"',
                'sec_fetch_dest': 'document',
                'sec_fetch_mode': 'navigate',
                'sec_fetch_site': 'none',
                'sec_fetch_user': '?1',
                'upgrade_insecure_requests': '1'
            }
        }

        # å±å¹•åˆ†è¾¨ç‡æ± ï¼ˆç”¨äºç”Ÿæˆæ›´çœŸå®çš„æŒ‡çº¹ï¼‰
        self.screen_resolutions = [
            '1920x1080', '1366x768', '1536x864', '1440x900', '1280x720',
            '2560x1440', '3840x2160', '1680x1050', '1600x900', '1024x768'
        ]

        # æ—¶åŒºåˆ—è¡¨
        self.timezones = [
            'Asia/Shanghai', 'Asia/Beijing', 'Asia/Hong_Kong', 'Asia/Taipei'
        ]

    def _generate_session_id(self) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„ä¼šè¯ID"""
        timestamp = str(int(time.time() * 1000))
        random_str = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=16))
        return hashlib.md5(f"{timestamp}_{random_str}".encode()).hexdigest()[:16]

    def _get_next_proxy(self) -> Optional[str]:
        """è·å–ä¸‹ä¸€ä¸ªä»£ç†"""
        if not self.proxy_pool:
            return None

        proxy = self.proxy_pool[self.current_proxy_index]
        self.current_proxy_index = (self.current_proxy_index + 1) % len(self.proxy_pool)
        return proxy

    def _detect_browser_type(self, user_agent: str) -> str:
        """æ ¹æ®User-Agentæ£€æµ‹æµè§ˆå™¨ç±»å‹"""
        if 'Edg/' in user_agent:
            return 'edge'
        elif 'Chrome' in user_agent and 'Safari' in user_agent and 'Edg' not in user_agent:
            return 'chrome'
        elif 'Firefox' in user_agent:
            return 'firefox'
        elif 'Safari' in user_agent and 'Chrome' not in user_agent:
            return 'safari'
        else:
            return 'chrome'  # é»˜è®¤

    def _select_consistent_user_agent(self) -> str:
        """é€‰æ‹©ä¸€ä¸ªä¸€è‡´çš„User-Agentå¹¶ä¿æŒä¼šè¯æœŸé—´ä¸å˜"""
        if not self.current_user_agent:
            self.current_user_agent = random.choice(self.user_agents)
            self.current_browser_type = self._detect_browser_type(self.current_user_agent)
            logger.debug(f"ğŸ­ é€‰æ‹©æµè§ˆå™¨ç±»å‹: {self.current_browser_type}")
        return self.current_user_agent

    def _generate_browser_fingerprint(self) -> Dict[str, str]:
        """ç”Ÿæˆä¸€è‡´çš„æµè§ˆå™¨æŒ‡çº¹"""
        if not self.current_browser_type:
            self._select_consistent_user_agent()

        fingerprint = self.browser_fingerprints[self.current_browser_type].copy()

        # æ ¹æ®User-AgentåŠ¨æ€è°ƒæ•´æŸäº›å¤´éƒ¨
        if self.current_browser_type == 'chrome':
            # ä»User-Agentä¸­æå–Chromeç‰ˆæœ¬
            chrome_version_match = re.search(r'Chrome/(\d+)', self.current_user_agent)
            if chrome_version_match:
                version = chrome_version_match.group(1)
                fingerprint['sec_ch_ua'] = f'"Chromium";v="{version}", "Not(A:Brand";v="24", "Google Chrome";v="{version}"'

            # æ£€æµ‹æ“ä½œç³»ç»Ÿ
            if 'Windows NT 11.0' in self.current_user_agent:
                fingerprint['sec_ch_ua_platform'] = '"Windows"'
                fingerprint['sec_ch_ua_platform_version'] = '"13.0.0"'
            elif 'Windows NT 10.0' in self.current_user_agent:
                fingerprint['sec_ch_ua_platform'] = '"Windows"'
                fingerprint['sec_ch_ua_platform_version'] = '"10.0.0"'
            elif 'Macintosh' in self.current_user_agent:
                fingerprint['sec_ch_ua_platform'] = '"macOS"'
                fingerprint['sec_ch_ua_platform_version'] = '"14.2.1"'
            elif 'Linux' in self.current_user_agent:
                fingerprint['sec_ch_ua_platform'] = '"Linux"'
                fingerprint['sec_ch_ua_platform_version'] = '""'

        elif self.current_browser_type == 'edge':
            # Edgeç‰¹æ®Šå¤„ç†
            edge_version_match = re.search(r'Edg/(\d+)', self.current_user_agent)
            if edge_version_match:
                version = edge_version_match.group(1)
                fingerprint['sec_ch_ua'] = f'"Microsoft Edge";v="{version}", "Chromium";v="{version}", "Not(A:Brand";v="24"'

        return fingerprint

    def _get_realistic_headers(self, referer: Optional[str] = None,
                              is_ajax: bool = False,
                              is_image: bool = False) -> Dict[str, str]:
        """è·å–æ›´çœŸå®çš„è¯·æ±‚å¤´"""
        user_agent = self._select_consistent_user_agent()
        fingerprint = self._generate_browser_fingerprint()

        headers = {
            'User-Agent': user_agent,
            'Accept': fingerprint['accept'],
            'Accept-Language': fingerprint['accept_language'],
            'Accept-Encoding': fingerprint['accept_encoding'],
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': fingerprint.get('upgrade_insecure_requests', '1'),
        }

        # æ ¹æ®è¯·æ±‚ç±»å‹è°ƒæ•´Acceptå¤´
        if is_ajax:
            headers['Accept'] = 'application/json, text/javascript, */*; q=0.01'
            headers['X-Requested-With'] = 'XMLHttpRequest'
        elif is_image:
            headers['Accept'] = 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8'

        # Chrome/Edgeç‰¹æœ‰çš„å®‰å…¨å¤´
        if self.current_browser_type in ['chrome', 'edge']:
            sec_headers = {
                'sec-ch-ua': fingerprint.get('sec_ch_ua', ''),
                'sec-ch-ua-mobile': fingerprint.get('sec_ch_ua_mobile', '?0'),
                'sec-ch-ua-platform': fingerprint.get('sec_ch_ua_platform', '"Windows"'),
            }

            # æ·»åŠ å¹³å°ç‰ˆæœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'sec_ch_ua_platform_version' in fingerprint:
                sec_headers['sec-ch-ua-platform-version'] = fingerprint['sec_ch_ua_platform_version']

            # æ ¹æ®è¯·æ±‚ç±»å‹è®¾ç½®Fetchå¤´
            if is_ajax:
                sec_headers.update({
                    'Sec-Fetch-Dest': 'empty',
                    'Sec-Fetch-Mode': 'cors',
                    'Sec-Fetch-Site': 'same-origin' if referer else 'cross-site',
                })
            elif is_image:
                sec_headers.update({
                    'Sec-Fetch-Dest': 'image',
                    'Sec-Fetch-Mode': 'no-cors',
                    'Sec-Fetch-Site': 'same-origin' if referer else 'cross-site',
                })
            else:
                sec_headers.update({
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none' if not referer else 'same-origin',
                    'Sec-Fetch-User': '?1',
                })

            headers.update(sec_headers)

        # æ·»åŠ Referer
        if referer:
            headers['Referer'] = referer

        # éšæœºæ·»åŠ ä¸€äº›çœŸå®æµè§ˆå™¨ä¼šæœ‰çš„å¤´
        random_headers = {}

        if random.random() < 0.4:  # 40%æ¦‚ç‡æ·»åŠ Cache-Control
            cache_controls = ['no-cache', 'max-age=0', 'no-store', 'must-revalidate']
            random_headers['Cache-Control'] = random.choice(cache_controls)

        if random.random() < 0.3:  # 30%æ¦‚ç‡æ·»åŠ Pragma
            random_headers['Pragma'] = 'no-cache'

        if random.random() < 0.2:  # 20%æ¦‚ç‡æ·»åŠ Purpose
            random_headers['Purpose'] = 'prefetch'

        if random.random() < 0.15:  # 15%æ¦‚ç‡æ·»åŠ Priority
            priorities = ['u=0, i', 'u=1, i', 'u=2, i', 'u=3, i']
            random_headers['Priority'] = random.choice(priorities)

        # Firefoxç‰¹æœ‰å¤´
        if self.current_browser_type == 'firefox' and 'te' in fingerprint:
            random_headers['TE'] = fingerprint['te']

        headers.update(random_headers)

        # ç§»é™¤Noneå€¼å’Œç©ºå€¼
        return {k: v for k, v in headers.items() if v is not None and v != ''}

    def _generate_realistic_cookies(self) -> Dict[str, str]:
        """ç”Ÿæˆæ›´çœŸå®çš„Cookie"""
        cookies = {}
        current_time = int(time.time())

        # è±†ç“£åŸºç¡€cookies
        cookies['bid'] = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=11))

        # Google Analytics cookies (è±†ç“£ä½¿ç”¨GA)
        ga_client_id = f"{random.randint(1000000000, 9999999999)}.{current_time - random.randint(86400, 31536000)}"
        cookies['_ga'] = f"GA1.2.{ga_client_id}"
        cookies['_gid'] = f"GA1.2.{random.randint(1000000000, 9999999999)}.{current_time - random.randint(0, 86400)}"

        # ä¼ ç»ŸUTM cookies
        session_start = current_time - random.randint(0, 3600)
        cookies['__utma'] = f"30149280.{ga_client_id}.{session_start}.{session_start}.{current_time}.1"
        cookies['__utmb'] = f"30149280.{random.randint(1, 10)}.10.{current_time}"
        cookies['__utmc'] = "30149280"
        cookies['__utmz'] = f"30149280.{session_start}.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)"

        # è±†ç“£ç‰¹æœ‰cookies
        if random.random() < 0.7:  # 70%æ¦‚ç‡
            cookies['ll'] = f'"{random.randint(100000, 999999)}"'

        if random.random() < 0.6:  # 60%æ¦‚ç‡
            cookies['dbcl2'] = f'"{random.randint(100000000, 999999999)}:{self.session_id}"'

        if random.random() < 0.5:  # 50%æ¦‚ç‡
            cookies['_pk_id.100001.8cb4'] = f"{random.randint(1000000000, 9999999999)}.{current_time}"
            cookies['_pk_ses.100001.8cb4'] = "1"

        if random.random() < 0.4:  # 40%æ¦‚ç‡
            cookies['ap_v'] = '0,6.0'

        if random.random() < 0.3:  # 30%æ¦‚ç‡
            cookies['viewed'] = f'"{random.randint(10000000, 99999999)}"'

        # æ·»åŠ ä¸€äº›éšæœºçš„æŠ€æœ¯cookies
        if random.random() < 0.2:  # 20%æ¦‚ç‡
            cookies['_vwo_uuid_v2'] = f"{random.randint(100000000, 999999999)}:{random.randint(100, 999)}"

        if random.random() < 0.15:  # 15%æ¦‚ç‡
            cookies['__gads'] = f"ID={random.randint(100000000000000000, 999999999999999999)}:T={current_time}:RT={current_time}:S=ALNI_M{random.randint(100000000000000000, 999999999999999999)}"

        # ä¼šè¯ç›¸å…³cookies
        cookies['_douban_session'] = self.session_id

        return cookies

    def _update_session_cookies(self, response_cookies) -> None:
        """æ›´æ–°ä¼šè¯cookies"""
        if not response_cookies:
            return

        try:
            for cookie in response_cookies:
                if hasattr(cookie, 'key') and hasattr(cookie, 'value'):
                    key, value = cookie.key, cookie.value
                elif hasattr(cookie, 'name') and hasattr(cookie, 'value'):
                    key, value = cookie.name, cookie.value
                else:
                    continue

                # åªä¿å­˜é‡è¦çš„cookies
                important_cookies = [
                    'bid', 'dbcl2', 'll', '_ga', '_gid', '__utma', '__utmb',
                    '__utmc', '__utmz', 'viewed', '_pk_id.100001.8cb4',
                    '_pk_ses.100001.8cb4', 'ap_v', '_douban_session'
                ]

                if key in important_cookies or key.startswith('_'):
                    self.session_cookies[key] = value
                    logger.debug(f"ğŸª æ›´æ–°Cookie: {key}")

        except Exception as e:
            logger.debug(f"Cookieæ›´æ–°é”™è¯¯: {e}")
    
    async def _adaptive_delay(self):
        """å¢å¼ºçš„è‡ªé€‚åº”å»¶è¿Ÿç­–ç•¥"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        session_duration = current_time - self.session_start_time

        # 1. åŸºç¡€å»¶è¿Ÿè®¡ç®—ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
        if self.failed_attempts == 0:
            base_delay = 3.0  # æˆåŠŸæ—¶çš„åŸºç¡€å»¶è¿Ÿ
        else:
            # æŒ‡æ•°é€€é¿ï¼š2^n * åŸºç¡€å»¶è¿Ÿï¼Œæœ€å¤§120ç§’
            base_delay = min(120.0, 5.0 * (2 ** min(self.failed_attempts - 1, 5)))

        # 2. ä¼šè¯æ—¶é•¿è°ƒæ•´ï¼ˆæ¨¡æ‹Ÿç”¨æˆ·ç–²åŠ³ï¼‰
        if session_duration > 1800:  # 30åˆ†é’Ÿå
            base_delay *= 1.3
        elif session_duration > 3600:  # 1å°æ—¶å
            base_delay *= 1.6

        # 3. è¯·æ±‚é¢‘ç‡æ§åˆ¶
        requests_per_minute = self.request_count / max(session_duration / 60, 1)
        if requests_per_minute > 10:  # æ¯åˆ†é’Ÿè¶…è¿‡10ä¸ªè¯·æ±‚
            base_delay *= 2.0
            logger.warning(f"ğŸš¨ è¯·æ±‚é¢‘ç‡è¿‡é«˜ ({requests_per_minute:.1f}/min)ï¼Œå¢åŠ å»¶è¿Ÿ")
        elif requests_per_minute > 6:  # æ¯åˆ†é’Ÿè¶…è¿‡6ä¸ªè¯·æ±‚
            base_delay *= 1.5

        # 4. æ—¶æ®µæ„ŸçŸ¥å»¶è¿Ÿ
        current_hour = datetime.now().hour
        time_factor = self._get_time_factor(current_hour)
        base_delay *= time_factor

        # 5. æ·»åŠ äººç±»è¡Œä¸ºéšæœºæ€§
        human_factor = random.uniform(0.8, 1.4)
        if random.random() < 0.1:  # 10%æ¦‚ç‡çš„"æ€è€ƒ"æ—¶é—´
            human_factor *= random.uniform(2.0, 4.0)
            logger.debug("ğŸ¤” æ¨¡æ‹Ÿç”¨æˆ·æ€è€ƒæ—¶é—´")

        final_delay = base_delay * human_factor

        # 6. ç‰¹æ®Šæƒ…å†µå¤„ç†
        if self.request_count > 0:
            # æ¯20ä¸ªè¯·æ±‚åçš„é•¿ä¼‘æ¯ï¼ˆç¼©çŸ­ä¸º15-30ç§’ï¼‰
            if self.request_count % 20 == 0:
                final_delay += random.uniform(15, 30)
                logger.info(f"ï¿½ é•¿ä¼‘æ¯æ—¶é—´: +{final_delay - base_delay * human_factor:.1f}ç§’")
                logger.warning("âš ï¸ é•¿ä¼‘æ¯åå°†é‡ç½®ä¼šè¯çŠ¶æ€ä»¥é¿å…è¢«æŒç»­æ ‡è®°")

            # æ¯50ä¸ªè¯·æ±‚åçš„è¶…é•¿ä¼‘æ¯ï¼ˆç¼©çŸ­ä¸º60-120ç§’ï¼‰
            elif self.request_count % 50 == 0:
                final_delay += random.uniform(60, 120)
                logger.info(f"ğŸ›Œ è¶…é•¿ä¼‘æ¯æ—¶é—´: +{final_delay - base_delay * human_factor:.1f}ç§’")
                logger.warning("âš ï¸ è¶…é•¿ä¼‘æ¯åå°†é‡ç½®ä¼šè¯çŠ¶æ€ä»¥é¿å…è¢«æŒç»­æ ‡è®°")

        # 7. æ‰§è¡Œå»¶è¿Ÿ
        if time_since_last < final_delay:
            actual_delay = final_delay - time_since_last
            logger.debug(f"â³ æ™ºèƒ½å»¶è¿Ÿ {actual_delay:.1f}ç§’ (å¤±è´¥:{self.failed_attempts}, é¢‘ç‡:{requests_per_minute:.1f}/min)")

            # åˆ†æ®µå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿç”¨æˆ·å¯èƒ½çš„ä¸­æ–­
            if actual_delay > 30:
                await self._segmented_delay(actual_delay)
            else:
                await asyncio.sleep(actual_delay)

        self.last_request_time = time.time()
        self.request_count += 1

    def _get_time_factor(self, hour: int) -> float:
        """æ ¹æ®æ—¶é—´æ®µè¿”å›å»¶è¿Ÿå› å­"""
        if 2 <= hour <= 6:  # æ·±å¤œ
            return 0.7  # å‡å°‘å»¶è¿Ÿï¼Œæ¨¡æ‹Ÿå¤œçŒ«å­
        elif 7 <= hour <= 9:  # æ—©é«˜å³°
            return 1.3  # å¢åŠ å»¶è¿Ÿï¼Œç½‘ç»œç¹å¿™
        elif 10 <= hour <= 11:  # ä¸Šåˆå·¥ä½œæ—¶é—´
            return 1.1
        elif 12 <= hour <= 14:  # åˆä¼‘æ—¶é—´
            return 0.9  # ç¨å¾®å‡å°‘
        elif 15 <= hour <= 17:  # ä¸‹åˆå·¥ä½œæ—¶é—´
            return 1.2
        elif 18 <= hour <= 20:  # æ™šé«˜å³°
            return 1.4  # æœ€ç¹å¿™æ—¶æ®µ
        elif 21 <= hour <= 23:  # æ™šä¸Šä¼‘é—²æ—¶é—´
            return 1.0  # æ­£å¸¸
        else:  # 0-1ç‚¹
            return 0.8

    async def _segmented_delay(self, total_delay: float):
        """åˆ†æ®µå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿç”¨æˆ·å¯èƒ½çš„ä¸­æ–­è¡Œä¸º"""
        remaining = total_delay
        is_long_delay = total_delay > 60  # åˆ¤æ–­æ˜¯å¦ä¸ºé•¿å»¶è¿Ÿ

        while remaining > 0:
            # éšæœºé€‰æ‹©ä¸€ä¸ªå»¶è¿Ÿæ®µ
            segment = min(remaining, random.uniform(5, 15))
            await asyncio.sleep(segment)
            remaining -= segment

            # å°æ¦‚ç‡çš„"ç”¨æˆ·å›æ¥"æ£€æŸ¥
            if remaining > 10 and random.random() < 0.05:
                logger.debug("ğŸ‘€ æ¨¡æ‹Ÿç”¨æˆ·ä¸­é€”æ£€æŸ¥")
                await asyncio.sleep(random.uniform(1, 3))

        # é•¿å»¶è¿Ÿåé‡ç½®ä¼šè¯çŠ¶æ€ä»¥é¿å…è¢«æŒç»­æ ‡è®°
        if is_long_delay:
            self._reset_session_state()
            logger.info("ğŸ”„ é•¿å»¶è¿Ÿåå·²é‡ç½®ä¼šè¯çŠ¶æ€ï¼Œé¿å…æŒç»­æ ‡è®°")

    def _should_rotate_session(self) -> bool:
        """æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦è½®æ¢ä¼šè¯"""
        current_time = time.time()
        session_duration = current_time - self.session_start_time

        # 1. å¤±è´¥æ¬¡æ•°è¿‡å¤šæ—¶è½®æ¢
        if self.failed_attempts >= 3:
            logger.info(f"ğŸ”„ å¤±è´¥æ¬¡æ•°è¿‡å¤š ({self.failed_attempts})ï¼Œè½®æ¢ä¼šè¯")
            return True

        # 2. è¯·æ±‚æ¬¡æ•°è¿‡å¤šæ—¶è½®æ¢
        if self.request_count >= 80:  # å¢åŠ åˆ°80ä¸ªè¯·æ±‚
            logger.info(f"ğŸ”„ è¯·æ±‚æ¬¡æ•°è¿‡å¤š ({self.request_count})ï¼Œè½®æ¢ä¼šè¯")
            return True

        # 3. ä¼šè¯æ—¶é—´è¿‡é•¿æ—¶è½®æ¢
        if session_duration > 7200:  # 2å°æ—¶
            logger.info(f"ğŸ”„ ä¼šè¯æ—¶é—´è¿‡é•¿ ({session_duration/3600:.1f}å°æ—¶)ï¼Œè½®æ¢ä¼šè¯")
            return True

        # 4. éšæœºè½®æ¢ï¼ˆæ¨¡æ‹Ÿç”¨æˆ·å…³é—­é‡å¼€æµè§ˆå™¨ï¼‰
        if session_duration > 1800 and random.random() < 0.03:  # 30åˆ†é’Ÿå3%æ¦‚ç‡
            logger.info("ğŸ”„ éšæœºè½®æ¢ä¼šè¯ï¼ˆæ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºï¼‰")
            return True

        # 5. æ£€æµ‹åˆ°å¯èƒ½çš„æ£€æµ‹æ—¶è½®æ¢
        if self.failed_attempts >= 2 and self.request_count > 20:
            logger.info("ğŸ”„ æ£€æµ‹åˆ°å¯èƒ½çš„åçˆ¬è™«æ£€æµ‹ï¼Œè½®æ¢ä¼šè¯")
            return True

        return False

    def _reset_session_state(self):
        """é‡ç½®ä¼šè¯çŠ¶æ€"""
        old_session_id = self.session_id[:8]
        self.session_id = self._generate_session_id()
        self.session_cookies = {}
        self.request_count = 0
        self.failed_attempts = max(0, self.failed_attempts - 2)  # å‡å°‘å¤±è´¥è®¡æ•°
        self.session_start_time = time.time()
        self.current_user_agent = None  # é‡ç½®User-Agent
        self.current_browser_type = None

        logger.info(f"ğŸ”„ ä¼šè¯å·²é‡ç½® ({old_session_id}... -> {self.session_id[:8]}...)")

        # é‡ç½®åçš„çŸ­æš‚å»¶è¿Ÿ
        return random.uniform(5, 15)

    def _extract_anti_csrf_token(self, html: str) -> Optional[str]:
        """æå–åCSRFä»¤ç‰Œ"""
        # æŸ¥æ‰¾å¸¸è§çš„CSRF token
        patterns = [
            r'name=["\']_token["\'] value=["\']([^"\']+)["\']',
            r'name=["\']csrf_token["\'] value=["\']([^"\']+)["\']',
            r'window\.__csrf_token__\s*=\s*["\']([^"\']+)["\']',
            r'data-csrf-token=["\']([^"\']+)["\']',
        ]

        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                return match.group(1)

        return None

    async def _handle_security_challenge(self, session: aiohttp.ClientSession,
                                       response, original_url: str) -> Optional[Dict[str, Any]]:
        """å¤„ç†è±†ç“£å®‰å…¨æŒ‘æˆ˜"""
        response_url = str(response.url)

        # æ£€æµ‹å„ç§å®‰å…¨éªŒè¯é¡µé¢
        security_indicators = [
            'sec.douban.com',
            'verify.douban.com',
            'captcha',
            'å®‰å…¨éªŒè¯',
            'è¯·è¾“å…¥éªŒè¯ç ',
            'robot',
            'blocked'
        ]

        html = await response.text()
        is_security_page = any(indicator in response_url or indicator in html
                              for indicator in security_indicators)

        if is_security_page:
            logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°å®‰å…¨éªŒè¯é¡µé¢: {response_url}")

            # ç­–ç•¥1: æŸ¥æ‰¾è‡ªåŠ¨è·³è½¬
            redirect_patterns = [
                r'location\.href\s*=\s*["\']([^"\']+)["\']',
                r'window\.location\s*=\s*["\']([^"\']+)["\']',
                r'document\.location\s*=\s*["\']([^"\']+)["\']',
                r'<meta[^>]*http-equiv=["\']refresh["\'][^>]*content=["\'][^;]*;\s*url=([^"\']+)["\']'
            ]

            for pattern in redirect_patterns:
                match = re.search(pattern, html, re.IGNORECASE)
                if match:
                    target_url = match.group(1)
                    if target_url.startswith('/'):
                        target_url = 'https://www.douban.com' + target_url

                    logger.info(f"ğŸ”„ å‘ç°è‡ªåŠ¨è·³è½¬: {target_url}")

                    # æ¨¡æ‹Ÿäººç±»ç­‰å¾…æ—¶é—´
                    await asyncio.sleep(random.uniform(3, 8))

                    return await self._make_ultimate_request(
                        session, target_url, referer=response_url
                    )

            # ç­–ç•¥2: æŸ¥æ‰¾éªŒè¯è¡¨å•
            form_match = re.search(r'<form[^>]*action=["\']([^"\']+)["\'][^>]*>', html)
            if form_match:
                form_action = form_match.group(1)
                logger.info(f"ğŸ“ å‘ç°éªŒè¯è¡¨å•: {form_action}")

                # å°è¯•æå–å¹¶å¤„ç†è¡¨å•
                csrf_token = self._extract_anti_csrf_token(html)
                if csrf_token:
                    logger.info(f"ğŸ”‘ æå–åˆ°CSRFä»¤ç‰Œ: {csrf_token[:8]}...")
                    # è¿™é‡Œå¯ä»¥å®ç°è‡ªåŠ¨è¡¨å•æäº¤é€»è¾‘

            # ç­–ç•¥3: ç­‰å¾…åé‡è¯•åŸå§‹URL
            logger.info("â³ ç­‰å¾…åé‡è¯•åŸå§‹è¯·æ±‚...")
            await asyncio.sleep(random.uniform(30, 60))

            # é‡ç½®ä¼šè¯çŠ¶æ€
            self._reset_session_state()

            return await self._make_ultimate_request(
                session, original_url, referer="https://www.douban.com"
            )

        # æ­£å¸¸å“åº”
        if response.status == 200:
            return {"text": html, "status": response.status}

        return None
    
    async def _make_ultimate_request(self, session: aiohttp.ClientSession,
                                   url: str, method: str = "GET",
                                   headers: Optional[Dict[str, str]] = None,
                                   params: Optional[Dict[str, Any]] = None,
                                   data: Optional[Dict[str, Any]] = None,
                                   referer: Optional[str] = None,
                                   is_ajax: bool = False,
                                   max_retries: int = 5) -> Optional[Dict[str, Any]]:
        """ç»ˆæHTTPè¯·æ±‚ - åŒ…å«æœ€å…ˆè¿›çš„ååçˆ¬è™«æœºåˆ¶"""

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½®æ¢ä¼šè¯
        if self._should_rotate_session():
            self._reset_session_state()

        for attempt in range(max_retries):
            try:
                # è‡ªé€‚åº”å»¶è¿Ÿ
                await self._adaptive_delay()

                # è·å–çœŸå®çš„è¯·æ±‚å¤´
                request_headers = headers or self._get_realistic_headers(referer, is_ajax)

                # åˆå¹¶ä¼šè¯cookieså’Œç”Ÿæˆçš„cookies
                cookies = self.session_cookies.copy()
                if not cookies:  # å¦‚æœæ²¡æœ‰ä¼šè¯cookiesï¼Œç”Ÿæˆä¸€äº›åŸºç¡€cookies
                    cookies.update(self._generate_realistic_cookies())

                # è·å–ä»£ç†ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
                proxy = self._get_next_proxy()

                # é…ç½®è¶…æ—¶
                timeout = aiohttp.ClientTimeout(
                    total=60,  # æ€»è¶…æ—¶æ—¶é—´
                    connect=30,  # è¿æ¥è¶…æ—¶
                    sock_read=30  # è¯»å–è¶…æ—¶
                )

                logger.debug(f"ğŸŒ è¯·æ±‚ {url} (ç¬¬{attempt+1}/{max_retries}æ¬¡, ä¼šè¯: {self.session_id[:8]})")

                # æ„å»ºè¯·æ±‚å‚æ•°
                request_kwargs = {
                    'method': method,
                    'url': url,
                    'headers': request_headers,
                    'cookies': cookies,
                    'timeout': timeout,
                    'ssl': False,  # å¿½ç•¥SSLéªŒè¯
                    'allow_redirects': True,
                    'max_redirects': 5
                }

                # æ·»åŠ å‚æ•°å’Œæ•°æ®
                if params:
                    request_kwargs['params'] = params
                if data:
                    if method.upper() == 'POST':
                        request_kwargs['data'] = data
                    else:
                        request_kwargs['json'] = data

                # æ·»åŠ ä»£ç†
                if proxy:
                    request_kwargs['proxy'] = proxy
                    logger.debug(f"ğŸŒ ä½¿ç”¨ä»£ç†: {proxy}")

                async with session.request(**request_kwargs) as response:

                    # æ›´æ–°cookies
                    if response.cookies:
                        self._update_session_cookies(response.cookies)

                    logger.debug(f"ğŸ“Š å“åº”: {response.status} {response.reason} (å¤§å°: {response.headers.get('content-length', 'unknown')})")

                    # æˆåŠŸå“åº”
                    if response.status == 200:
                        # æ£€æŸ¥å®‰å…¨æŒ‘æˆ˜
                        result = await self._handle_security_challenge(session, response, url)
                        if result:
                            self.failed_attempts = 0  # é‡ç½®å¤±è´¥è®¡æ•°
                            return result

                    # å¢å¼ºçš„é”™è¯¯çŠ¶æ€å¤„ç†
                    elif response.status == 403:
                        self.failed_attempts += 1
                        logger.warning(f"ğŸš« 403 Forbidden (å¤±è´¥æ¬¡æ•°: {self.failed_attempts})")

                        # æ£€æŸ¥å“åº”å†…å®¹ä»¥ç¡®å®šå…·ä½“åŸå› 
                        try:
                            response_text = await response.text()
                            if 'éªŒè¯ç ' in response_text or 'captcha' in response_text.lower():
                                logger.error("ğŸ¤– æ£€æµ‹åˆ°éªŒè¯ç è¦æ±‚ï¼Œéœ€è¦äººå·¥å¹²é¢„")
                                return None
                            elif 'blocked' in response_text.lower() or 'å°ç¦' in response_text:
                                logger.error("ğŸš« IPå¯èƒ½è¢«å°ç¦")
                                # å°è¯•æ›´é•¿çš„ç­‰å¾…æ—¶é—´
                                wait_time = min(600, 120 * (attempt + 1))
                            else:
                                wait_time = min(180, 30 * (attempt + 1))
                        except:
                            wait_time = min(120, 20 * (attempt + 1))

                        if attempt < max_retries - 1:
                            logger.info(f"â³ 403é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                            await asyncio.sleep(wait_time)

                            # å¼ºåˆ¶è½®æ¢ä¼šè¯
                            reset_delay = self._reset_session_state()
                            await asyncio.sleep(reset_delay)
                            continue

                    elif response.status == 429:
                        self.failed_attempts += 1
                        logger.warning(f"ğŸŒ 429 Too Many Requests (å¤±è´¥æ¬¡æ•°: {self.failed_attempts})")

                        # ä»å“åº”å¤´è·å–é‡è¯•æ—¶é—´
                        retry_after = response.headers.get('Retry-After')
                        if retry_after:
                            try:
                                wait_time = int(retry_after)
                            except ValueError:
                                wait_time = 300
                        else:
                            # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤šç­‰å¾…10åˆ†é’Ÿ
                            wait_time = min(600, 60 * (2 ** attempt))

                        logger.info(f"â³ é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        await asyncio.sleep(wait_time)

                        # 429é”™è¯¯åä¹Ÿè½®æ¢ä¼šè¯
                        if attempt == max_retries // 2:
                            reset_delay = self._reset_session_state()
                            await asyncio.sleep(reset_delay)
                        continue

                    elif response.status == 418:  # I'm a teapot - æœ‰äº›ç½‘ç«™ç”¨è¿™ä¸ªè¡¨ç¤ºåçˆ¬è™«
                        self.failed_attempts += 1
                        logger.warning("ğŸ«– 418 I'm a teapot - å¯èƒ½è§¦å‘åçˆ¬è™«æœºåˆ¶")
                        if attempt < max_retries - 1:
                            wait_time = random.uniform(300, 600)  # 5-10åˆ†é’Ÿ
                            logger.info(f"â³ ç­‰å¾… {wait_time:.0f} ç§’åé‡è¯•...")
                            await asyncio.sleep(wait_time)
                            reset_delay = self._reset_session_state()
                            await asyncio.sleep(reset_delay)
                            continue

                    elif response.status in [301, 302, 303, 307, 308]:
                        # å¢å¼ºçš„é‡å®šå‘å¤„ç†
                        location = response.headers.get('Location')
                        if location:
                            # æ£€æŸ¥é‡å®šå‘æ˜¯å¦æ˜¯å®‰å…¨éªŒè¯é¡µé¢
                            if any(keyword in location.lower() for keyword in ['verify', 'captcha', 'security', 'robot']):
                                logger.warning(f"ğŸ”’ é‡å®šå‘åˆ°å®‰å…¨éªŒè¯é¡µé¢: {location}")
                                # å°è¯•å¤„ç†å®‰å…¨éªŒè¯
                                return await self._handle_security_challenge(session, response, url)
                            else:
                                logger.info(f"ğŸ”„ é‡å®šå‘åˆ°: {location}")
                                return await self._make_ultimate_request(
                                    session, location, method='GET', referer=url
                                )

                    elif response.status == 404:
                        logger.warning(f"ğŸ” 404 Not Found: {url}")
                        return None  # 404é€šå¸¸ä¸éœ€è¦é‡è¯•

                    elif response.status in [502, 503, 504]:
                        logger.warning(f"ğŸ”§ æœåŠ¡å™¨é”™è¯¯: {response.status}")
                        if attempt < max_retries - 1:
                            # æœåŠ¡å™¨é”™è¯¯ä½¿ç”¨è¾ƒçŸ­çš„ç­‰å¾…æ—¶é—´
                            wait_time = random.uniform(10, 30) * (attempt + 1)
                            logger.info(f"â³ æœåŠ¡å™¨é”™è¯¯ï¼Œç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                            await asyncio.sleep(wait_time)
                            continue

                    elif response.status >= 500:
                        logger.warning(f"ğŸ”§ å…¶ä»–æœåŠ¡å™¨é”™è¯¯: {response.status}")
                        if attempt < max_retries - 1:
                            wait_time = random.uniform(15, 45)
                            await asyncio.sleep(wait_time)
                            continue

                    else:
                        logger.warning(f"âŒ æœªçŸ¥çŠ¶æ€ç : {response.status} {response.reason}")
                        if attempt < max_retries - 1:
                            wait_time = random.uniform(5, 15)
                            await asyncio.sleep(wait_time)
                            continue

            except asyncio.TimeoutError:
                self.failed_attempts += 1
                logger.warning(f"â° è¯·æ±‚è¶…æ—¶ (ç¬¬{attempt+1}æ¬¡ï¼Œæ€»å¤±è´¥:{self.failed_attempts})")
                if attempt < max_retries - 1:
                    # è¶…æ—¶åå¢åŠ ç­‰å¾…æ—¶é—´
                    wait_time = random.uniform(15, 30) * (attempt + 1)
                    logger.info(f"â³ è¶…æ—¶é‡è¯•ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                    await asyncio.sleep(wait_time)
                    continue

            except aiohttp.ClientConnectorError as e:
                self.failed_attempts += 1
                logger.error(f"ğŸ”Œ è¿æ¥é”™è¯¯: {e}")
                if attempt < max_retries - 1:
                    # è¿æ¥é”™è¯¯å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
                    wait_time = random.uniform(30, 60) * (attempt + 1)
                    logger.info(f"â³ è¿æ¥é”™è¯¯é‡è¯•ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                    await asyncio.sleep(wait_time)

                    # è¿æ¥é”™è¯¯æ—¶è½®æ¢ä¼šè¯
                    if attempt >= max_retries // 2:
                        reset_delay = self._reset_session_state()
                        await asyncio.sleep(reset_delay)
                    continue

            except aiohttp.ClientResponseError as e:
                self.failed_attempts += 1
                logger.error(f"ğŸ“¡ å“åº”é”™è¯¯: {e.status} {e.message}")
                if attempt < max_retries - 1:
                    wait_time = random.uniform(10, 25)
                    await asyncio.sleep(wait_time)
                    continue

            except aiohttp.ClientError as e:
                self.failed_attempts += 1
                logger.error(f"ğŸŒ å®¢æˆ·ç«¯é”™è¯¯: {type(e).__name__}: {e}")
                if attempt < max_retries - 1:
                    wait_time = random.uniform(10, 20)
                    await asyncio.sleep(wait_time)
                    continue

            except json.JSONDecodeError as e:
                logger.error(f"ğŸ“„ JSONè§£æé”™è¯¯: {e}")
                # JSONé”™è¯¯é€šå¸¸ä¸éœ€è¦é‡è¯•
                return None

            except UnicodeDecodeError as e:
                logger.error(f"ğŸ”¤ ç¼–ç é”™è¯¯: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(random.uniform(5, 10))
                    continue

            except Exception as e:
                self.failed_attempts += 1
                logger.error(f"ğŸ’¥ æœªçŸ¥é”™è¯¯: {type(e).__name__}: {e}")
                if attempt < max_retries - 1:
                    wait_time = random.uniform(15, 30)
                    await asyncio.sleep(wait_time)
                    continue

        logger.error(f"âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†: {url}")
        return None
    
    async def _init_session_carefully(self, session: aiohttp.ClientSession) -> bool:
        """è°¨æ…åˆå§‹åŒ–ä¼šè¯ - æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º"""
        logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–è±†ç“£ä¼šè¯...")

        try:
            # æ­¥éª¤1: è®¿é—®è±†ç“£é¦–é¡µ
            logger.debug("ğŸ“± è®¿é—®è±†ç“£é¦–é¡µ...")
            home_response = await self._make_ultimate_request(
                session, "https://www.douban.com"
            )

            if not home_response:
                logger.warning("âŒ æ— æ³•è®¿é—®è±†ç“£é¦–é¡µ")
                return False

            # æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆè¡Œä¸º - çŸ­æš‚åœç•™
            await asyncio.sleep(random.uniform(2, 5))

            # æ­¥éª¤2: è®¿é—®ç”µå½±é¦–é¡µ
            logger.debug("ğŸ¬ è®¿é—®è±†ç“£ç”µå½±é¦–é¡µ...")
            movie_response = await self._make_ultimate_request(
                session, "https://movie.douban.com",
                referer="https://www.douban.com"
            )

            if not movie_response:
                logger.warning("âŒ æ— æ³•è®¿é—®è±†ç“£ç”µå½±é¦–é¡µ")
                return False

            # æ¨¡æ‹Ÿç”¨æˆ·æµè§ˆè¡Œä¸º - æŸ¥çœ‹é¡µé¢å†…å®¹
            await asyncio.sleep(random.uniform(3, 7))

            # æ­¥éª¤3: è®¿é—®æœç´¢é¡µé¢ï¼ˆé¢„çƒ­ï¼‰
            logger.debug("ğŸ” é¢„çƒ­æœç´¢åŠŸèƒ½...")
            search_page_response = await self._make_ultimate_request(
                session, "https://search.douban.com/movie/subject_search",
                referer="https://movie.douban.com"
            )

            if search_page_response:
                logger.debug("âœ… æœç´¢é¡µé¢é¢„çƒ­æˆåŠŸ")

            # æ£€æŸ¥æ˜¯å¦è·å¾—äº†æœ‰æ•ˆçš„cookies
            if len(self.session_cookies) > 0:
                logger.info(f"âœ… ä¼šè¯åˆå§‹åŒ–æˆåŠŸ (è·å¾— {len(self.session_cookies)} ä¸ªcookies)")
                return True
            else:
                logger.warning("âš ï¸ ä¼šè¯åˆå§‹åŒ–å®Œæˆï¼Œä½†æœªè·å¾—cookies")
                return True  # ä»ç„¶ç»§ç»­ï¼Œå¯èƒ½ä¸éœ€è¦cookies

        except Exception as e:
            logger.error(f"âŒ ä¼šè¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def search_anime_with_mobile_api(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """ä½¿ç”¨ç§»åŠ¨ç«¯APIæœç´¢åŠ¨æ¼«"""
        logger.info(f"ğŸ“± ä½¿ç”¨ç§»åŠ¨ç«¯APIæœç´¢: {title}")

        try:
            # æ›´ä¸°å¯Œçš„ç§»åŠ¨ç«¯User-Agentæ± 
            mobile_agents = [
                # iPhone Safari
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 16_7 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (iPhone; CPU iPhone OS 15_8 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.6 Mobile/15E148 Safari/604.1',

                # Android Chrome
                'Mozilla/5.0 (Linux; Android 14; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 13; Pixel 7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Mobile Safari/537.36',
                'Mozilla/5.0 (Linux; Android 12; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',

                # Android Firefox
                'Mozilla/5.0 (Mobile; rv:123.0) Gecko/123.0 Firefox/123.0',
                'Mozilla/5.0 (Android 14; Mobile; rv:122.0) Gecko/122.0 Firefox/122.0',

                # å¾®ä¿¡å†…ç½®æµè§ˆå™¨
                'Mozilla/5.0 (Linux; Android 12; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/120.0.0.0 Mobile Safari/537.36 MicroMessenger/8.0.47.2560(0x28002F30) Process/tools WeChat/arm64 Weixin NetType/WIFI Language/zh_CN ABI/arm64',
            ]

            # éšæœºé€‰æ‹©ä¸€ä¸ªç§»åŠ¨ç«¯User-Agent
            mobile_ua = random.choice(mobile_agents)

            mobile_headers = {
                'User-Agent': mobile_ua,
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Referer': 'https://m.douban.com/',
                'Origin': 'https://m.douban.com',
                'X-Requested-With': 'XMLHttpRequest',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
            }

            # å¦‚æœæ˜¯å¾®ä¿¡æµè§ˆå™¨ï¼Œæ·»åŠ ç‰¹æ®Šå¤´
            if 'MicroMessenger' in mobile_ua:
                mobile_headers['X-Requested-With'] = 'com.tencent.mm'

            # æ‰©å±•çš„ç§»åŠ¨ç«¯APIç«¯ç‚¹
            api_endpoints = [
                {
                    'url': 'https://m.douban.com/rexxar/api/v2/search',
                    'params': {'q': title, 'type': 'movie', 'count': 10, 'start': 0}
                },
                {
                    'url': 'https://m.douban.com/j/search',
                    'params': {'q': title, 'cat': '1002', 'start': 0}
                },
                {
                    'url': 'https://frodo.douban.com/api/v2/search/movie',
                    'params': {'q': title, 'count': 10, 'start': 0}
                },
                {
                    'url': 'https://m.douban.com/search/',
                    'params': {'query': title, 'type': 'movie'}
                },
                {
                    'url': 'https://movie.douban.com/j/subject_suggest',
                    'params': {'q': title}
                }
            ]

            for endpoint in api_endpoints:
                try:
                    api_url = endpoint['url']
                    params = endpoint['params']

                    logger.debug(f"ğŸ” å°è¯•ç§»åŠ¨ç«¯API: {api_url}")

                    response = await self._make_ultimate_request(
                        session, api_url,
                        headers=mobile_headers,
                        params=params,
                        referer="https://m.douban.com/",
                        is_ajax=True
                    )

                    if response:
                        results = self._parse_mobile_api_response(response, title)
                        if results:
                            logger.info(f"âœ… ç§»åŠ¨ç«¯APIæœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                            return results

                    # çŸ­æš‚å»¶è¿Ÿåå°è¯•ä¸‹ä¸€ä¸ªAPI
                    await asyncio.sleep(random.uniform(2, 5))

                except Exception as e:
                    logger.debug(f"ç§»åŠ¨ç«¯API {api_url} å¤±è´¥: {e}")
                    continue

            logger.warning("âŒ æ‰€æœ‰ç§»åŠ¨ç«¯APIéƒ½å¤±è´¥äº†")
            return []

        except Exception as e:
            logger.error(f"ç§»åŠ¨ç«¯APIæœç´¢å¼‚å¸¸: {e}")
            return []

    def _parse_mobile_api_response(self, response: Dict[str, Any], search_title: str) -> List[AnimeInfo]:
        """è§£æç§»åŠ¨ç«¯APIå“åº”"""
        results = []

        try:
            # å°è¯•ä¸åŒçš„å“åº”æ ¼å¼
            items = None

            if isinstance(response, dict):
                # æ ‡å‡†æ ¼å¼
                if 'items' in response:
                    items = response['items']
                elif 'subjects' in response:
                    items = response['subjects']
                elif 'data' in response and isinstance(response['data'], list):
                    items = response['data']
                elif isinstance(response.get('text'), str):
                    # HTMLå“åº”ï¼Œå°è¯•è§£æ
                    return self._parse_mobile_html_response(response['text'], search_title)

            if items and isinstance(items, list):
                for item in items[:5]:  # é™åˆ¶ç»“æœæ•°é‡
                    try:
                        douban_id = str(item.get('id', ''))
                        title_text = item.get('title', item.get('name', item.get('subject', {}).get('title', '')))

                        # æå–æ›´å¤šä¿¡æ¯
                        year = item.get('year', item.get('pubdate', ''))
                        rating = item.get('rating', item.get('rate', ''))

                        if douban_id and title_text:
                            anime_info = AnimeInfo(
                                title=title_text,
                                external_ids={WebsiteName.DOUBAN: douban_id}
                            )

                            # å¦‚æœæœ‰å¹´ä»½ä¿¡æ¯ï¼Œæ·»åŠ åˆ°å¯¹è±¡ä¸­
                            if year:
                                try:
                                    anime_info.year = int(str(year)[:4])
                                except:
                                    pass

                            results.append(anime_info)

                    except Exception as e:
                        logger.debug(f"è§£æç§»åŠ¨ç«¯APIç»“æœé¡¹å¤±è´¥: {e}")
                        continue

        except Exception as e:
            logger.debug(f"è§£æç§»åŠ¨ç«¯APIå“åº”å¤±è´¥: {e}")

        return results

    def _parse_mobile_html_response(self, html: str, search_title: str) -> List[AnimeInfo]:
        """è§£æç§»åŠ¨ç«¯HTMLå“åº”"""
        results = []

        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')

            # æŸ¥æ‰¾æœç´¢ç»“æœ
            result_items = soup.find_all(['div', 'li'], class_=re.compile(r'result|item|subject'))

            for item in result_items[:5]:
                try:
                    # æŸ¥æ‰¾é“¾æ¥
                    link = item.find('a', href=re.compile(r'/subject/\d+'))
                    if link:
                        href = link.get('href', '')
                        douban_id_match = re.search(r'/subject/(\d+)', href)

                        if douban_id_match:
                            douban_id = douban_id_match.group(1)
                            title_text = link.get_text(strip=True) or link.get('title', '')

                            if title_text:
                                anime_info = AnimeInfo(
                                    title=title_text,
                                    external_ids={WebsiteName.DOUBAN: douban_id}
                                )
                                results.append(anime_info)

                except Exception as e:
                    logger.debug(f"è§£æHTMLç»“æœé¡¹å¤±è´¥: {e}")
                    continue

        except Exception as e:
            logger.debug(f"è§£æHTMLå“åº”å¤±è´¥: {e}")

        return results
    
    async def search_anime_with_proxy(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """ä½¿ç”¨ä»£ç†æœç´¢åŠ¨æ¼«"""
        logger.info(f"ğŸŒ ä½¿ç”¨ä»£ç†æœç´¢: {title}")

        # ä»£ç†æœåŠ¡å™¨é…ç½®ï¼ˆå¯ä»¥ä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
        proxy_configs = [
            # ç¤ºä¾‹ä»£ç†é…ç½®ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ›¿æ¢ä¸ºçœŸå®çš„ä»£ç†
            # {"http": "http://proxy1:port", "https": "https://proxy1:port"},
            # {"http": "http://proxy2:port", "https": "https://proxy2:port"},
        ]

        if not proxy_configs:
            logger.warning("æœªé…ç½®ä»£ç†æœåŠ¡å™¨")
            return []

        for proxy_config in proxy_configs:
            try:
                logger.debug(f"ğŸ”„ å°è¯•ä»£ç†: {proxy_config}")

                # ä½¿ç”¨ä»£ç†çš„ç‰¹æ®Šè¯·æ±‚å¤´
                proxy_headers = self._get_realistic_headers()
                proxy_headers['X-Forwarded-For'] = f"{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}"

                search_url = "https://search.douban.com/movie/subject_search"
                params = {
                    'search_text': title,
                    'cat': '1002'
                }

                # åˆ›å»ºå¸¦ä»£ç†çš„è¿æ¥å™¨
                connector = aiohttp.TCPConnector(ssl=False)
                timeout = aiohttp.ClientTimeout(total=60)

                async with aiohttp.ClientSession(
                    connector=connector,
                    timeout=timeout
                ) as proxy_session:

                    async with proxy_session.get(
                        search_url,
                        params=params,
                        headers=proxy_headers,
                        proxy=proxy_config.get('http')
                    ) as response:

                        if response.status == 200:
                            html = await response.text()
                            results = self._parse_search_results(html, title)
                            if results:
                                logger.info(f"âœ… ä»£ç†æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                                return results

                await asyncio.sleep(random.uniform(3, 8))

            except Exception as e:
                logger.debug(f"ä»£ç† {proxy_config} å¤±è´¥: {e}")
                continue

        logger.warning("âŒ æ‰€æœ‰ä»£ç†éƒ½å¤±è´¥äº†")
        return []

    def _parse_search_results(self, html: str, search_title: str) -> List[AnimeInfo]:
        """è§£ææœç´¢ç»“æœHTML"""
        results = []

        try:
            # å°è¯•ä»JavaScriptæ•°æ®ä¸­æå–
            data_match = re.search(r'window\.__DATA__\s*=\s*({.*?});', html, re.DOTALL)
            if data_match:
                import json
                data = json.loads(data_match.group(1))
                items = data.get('items', [])

                # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                logger.debug(f"   ğŸ“Š JavaScriptæ•°æ®è§£ææˆåŠŸ")
                logger.debug(f"      æ€»æ•°: {data.get('count', 0)}")
                logger.debug(f"      é”™è¯¯ä¿¡æ¯: {data.get('error_info', 'æ— ')}")
                logger.debug(f"      ç»“æœé¡¹æ•°: {len(items)}")
                logger.debug(f"      æœç´¢æ–‡æœ¬: {data.get('text', 'æœªçŸ¥')}")

                if len(items) == 0:
                    error_info = data.get('error_info', '')
                    if 'é¢‘ç¹' in error_info or 'é™åˆ¶' in error_info:
                        logger.error(f"   ğŸš« è±†ç“£é¢‘ç‡é™åˆ¶: {error_info}")
                        logger.warning(f"   ğŸ’¡ å»ºè®®ç­‰å¾…30åˆ†é’Ÿåå†å°è¯•æœç´¢")
                        # æŠ›å‡ºç‰¹å®šå¼‚å¸¸ï¼Œè®©ä¸Šå±‚åˆ‡æ¢ç­–ç•¥
                        raise Exception(f"è±†ç“£é¢‘ç‡é™åˆ¶: {error_info}")
                    else:
                        logger.warning(f"   âš ï¸ è±†ç“£è¿”å›0ä¸ªæœç´¢ç»“æœ")
                    logger.debug(f"      å®Œæ•´æ•°æ®: {data}")
                    return []

                for item in items[:5]:
                    try:
                        douban_id = str(item.get('id', ''))
                        title_text = item.get('title', '')

                        if douban_id and title_text:
                            anime_info = AnimeInfo(
                                title=title_text,
                                external_ids={WebsiteName.DOUBAN: douban_id}
                            )
                            results.append(anime_info)
                            logger.debug(f"      âœ… è§£ææˆåŠŸ: {title_text} (ID: {douban_id})")
                    except Exception as e:
                        logger.debug(f"è§£ææœç´¢ç»“æœé¡¹å¤±è´¥: {e}")
                        continue
            else:
                logger.warning(f"   âš ï¸ æœªæ‰¾åˆ°JavaScriptæ•°æ® window.__DATA__")
                # ä¿å­˜HTMLç”¨äºè°ƒè¯•
                import time
                debug_file = f"debug_no_data_{int(time.time())}.html"
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(html)
                logger.debug(f"      HTMLå·²ä¿å­˜åˆ°: {debug_file}")

            # å¦‚æœJavaScriptè§£æå¤±è´¥ï¼Œå°è¯•HTMLè§£æ
            if not results:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html, 'html.parser')

                result_items = soup.find_all('div', class_='result')
                for item in result_items[:5]:
                    try:
                        link = item.find('a', href=re.compile(r'/subject/\d+'))
                        if link:
                            href = link.get('href', '')
                            douban_id_match = re.search(r'/subject/(\d+)', href)

                            if douban_id_match:
                                douban_id = douban_id_match.group(1)
                                title_text = link.get_text(strip=True)

                                if title_text:
                                    anime_info = AnimeInfo(
                                        title=title_text,
                                        external_ids={WebsiteName.DOUBAN: douban_id}
                                    )
                                    results.append(anime_info)
                    except Exception as e:
                        logger.debug(f"è§£æHTMLç»“æœé¡¹å¤±è´¥: {e}")
                        continue

        except Exception as e:
            logger.debug(f"è§£ææœç´¢ç»“æœå¤±è´¥: {e}")

        return results

    async def search_anime_with_selenium(self, title: str) -> List[AnimeInfo]:
        """ä½¿ç”¨Seleniumæ¨¡æ‹Ÿæµè§ˆå™¨æœç´¢"""
        logger.info(f"ğŸ¤– ä½¿ç”¨Seleniumæœç´¢: {title}")

        try:
            from selenium import webdriver
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from selenium.webdriver.chrome.options import Options

            # é…ç½®Chromeé€‰é¡¹
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)

            # éšæœºUser-Agent
            chrome_options.add_argument(f'--user-agent={random.choice(self.user_agents)}')

            driver = webdriver.Chrome(options=chrome_options)
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            try:
                # è®¿é—®è±†ç“£æœç´¢é¡µé¢
                search_url = f"https://movie.douban.com/search?q={title}"
                driver.get(search_url)

                # ç­‰å¾…é¡µé¢åŠ è½½
                wait = WebDriverWait(driver, 10)

                # æŸ¥æ‰¾æœç´¢ç»“æœ
                results = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '.result')))

                anime_list = []
                for result in results[:5]:  # é™åˆ¶ç»“æœæ•°é‡
                    try:
                        # æå–åŠ¨æ¼«ä¿¡æ¯
                        title_element = result.find_element(By.CSS_SELECTOR, '.title a')
                        anime_title = title_element.text
                        anime_url = title_element.get_attribute('href')

                        # æå–è±†ç“£ID
                        douban_id = re.search(r'/subject/(\d+)/', anime_url)
                        if douban_id:
                            douban_id = douban_id.group(1)

                            # åˆ›å»ºAnimeInfoå¯¹è±¡
                            anime_info = AnimeInfo(
                                title=anime_title,
                                external_ids={WebsiteName.DOUBAN: douban_id}
                            )
                            anime_list.append(anime_info)

                    except Exception as e:
                        logger.warning(f"è§£ææœç´¢ç»“æœå¤±è´¥: {e}")
                        continue

                return anime_list

            finally:
                driver.quit()

        except ImportError:
            logger.warning("Seleniumæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æµè§ˆå™¨æ¨¡æ‹Ÿ")
            return []
        except Exception as e:
            logger.error(f"Seleniumæœç´¢å¤±è´¥: {e}")
            return []

    async def search_anime_alternative_sites(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """ä½¿ç”¨æ›¿ä»£ç½‘ç«™æœç´¢è±†ç“£ID"""
        logger.info(f"ğŸ”„ ä½¿ç”¨æ›¿ä»£ç½‘ç«™æœç´¢è±†ç“£ID: {title}")

        # å¤šç§æ›¿ä»£æœç´¢ç­–ç•¥
        strategies = [
            self._search_via_search_engines,
            self._search_via_mirror_sites,
            self._search_via_api_aggregators,
        ]

        for strategy in strategies:
            try:
                results = await strategy(session, title)
                if results:
                    logger.info(f"âœ… æ›¿ä»£ç½‘ç«™æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                    return results

                # ç­–ç•¥é—´å»¶è¿Ÿ
                await asyncio.sleep(random.uniform(2, 5))

            except Exception as e:
                logger.debug(f"æ›¿ä»£æœç´¢ç­–ç•¥å¤±è´¥: {e}")
                continue

        logger.warning("âŒ æ‰€æœ‰æ›¿ä»£ç½‘ç«™æœç´¢éƒ½å¤±è´¥äº†")
        return []

    async def _search_via_search_engines(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """é€šè¿‡æœç´¢å¼•æ“æœç´¢è±†ç“£é“¾æ¥"""
        search_engines = [
            {
                'name': 'Bing',
                'url': 'https://www.bing.com/search',
                'params': {'q': f'site:movie.douban.com {title}', 'count': 10}
            },
            {
                'name': 'DuckDuckGo',
                'url': 'https://duckduckgo.com/html/',
                'params': {'q': f'site:movie.douban.com {title}'}
            },
            {
                'name': 'Yandex',
                'url': 'https://yandex.com/search/',
                'params': {'text': f'site:movie.douban.com {title}'}
            }
        ]

        for engine in search_engines:
            try:
                logger.debug(f"ğŸ” å°è¯• {engine['name']} æœç´¢")

                headers = self._get_realistic_headers()
                headers['Accept'] = 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'

                response = await self._make_ultimate_request(
                    session, engine['url'],
                    params=engine['params'],
                    headers=headers
                )

                if response and 'text' in response:
                    # æå–è±†ç“£é“¾æ¥
                    douban_links = re.findall(
                        r'https?://movie\.douban\.com/subject/(\d+)/?',
                        response['text']
                    )

                    if douban_links:
                        results = []
                        for douban_id in douban_links[:3]:  # é™åˆ¶æ•°é‡
                            anime_info = AnimeInfo(
                                title=title,
                                external_ids={WebsiteName.DOUBAN: douban_id}
                            )
                            results.append(anime_info)

                        if results:
                            logger.info(f"âœ… {engine['name']} æ‰¾åˆ° {len(results)} ä¸ªè±†ç“£ID")
                            return results

                await asyncio.sleep(random.uniform(3, 7))

            except Exception as e:
                logger.debug(f"{engine['name']} æœç´¢å¤±è´¥: {e}")
                continue

        return []

    async def _search_via_mirror_sites(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """é€šè¿‡é•œåƒç«™ç‚¹æœç´¢"""
        # è±†ç“£é•œåƒç«™ç‚¹ï¼ˆæ³¨æ„ï¼šè¿™äº›å¯èƒ½ä¸ç¨³å®šï¼Œéœ€è¦å®šæœŸæ›´æ–°ï¼‰
        mirror_sites = [
            # ç¤ºä¾‹é•œåƒç«™ç‚¹ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ›¿æ¢ä¸ºçœŸå®å¯ç”¨çš„é•œåƒ
            # "https://douban-mirror1.com",
            # "https://douban-mirror2.com",
        ]

        if not mirror_sites:
            logger.debug("æœªé…ç½®é•œåƒç«™ç‚¹")
            return []

        for mirror_url in mirror_sites:
            try:
                logger.debug(f"ğŸª å°è¯•é•œåƒç«™ç‚¹: {mirror_url}")

                search_url = f"{mirror_url}/search"
                params = {'q': title, 'cat': '1002'}

                headers = self._get_realistic_headers()

                response = await self._make_ultimate_request(
                    session, search_url,
                    params=params,
                    headers=headers
                )

                if response and 'text' in response:
                    results = self._parse_search_results(response['text'], title)
                    if results:
                        logger.info(f"âœ… é•œåƒç«™ç‚¹æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                        return results

                await asyncio.sleep(random.uniform(3, 8))

            except Exception as e:
                logger.debug(f"é•œåƒç«™ç‚¹ {mirror_url} å¤±è´¥: {e}")
                continue

        return []

    async def _search_via_api_aggregators(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """é€šè¿‡APIèšåˆæœåŠ¡æœç´¢"""
        # ç¬¬ä¸‰æ–¹APIèšåˆæœåŠ¡ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰
        aggregators = [
            # ç¤ºä¾‹APIèšåˆæœåŠ¡
            # {
            #     'name': 'RapidAPI',
            #     'url': 'https://douban-api.rapidapi.com/search',
            #     'headers': {'X-RapidAPI-Key': 'your-api-key'}
            # }
        ]

        if not aggregators:
            logger.debug("æœªé…ç½®APIèšåˆæœåŠ¡")
            return []

        for aggregator in aggregators:
            try:
                logger.debug(f"ğŸ”— å°è¯•APIèšåˆ: {aggregator['name']}")

                headers = self._get_realistic_headers()
                headers.update(aggregator.get('headers', {}))

                params = {'q': title, 'type': 'movie'}

                response = await self._make_ultimate_request(
                    session, aggregator['url'],
                    params=params,
                    headers=headers,
                    is_ajax=True
                )

                if response:
                    results = self._parse_mobile_api_response(response, title)
                    if results:
                        logger.info(f"âœ… APIèšåˆæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                        return results

                await asyncio.sleep(random.uniform(2, 5))

            except Exception as e:
                logger.debug(f"APIèšåˆ {aggregator['name']} å¤±è´¥: {e}")
                continue

        return []

    def _build_optimized_search_terms(self, title: str, anime_info: Optional['AnimeInfo'] = None) -> List[str]:
        """æ„å»ºä¼˜åŒ–çš„æœç´¢è¯ï¼šä¼˜å…ˆä¸­æ–‡åï¼Œç„¶åæ—¥æ–‡åï¼ˆä¸ä½¿ç”¨ç®€åŒ–æ ‡é¢˜ï¼‰"""
        search_terms = []

        # å¦‚æœæœ‰AnimeInfoå¯¹è±¡ï¼Œä¼˜å…ˆä½¿ç”¨å…¶ä¸­çš„æ ‡é¢˜ä¿¡æ¯
        if anime_info:
            # 1. ä¸­æ–‡æ ‡é¢˜ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            if anime_info.title_chinese:
                search_terms.append(anime_info.title_chinese)
                logger.info(f"   ğŸ‡¨ğŸ‡³ ä½¿ç”¨ä¸­æ–‡æ ‡é¢˜: {anime_info.title_chinese}")

            # 2. æ—¥æ–‡æ ‡é¢˜ï¼ˆç¬¬äºŒä¼˜å…ˆçº§ï¼‰
            if anime_info.title_japanese:
                search_terms.append(anime_info.title_japanese)
                logger.info(f"   ğŸ‡¯ğŸ‡µ ä½¿ç”¨æ—¥æ–‡æ ‡é¢˜: {anime_info.title_japanese}")

        # 3. å¦‚æœæ²¡æœ‰ä¸­æ–‡å’Œæ—¥æ–‡æ ‡é¢˜ï¼Œä½¿ç”¨åŸå§‹æ ‡é¢˜ä½œä¸ºå¤‡ç”¨
        if not search_terms:
            search_terms.append(title)
            logger.info(f"   ğŸ“ ä½¿ç”¨åŸå§‹æ ‡é¢˜: {title}")

        # ç¡®ä¿æœ€å¤š2ä¸ªæœç´¢è¯ï¼šä¸­æ–‡ + æ—¥æ–‡ï¼ˆä¸åŒ…å«ç®€åŒ–ç‰ˆæœ¬ï¼‰
        final_terms = search_terms[:2]
        logger.info(f"   âœ… æœ€ç»ˆæœç´¢è¯: {final_terms}")

        return final_terms

    async def search_anime(self, session: aiohttp.ClientSession, title: str, anime_info: Optional['AnimeInfo'] = None) -> List[AnimeInfo]:
        """æœç´¢åŠ¨æ¼« - å¤šç­–ç•¥è‡ªåŠ¨åˆ‡æ¢ï¼šä¸»é¡µé¢æœç´¢ â†’ ç§»åŠ¨ç«¯API â†’ å¤‡ç”¨æ–¹æ³•"""
        logger.info(f"ğŸ” è±†ç“£ä¼˜åŒ–æœç´¢å¼€å§‹: {title}")

        # æ„å»ºæœç´¢è¯ï¼šä¼˜å…ˆæ—¥æ–‡ï¼Œç„¶åä¸­æ–‡ï¼Œæœ€åè‹±æ–‡
        search_terms = self._build_optimized_search_terms(title, anime_info)
        logger.info(f"ğŸ”¤ æœç´¢è¯ç­–ç•¥: {search_terms}")

        # æœç´¢ç­–ç•¥ï¼šåªä½¿ç”¨ç§»åŠ¨ç«¯APIï¼ˆç¨³å®šä¸”æœ‰å®Œæ•´è¯„åˆ†æ•°æ®ï¼‰
        logger.debug(f"ğŸš€ ä½¿ç”¨ç§»åŠ¨ç«¯APIç­–ç•¥ï¼ˆå”¯ä¸€ç­–ç•¥ï¼‰")

        # æŒ‰ä¼˜å…ˆçº§å°è¯•æ¯ä¸ªæœç´¢è¯
        for i, term in enumerate(search_terms, 1):
            logger.info(f"ğŸ¯ [{i}/{len(search_terms)}] æœç´¢è¯: '{term}' (ç­–ç•¥: ç§»åŠ¨ç«¯API)")

            try:
                # ä½¿ç”¨ç§»åŠ¨ç«¯APIæœç´¢
                results = await self._search_with_mobile_api_v2(session, term)

                if results:
                    # éªŒè¯æœç´¢ç»“æœ
                    validated_results = self._validate_search_results(results, term)
                    if validated_results:
                        logger.success(f"âœ… ç§»åŠ¨ç«¯APIæœç´¢æˆåŠŸ! æ‰¾åˆ° {len(validated_results)} ä¸ªæœ‰æ•ˆç»“æœ")
                        return validated_results

                # å¦‚æœæ²¡æ‰¾åˆ°ç»“æœï¼Œå°è¯•ä¸‹ä¸€ä¸ªæœç´¢è¯
                if i < len(search_terms):
                    logger.info("â³ ç­‰å¾…5ç§’åå°è¯•ä¸‹ä¸€ä¸ªæœç´¢è¯...")
                    await asyncio.sleep(5)

            except Exception as e:
                error_msg = str(e).lower()
                if 'é¢‘ç¹' in error_msg or 'é™åˆ¶' in error_msg or 'rate' in error_msg:
                    logger.warning(f"ğŸš« ç§»åŠ¨ç«¯APIé‡åˆ°é¢‘ç‡é™åˆ¶ï¼Œåœæ­¢æœç´¢")
                    break
                else:
                    logger.warning(f"âŒ ç§»åŠ¨ç«¯APIæœç´¢è¯ '{term}' å¤±è´¥: {e}")
                    # ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæœç´¢è¯
                    if i < len(search_terms):
                        await asyncio.sleep(5)

        logger.warning(f"âŒ ç§»åŠ¨ç«¯APIæœç´¢æœªæ‰¾åˆ°ç»“æœï¼Œè·³è¿‡: {title}")
        return []




    async def _search_with_homepage_form(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """ä½¿ç”¨ä¸»é¡µé¢æœç´¢è¡¨å• - æ¨¡æ‹Ÿè¡¨å•æäº¤"""
        logger.info(f"ğŸ¯ ä¸»é¡µé¢è¡¨å•æœç´¢: {title}")

        try:
            # ä½¿ç”¨å‘ç°çš„æœç´¢URL
            search_urls = [
                f"https://movie.douban.com/subject_search?search_text={title}",
                f"https://movie.douban.com/search?q={title}",
                f"https://search.douban.com/movie/subject_search?search_text={title}",
            ]

            for search_url in search_urls:
                try:
                    logger.debug(f"   ğŸ”— å°è¯•URL: {search_url}")

                    response = await self._make_ultimate_request(
                        session, search_url,
                        referer="https://movie.douban.com/"
                    )

                    if response and 'text' in response:
                        results = self._parse_search_results(response['text'], title)
                        if results:
                            logger.success(f"âœ… ä¸»é¡µé¢æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                            return results

                    # çŸ­æš‚å»¶è¿Ÿåå°è¯•ä¸‹ä¸€ä¸ªURL
                    await asyncio.sleep(random.uniform(2, 4))

                except Exception as e:
                    logger.debug(f"æœç´¢URLå¤±è´¥ {search_url}: {e}")
                    continue

            return []

        except Exception as e:
            logger.error(f"ä¸»é¡µé¢æœç´¢å¤±è´¥: {e}")
            return []

    async def _search_with_mobile_api_v2(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """ä½¿ç”¨ç§»åŠ¨ç«¯API v2 - åŸºäºæ–°å‘ç°çš„æ¥å£"""
        logger.info(f"ğŸ“± ç§»åŠ¨ç«¯API v2æœç´¢: {title}")

        try:
            # ä½¿ç”¨å‘ç°çš„ç§»åŠ¨ç«¯API
            api_url = "https://m.douban.com/rexxar/api/v2/search"
            params = {
                'q': title,
                'type': 'movie',
                'count': 10
            }

            # ç§»åŠ¨ç«¯è¯·æ±‚å¤´
            mobile_headers = self._get_realistic_headers(
                referer="https://m.douban.com/",
                is_ajax=True
            )

            # ä½¿ç”¨ç§»åŠ¨ç«¯User-Agent
            mobile_agents = [
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (Linux; Android 14; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Mobile Safari/537.36',
            ]
            mobile_headers['User-Agent'] = random.choice(mobile_agents)

            response = await self._make_ultimate_request(
                session, api_url,
                params=params,
                headers=mobile_headers,
                referer="https://m.douban.com/",
                is_ajax=True
            )

            if not response:
                return []

            # è§£æå“åº”
            if isinstance(response, dict) and 'text' in response:
                import json
                try:
                    data = json.loads(response['text'])
                except json.JSONDecodeError:
                    return []
            else:
                data = response

            if not isinstance(data, dict):
                return []

            # æå–æœç´¢ç»“æœ
            subjects = data.get('subjects', {})
            items = subjects.get('items', []) if isinstance(subjects, dict) else []

            # åˆ†ç±»å¤„ç†ç»“æœï¼šæŒ‰ç±»å‹ä¼˜å…ˆçº§æ’åº
            anime_movie_results = []    # ç”µå½±ç±»å‹
            anime_drama_results = []    # å‰§é›†ç±»å‹
            other_results = []          # å…¶ä»–ç±»å‹

            for item in items[:10]:  # å¤„ç†æ›´å¤šç»“æœ
                try:
                    if not isinstance(item, dict):
                        continue

                    target = item.get('target', {})
                    target_type = item.get('target_type', '')
                    type_name = item.get('type_name', '')

                    if isinstance(target, dict):
                        douban_id = str(target.get('id', ''))
                        title_text = target.get('title', '')
                        card_subtitle = target.get('card_subtitle', '')

                        if douban_id and title_text:
                            # æå–å¹´ä»½ä¿¡æ¯
                            year_str = target.get('year', '')
                            release_year = None
                            if year_str and year_str.isdigit():
                                release_year = int(year_str)

                            anime_info = AnimeInfo(
                                title=title_text,
                                external_ids={WebsiteName.DOUBAN: douban_id},
                                year=int(year_str) if year_str and year_str.isdigit() else None
                            )

                            # æå–è¯„åˆ†æ•°æ®
                            rating_info = target.get('rating', {})
                            if isinstance(rating_info, dict):
                                raw_score = rating_info.get('value', 0)
                                vote_count = rating_info.get('count', 0)

                                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„è¯„åˆ†
                                if raw_score and raw_score > 0 and vote_count > 0:
                                    # åˆ›å»ºè¯„åˆ†æ•°æ®å¯¹è±¡
                                    from ..models.anime import RatingData
                                    rating_data = RatingData(
                                        website=WebsiteName.DOUBAN,
                                        raw_score=float(raw_score),
                                        vote_count=int(vote_count)
                                    )

                                    anime_info._rating_data = rating_data
                                    logger.debug(f"   âœ… æ‰¾åˆ°: {title_text} (ID: {douban_id}, ç±»å‹: {target_type}) - è¯„åˆ†: {raw_score}, æŠ•ç¥¨: {vote_count:,}")
                                else:
                                    # æ£€æŸ¥æ— è¯„åˆ†åŸå› 
                                    null_reason = target.get('null_rating_reason', '')
                                    if null_reason:
                                        logger.debug(f"   âœ… æ‰¾åˆ°: {title_text} (ID: {douban_id}, ç±»å‹: {target_type}) - æ— è¯„åˆ†: {null_reason}")
                                    else:
                                        logger.debug(f"   âœ… æ‰¾åˆ°: {title_text} (ID: {douban_id}, ç±»å‹: {target_type}) - æ— è¯„åˆ†æ•°æ®")
                            else:
                                logger.debug(f"   âœ… æ‰¾åˆ°: {title_text} (ID: {douban_id}, ç±»å‹: {target_type}) - æ— è¯„åˆ†ä¿¡æ¯")

                            # æŒ‰ç±»å‹åˆ†ç±»ï¼Œä¼˜å…ˆmovieå’Œdramaç±»å‹ï¼ˆåŠ¨æ¼«é€šå¸¸æ˜¯è¿™ä¸¤ç§ï¼‰
                            if target_type == 'movie':
                                anime_movie_results.append((anime_info, year_str))
                            elif target_type == 'drama':
                                anime_drama_results.append((anime_info, year_str))
                            else:
                                other_results.append((anime_info, year_str))

                except Exception as e:
                    logger.debug(f"è§£æç§»åŠ¨ç«¯APIç»“æœé¡¹å¤±è´¥: {e}")
                    continue

            # åˆå¹¶æ‰€æœ‰ç»“æœå¹¶æŒ‰ä¼˜å…ˆçº§æ’åºï¼šç”µå½± > å‰§é›† > å…¶ä»–ç±»å‹
            all_results = anime_movie_results + anime_drama_results + other_results

            # æå–AnimeInfoå¯¹è±¡ï¼ˆå»æ‰å¹´ä»½ä¿¡æ¯ï¼‰
            results = [item[0] for item in all_results]

            # è®°å½•åˆ†ç±»ç»Ÿè®¡
            logger.debug(f"   ğŸ“Š ç»“æœåˆ†ç±»: ç”µå½±={len(anime_movie_results)}, å‰§é›†={len(anime_drama_results)}, å…¶ä»–={len(other_results)}")

            if results:
                logger.success(f"âœ… ç§»åŠ¨ç«¯API v2æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")

            return results

        except Exception as e:
            logger.error(f"ç§»åŠ¨ç«¯API v2æœç´¢å¤±è´¥: {e}")
            return []

    def _is_anime_related(self, card_subtitle: str, title: str, type_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºåŠ¨æ¼«ç›¸å…³å†…å®¹"""
        try:
            # åŠ¨æ¼«å…³é”®è¯
            anime_keywords = [
                'åŠ¨ç”»', 'åŠ¨æ¼«', 'ç•ªå‰§', 'æ¼«ç”»',
                'å¥‡å¹»', 'å†’é™©', 'é­”æ³•', 'å¼‚ä¸–ç•Œ',
                'æœºç”²', 'çƒ­è¡€', 'æ ¡å›­', 'æ‹çˆ±',
                'æ²»æ„ˆ', 'æ—¥å¸¸', 'æç¬‘', 'æ‚¬ç–‘'
            ]

            # åŠ¨æ¼«åˆ¶ä½œå…¬å¸/å¯¼æ¼”å…³é”®è¯
            anime_studios = [
                'å®«å´éª', 'æ–°æµ·è¯š', 'ä»Šæ•', 'æŠ¼äº•å®ˆ',
                'å‰åœåŠ›', 'äº¬éƒ½åŠ¨ç”»', 'MAPPA', 'WIT',
                'ä¸œæ˜ ', 'éª¨å¤´ç¤¾', 'A-1', 'P.A.WORKS'
            ]

            # æ£€æŸ¥card_subtitle
            if card_subtitle:
                subtitle_lower = card_subtitle.lower()
                for keyword in anime_keywords:
                    if keyword in card_subtitle:
                        return True
                for studio in anime_studios:
                    if studio in card_subtitle:
                        return True

            # æ£€æŸ¥æ ‡é¢˜
            if title:
                title_lower = title.lower()
                # å¸¸è§åŠ¨æ¼«æ ‡é¢˜æ¨¡å¼
                anime_title_patterns = [
                    'ç¬¬', 'å­£', 'OVA', 'OAD', 'å‰§åœºç‰ˆ',
                    'ä¹‹', 'ç‰©è¯­', 'ä¼ è¯´', 'æˆ˜è®°'
                ]
                for pattern in anime_title_patterns:
                    if pattern in title:
                        return True

            # æ£€æŸ¥type_name
            if type_name and 'åŠ¨ç”»' in type_name:
                return True

            return False

        except Exception as e:
            logger.debug(f"åŠ¨æ¼«ç›¸å…³æ€§åˆ¤æ–­å¤±è´¥: {e}")
            return False

    async def _search_with_backup_urls(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """ä½¿ç”¨å¤‡ç”¨æœç´¢URL"""
        logger.info(f"ğŸ”„ å¤‡ç”¨URLæœç´¢: {title}")

        try:
            # å…¶ä»–å‘ç°çš„æœç´¢URL
            backup_urls = [
                f"https://www.douban.com/search?q={title}&cat=1002",
            ]

            for url in backup_urls:
                try:
                    logger.debug(f"   ğŸ”— å°è¯•å¤‡ç”¨URL: {url}")

                    response = await self._make_ultimate_request(
                        session, url,
                        referer="https://www.douban.com/"
                    )

                    if response and 'text' in response:
                        results = self._parse_search_results(response['text'], title)
                        if results:
                            logger.success(f"âœ… å¤‡ç”¨URLæœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                            return results

                    await asyncio.sleep(random.uniform(2, 4))

                except Exception as e:
                    logger.debug(f"å¤‡ç”¨URLå¤±è´¥ {url}: {e}")
                    continue

            return []

        except Exception as e:
            logger.error(f"å¤‡ç”¨URLæœç´¢å¤±è´¥: {e}")
            return []

    async def _make_enhanced_request(self, session: aiohttp.ClientSession, url: str,
                                   method: str = "GET", headers: Optional[Dict[str, str]] = None,
                                   params: Optional[Dict[str, Any]] = None,
                                   referer: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """å¢å¼ºçš„è¯·æ±‚æ–¹æ³•ï¼Œå…¼å®¹æ—§æ¥å£"""
        return await self._make_ultimate_request(
            session, url, method=method, headers=headers,
            params=params, referer=referer
        )

    def _prepare_search_terms(self, title: str) -> List[str]:
        """å‡†å¤‡å¤šç§æœç´¢è¯å˜ä½“"""
        terms = [title]  # åŸå§‹æ ‡é¢˜

        # å»é™¤å¸¸è§åç¼€
        suffixes_to_remove = [
            'ç¬¬äºŒå­£', 'ç¬¬2å­£', 'ç¬¬ä¸‰å­£', 'ç¬¬3å­£', 'ç¬¬å››å­£', 'ç¬¬4å­£',
            'Season 2', 'Season 3', 'Season 4', 'S2', 'S3', 'S4',
            '2nd Season', '3rd Season', '4th Season',
            'II', 'III', 'IV', 'ç»­', 'æ–°ä½œ', 'å®Œç»“ç¯‡', 'æœ€ç»ˆå­£'
        ]

        for suffix in suffixes_to_remove:
            if suffix in title:
                simplified = title.replace(suffix, '').strip()
                if simplified and simplified not in terms:
                    terms.append(simplified)

        # å»é™¤æ‹¬å·å†…å®¹
        import re
        no_brackets = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', title).strip()
        if no_brackets and no_brackets not in terms:
            terms.append(no_brackets)

        # åªä¿ç•™å‰3ä¸ªæœ€æœ‰å¯èƒ½çš„æœç´¢è¯
        return terms[:3]

    def _validate_search_results(self, results: List[AnimeInfo], original_title: str) -> List[AnimeInfo]:
        """éªŒè¯æœç´¢ç»“æœçš„è´¨é‡"""
        if not results:
            return []

        validated = []
        original_lower = original_title.lower()

        for result in results:
            # åŸºæœ¬éªŒè¯ï¼šå¿…é¡»æœ‰è±†ç“£ID
            douban_id = result.external_ids.get(WebsiteName.DOUBAN)
            if not douban_id:
                continue

            # æ ‡é¢˜ç›¸ä¼¼æ€§éªŒè¯
            result_title = result.title.lower()

            # æ£€æŸ¥æ˜¯å¦åŒ…å«éæ‹‰ä¸å­—ç¬¦ï¼ˆå¦‚æ—¥æ–‡ã€ä¸­æ–‡ï¼‰
            has_non_latin = any(ord(c) > 127 for c in original_title)

            # å¯¹äºéæ‹‰ä¸å­—ç¬¦ï¼Œæ”¾å®½éªŒè¯æ¡ä»¶
            if has_non_latin:
                # å¯¹äºæ—¥æ–‡ç­‰å­—ç¬¦ï¼Œåªè¦æ‰¾åˆ°ç»“æœå°±è®¤ä¸ºæœ‰æ•ˆ
                # å› ä¸ºè±†ç“£çš„æœç´¢ç®—æ³•å·²ç»åšäº†åŒ¹é…
                validated.append(result)
                logger.debug(f"   âœ… éæ‹‰ä¸å­—ç¬¦æ ‡é¢˜éªŒè¯é€šè¿‡: {result.title}")
            else:
                # æ‹‰ä¸å­—ç¬¦ä½¿ç”¨ä¸¥æ ¼çš„ç›¸ä¼¼æ€§æ£€æŸ¥
                if (original_lower in result_title or
                    result_title in original_lower or
                    self._calculate_similarity(original_lower, result_title) > 0.6):
                    validated.append(result)
                    logger.debug(f"   âœ… æ‹‰ä¸å­—ç¬¦æ ‡é¢˜éªŒè¯é€šè¿‡: {result.title}")
                else:
                    logger.debug(f"   âŒ æ ‡é¢˜ç›¸ä¼¼æ€§ä¸è¶³: {original_title} vs {result.title}")

        logger.debug(f"   ğŸ“Š éªŒè¯ç»“æœ: {len(results)} -> {len(validated)}")
        return validated[:5]  # æœ€å¤šè¿”å›5ä¸ªç»“æœ

    def _calculate_similarity(self, str1: str, str2: str) -> float:
        """è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰"""
        if not str1 or not str2:
            return 0.0

        # è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ—é•¿åº¦
        len1, len2 = len(str1), len(str2)
        dp = [[0] * (len2 + 1) for _ in range(len1 + 1)]

        for i in range(1, len1 + 1):
            for j in range(1, len2 + 1):
                if str1[i-1] == str2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])

        lcs_length = dp[len1][len2]
        return lcs_length / max(len1, len2)

    async def _search_with_selenium_wrapper(self, title: str) -> List[AnimeInfo]:
        """Seleniumæœç´¢åŒ…è£…å™¨"""
        try:
            return await self.search_anime_with_selenium(title)
        except Exception as e:
            logger.warning(f"SeleniumåŒ…è£…å™¨é”™è¯¯: {e}")
            return []

    async def _try_original_search(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """å°è¯•åŸå§‹æœç´¢æ–¹æ³•"""
        search_url = "https://search.douban.com/movie/subject_search"
        params = {
            'search_text': title,
            'cat': '1002'  # åŠ¨æ¼«åˆ†ç±»
        }

        response = await self._make_enhanced_request(
            session, search_url, params=params,
            referer="https://movie.douban.com"
        )

        if not response or 'text' not in response:
            return []

        # è§£ææœç´¢ç»“æœ
        html = response['text']
        results = []

        try:
            # æŸ¥æ‰¾ window.__DATA__ ä¸­çš„æ•°æ®
            data_match = re.search(r'window\.__DATA__\s*=\s*({.*?});', html, re.DOTALL)
            if data_match:
                data = json.loads(data_match.group(1))
                items = data.get('items', [])

                # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                logger.debug(f"   ğŸ“Š JavaScriptæ•°æ®è§£ææˆåŠŸ")
                logger.debug(f"      æ€»æ•°: {data.get('count', 0)}")
                logger.debug(f"      é”™è¯¯ä¿¡æ¯: {data.get('error_info', 'æ— ')}")
                logger.debug(f"      ç»“æœé¡¹æ•°: {len(items)}")
                logger.debug(f"      æœç´¢æ–‡æœ¬: {data.get('text', 'æœªçŸ¥')}")

                if len(items) == 0:
                    logger.warning(f"   âš ï¸ è±†ç“£è¿”å›0ä¸ªæœç´¢ç»“æœ")
                    logger.debug(f"      å®Œæ•´æ•°æ®: {data}")
                    return []

                for item in items[:5]:
                    try:
                        douban_id = str(item.get('id', ''))
                        if douban_id:
                            anime_info = AnimeInfo(
                                title=item.get('title', title),
                                external_ids={WebsiteName.DOUBAN: douban_id}
                            )
                            results.append(anime_info)
                            logger.debug(f"      âœ… è§£ææˆåŠŸ: {item.get('title', title)} (ID: {douban_id})")
                    except Exception as e:
                        logger.warning(f"è§£ææœç´¢é¡¹å¤±è´¥: {e}")
                        continue
            else:
                logger.warning(f"   âš ï¸ æœªæ‰¾åˆ°JavaScriptæ•°æ® window.__DATA__")
                # ä¿å­˜HTMLç”¨äºè°ƒè¯•
                import time
                debug_file = f"debug_no_data_{int(time.time())}.html"
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(html)
                logger.debug(f"      HTMLå·²ä¿å­˜åˆ°: {debug_file}")

            return results

        except Exception as e:
            logger.error(f"è§£ææœç´¢å“åº”å¤±è´¥: {e}")
            return []

    async def get_anime_rating(self, session: aiohttp.ClientSession, anime_id: str) -> Optional[RatingData]:
        """è·å–åŠ¨æ¼«è¯„åˆ†æ•°æ® - ä¼˜å…ˆä½¿ç”¨ç§»åŠ¨ç«¯API"""

        # é¦–å…ˆå°è¯•ä½¿ç”¨ç§»åŠ¨ç«¯APIè·å–è¯„åˆ†ï¼ˆæ›´å¿«æ›´å‡†ç¡®ï¼‰
        try:
            rating_data = await self._get_rating_from_mobile_api(session, anime_id)
            if rating_data:
                logger.debug(f"âœ… é€šè¿‡ç§»åŠ¨ç«¯APIè·å–è¯„åˆ†: {anime_id}")
                return rating_data
        except Exception as e:
            logger.debug(f"ç§»åŠ¨ç«¯APIè·å–è¯„åˆ†å¤±è´¥: {e}")

        # å¤‡ç”¨æ–¹æ¡ˆï¼šä»é¡µé¢HTMLæå–è¯„åˆ†
        logger.debug(f"ğŸ”„ ä½¿ç”¨é¡µé¢è§£æè·å–è¯„åˆ†: {anime_id}")
        url = f"{self.base_url}/subject/{anime_id}/"

        response = await self._make_ultimate_request(
            session, url, referer=f"{self.base_url}"
        )

        if not response or 'text' not in response:
            return None

        try:
            rating_data = self._extract_rating_from_page(response['text'])
            if not rating_data:
                return None

            rating = RatingData(
                website=WebsiteName.DOUBAN,
                raw_score=rating_data['score'],
                vote_count=rating_data['vote_count'],
                score_distribution=rating_data['score_distribution'],
                site_mean=None,
                site_std=None,
                last_updated=datetime.now(),
                url=url
            )

            return rating

        except Exception as e:
            logger.error(f"è·å–è±†ç“£è¯„åˆ†å¤±è´¥ {anime_id}: {e}")
            return None

    async def _get_rating_from_mobile_api(self, session: aiohttp.ClientSession, anime_id: str) -> Optional[RatingData]:
        """é€šè¿‡ç§»åŠ¨ç«¯APIè·å–è¯„åˆ†æ•°æ®"""
        try:
            # æ„é€ ç§»åŠ¨ç«¯API URL
            api_url = f"https://m.douban.com/rexxar/api/v2/subject/{anime_id}"

            # ç§»åŠ¨ç«¯è¯·æ±‚å¤´
            mobile_headers = self._get_realistic_headers(
                referer="https://m.douban.com/",
                is_ajax=True
            )
            mobile_headers['User-Agent'] = 'Mozilla/5.0 (iPhone; CPU iPhone OS 17_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Mobile/15E148 Safari/604.1'

            response = await self._make_ultimate_request(
                session, api_url,
                headers=mobile_headers,
                referer="https://m.douban.com/",
                is_ajax=True
            )

            if not response:
                return None

            # è§£æJSONå“åº”
            if isinstance(response, dict) and 'text' in response:
                import json
                try:
                    data = json.loads(response['text'])
                except json.JSONDecodeError:
                    return None
            else:
                data = response

            if not isinstance(data, dict):
                return None

            # æå–è¯„åˆ†æ•°æ®
            rating_info = data.get('rating', {})
            if not isinstance(rating_info, dict):
                return None

            raw_score = rating_info.get('value', 0)
            vote_count = rating_info.get('count', 0)

            if raw_score <= 0:
                return None

            # åˆ›å»ºRatingDataå¯¹è±¡
            rating = RatingData(
                website=WebsiteName.DOUBAN,
                raw_score=float(raw_score),
                vote_count=int(vote_count),
                score_distribution={},  # ç§»åŠ¨ç«¯APIé€šå¸¸ä¸æä¾›è¯¦ç»†åˆ†å¸ƒ
                site_mean=None,
                site_std=None,
                last_updated=datetime.now(),
                url=f"https://movie.douban.com/subject/{anime_id}/"
            )

            return rating

        except Exception as e:
            logger.debug(f"ç§»åŠ¨ç«¯APIè·å–è¯„åˆ†å¼‚å¸¸: {e}")
            return None

    def _extract_rating_from_page(self, html: str) -> Optional[Dict[str, Any]]:
        """ä»è±†ç“£é¡µé¢æå–è¯„åˆ†ä¿¡æ¯ - å¢å¼ºç‰ˆ"""
        try:
            soup = BeautifulSoup(html, 'html.parser')

            # å¤šç§æ–¹å¼æŸ¥æ‰¾è¯„åˆ†
            raw_score = None
            vote_count = 0
            score_distribution = {}

            # æ–¹æ³•1: æ ‡å‡†è¯„åˆ†å…ƒç´ 
            rating_element = soup.find('strong', class_='ll rating_num')
            if rating_element:
                try:
                    raw_score = float(rating_element.text.strip())
                except ValueError:
                    pass

            # æ–¹æ³•2: å¤‡ç”¨è¯„åˆ†å…ƒç´ 
            if raw_score is None:
                rating_selectors = [
                    'span.rating_num',
                    '.rating_num strong',
                    '[property="v:average"]',
                    '.rating-info .rating_num'
                ]

                for selector in rating_selectors:
                    element = soup.select_one(selector)
                    if element:
                        try:
                            raw_score = float(element.text.strip())
                            break
                        except ValueError:
                            continue

            # æ–¹æ³•3: ä»JSONæ•°æ®ä¸­æå–
            if raw_score is None:
                json_match = re.search(r'"rating":\s*{\s*"average":\s*([0-9.]+)', html)
                if json_match:
                    try:
                        raw_score = float(json_match.group(1))
                    except ValueError:
                        pass

            if raw_score is None:
                logger.debug("æœªæ‰¾åˆ°è¯„åˆ†ä¿¡æ¯")
                return None

            # æŸ¥æ‰¾è¯„åˆ†äººæ•° - å¤šç§æ–¹å¼
            vote_selectors = [
                'a.rating_people',
                '.rating_people',
                '[property="v:votes"]',
                '.rating-info .rating_people'
            ]

            for selector in vote_selectors:
                element = soup.select_one(selector)
                if element:
                    vote_text = element.text
                    # æ”¯æŒä¸åŒæ ¼å¼çš„æ•°å­—
                    vote_patterns = [
                        r'(\d+)äººè¯„ä»·',
                        r'(\d+)\s*äºº',
                        r'(\d+)',
                        r'(\d+(?:,\d+)*)'  # æ”¯æŒé€—å·åˆ†éš”çš„æ•°å­—
                    ]

                    for pattern in vote_patterns:
                        vote_match = re.search(pattern, vote_text)
                        if vote_match:
                            try:
                                vote_str = vote_match.group(1).replace(',', '')
                                vote_count = int(vote_str)
                                break
                            except ValueError:
                                continue

                    if vote_count > 0:
                        break

            # æå–è¯„åˆ†åˆ†å¸ƒ
            score_distribution = self._extract_score_distribution(soup, vote_count)

            return {
                'score': raw_score,
                'vote_count': vote_count,
                'score_distribution': score_distribution
            }

        except Exception as e:
            logger.error(f"æå–è¯„åˆ†ä¿¡æ¯å¤±è´¥: {e}")
            return None

    def _extract_score_distribution(self, soup: BeautifulSoup, total_votes: int) -> Dict[str, int]:
        """æå–è¯„åˆ†åˆ†å¸ƒ"""
        distribution = {}

        try:
            # æŸ¥æ‰¾è¯„åˆ†åˆ†å¸ƒå…ƒç´ 
            rating_per_elements = soup.find_all('span', class_='rating_per')

            if len(rating_per_elements) == 5:
                # è±†ç“£æ˜¯5æ˜Ÿåˆ¶ï¼Œè½¬æ¢ä¸º10åˆ†åˆ¶
                star_labels = ['5æ˜Ÿ', '4æ˜Ÿ', '3æ˜Ÿ', '2æ˜Ÿ', '1æ˜Ÿ']
                score_values = [10, 8, 6, 4, 2]  # å¯¹åº”çš„10åˆ†åˆ¶åˆ†æ•°

                for i, (element, score) in enumerate(zip(rating_per_elements, score_values)):
                    try:
                        percent_text = element.text.strip().replace('%', '')
                        percent = float(percent_text)
                        count = int(total_votes * percent / 100) if total_votes > 0 else 0
                        distribution[str(score)] = count
                    except (ValueError, TypeError):
                        continue

            # å¤‡ç”¨æ–¹æ³•ï¼šä»CSSæˆ–å…¶ä»–å…ƒç´ æå–
            if not distribution:
                # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„åˆ†å¸ƒæ•°æ®
                distribution_elements = soup.select('.rating_betterthan, .rating_distribution')
                for element in distribution_elements:
                    # å°è¯•è§£æä¸åŒæ ¼å¼çš„åˆ†å¸ƒæ•°æ®
                    text = element.get_text()
                    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šè§£æé€»è¾‘
                    pass

        except Exception as e:
            logger.debug(f"æå–è¯„åˆ†åˆ†å¸ƒå¤±è´¥: {e}")

        return distribution

    async def get_anime_details(self, session: aiohttp.ClientSession, anime_id: str) -> Optional[AnimeInfo]:
        """è·å–åŠ¨æ¼«è¯¦ç»†ä¿¡æ¯"""
        url = f"{self.base_url}/subject/{anime_id}/"

        response = await self._make_ultimate_request(
            session, url, referer=f"{self.base_url}"
        )

        if not response or 'text' not in response:
            return None

        try:
            return self._extract_anime_info_from_page(response['text'], anime_id)
        except Exception as e:
            logger.error(f"è·å–è±†ç“£åŠ¨æ¼«è¯¦æƒ…å¤±è´¥ {anime_id}: {e}")
            return None

    def _extract_anime_info_from_page(self, html: str, douban_id: str) -> Optional[AnimeInfo]:
        """ä»è±†ç“£é¡µé¢æå–åŠ¨æ¼«ä¿¡æ¯"""
        soup = BeautifulSoup(html, 'html.parser')

        # æ ‡é¢˜
        title_element = soup.find('span', property='v:itemreviewed')
        title = title_element.text.strip() if title_element else ''

        # åŸºæœ¬ä¿¡æ¯
        info_element = soup.find('div', id='info')
        if not info_element:
            return AnimeInfo(
                title=title,
                external_ids={WebsiteName.DOUBAN: douban_id}
            )

        info_text = info_element.get_text()

        # è§£æåŸºæœ¬ä¿¡æ¯
        anime_type = None
        episodes = None
        start_date = None
        studios = []
        genres = []

        # æŸ¥æ‰¾ç±»å‹
        type_match = re.search(r'ç±»å‹:\s*([^\n]+)', info_text)
        if type_match:
            type_str = type_match.group(1).strip()
            anime_type = self._parse_anime_type(type_str)
            genres = [g.strip() for g in type_str.split('/') if g.strip()]

        # æŸ¥æ‰¾é›†æ•°
        episodes_match = re.search(r'é›†æ•°:\s*(\d+)', info_text)
        if episodes_match:
            episodes = int(episodes_match.group(1))

        # æŸ¥æ‰¾é¦–æ’­æ—¥æœŸ
        date_match = re.search(r'é¦–æ’­:\s*([^\n]+)', info_text)
        if date_match:
            start_date = self._parse_date(date_match.group(1).strip())

        # æŸ¥æ‰¾åˆ¶ä½œå…¬å¸
        studio_match = re.search(r'åˆ¶ç‰‡å›½å®¶/åœ°åŒº:\s*([^\n]+)', info_text)
        if studio_match:
            studios = [s.strip() for s in studio_match.group(1).split('/') if s.strip()]

        anime_info = AnimeInfo(
            title=title,
            anime_type=anime_type,
            episodes=episodes,
            start_date=start_date,
            studios=studios,
            genres=genres,
            external_ids={WebsiteName.DOUBAN: douban_id}
        )

        return anime_info

    def _parse_anime_type(self, douban_type: str) -> Optional[AnimeType]:
        """è§£æè±†ç“£åŠ¨æ¼«ç±»å‹"""
        if 'ç”µè§†' in douban_type or 'TV' in douban_type:
            return AnimeType.TV
        elif 'ç”µå½±' in douban_type or 'å‰§åœºç‰ˆ' in douban_type:
            return AnimeType.MOVIE
        elif 'OVA' in douban_type:
            return AnimeType.OVA
        elif 'ONA' in douban_type:
            return AnimeType.ONA
        elif 'ç‰¹åˆ«ç¯‡' in douban_type or 'ç‰¹å…¸' in douban_type:
            return AnimeType.SPECIAL
        else:
            return AnimeType.TV  # é»˜è®¤ä¸ºTV

    def _parse_date(self, date_str: str) -> Optional[date]:
        """è§£æè±†ç“£æ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return None

        # è±†ç“£æ—¥æœŸæ ¼å¼å¤šæ ·ï¼Œå°è¯•å¤šç§è§£ææ–¹å¼
        date_patterns = [
            r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2024-01-15
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # 2024å¹´1æœˆ15æ—¥
            r'(\d{4})å¹´(\d{1,2})æœˆ',  # 2024å¹´1æœˆ
            r'(\d{4})',  # 2024
        ]

        for pattern in date_patterns:
            match = re.search(pattern, date_str)
            if match:
                try:
                    groups = match.groups()
                    year = int(groups[0])
                    month = int(groups[1]) if len(groups) > 1 else 1
                    day = int(groups[2]) if len(groups) > 2 else 1
                    return date(year, month, day)
                except ValueError:
                    continue

        logger.warning(f"Failed to parse Douban date: {date_str}")
        return None

    async def get_seasonal_anime(self, session: aiohttp.ClientSession, season: str, year: int) -> List[AnimeInfo]:
        """è·å–å­£åº¦åŠ¨æ¼«åˆ—è¡¨ - è±†ç“£ä¸æä¾›æ­¤åŠŸèƒ½"""
        logger.warning("è±†ç“£ä¸æä¾›å­£åº¦åŠ¨æ¼«åˆ—è¡¨åŠŸèƒ½")
        return []

    async def get_site_statistics(self, session: aiohttp.ClientSession) -> Dict[str, Any]:
        """è·å–ç½‘ç«™ç»Ÿè®¡ä¿¡æ¯ - è±†ç“£ä¸æä¾›æ­¤åŠŸèƒ½"""
        logger.warning("è±†ç“£ä¸æä¾›ç½‘ç«™ç»Ÿè®¡ä¿¡æ¯")
        return {}

# æ³¨å†Œè±†ç“£å¢å¼ºçˆ¬è™«åˆ°å·¥å‚
from .base import ScraperFactory
ScraperFactory.register_scraper(WebsiteName.DOUBAN, DoubanEnhancedScraper)
