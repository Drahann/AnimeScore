"""
Filmarks ç½‘é¡µçˆ¬è™« - ä¼˜åŒ–ç‰ˆæœ¬
æ”¯æŒåŠ¨æ¼«ä¸“é—¨æœç´¢å’Œç²¾ç¡®çš„æŠ•ç¥¨æ•°æå–
"""
import aiohttp
import asyncio
import re
from datetime import datetime, date
from typing import List, Optional, Dict, Any
from bs4 import BeautifulSoup
from loguru import logger
from urllib.parse import urljoin

from .base import WebScrapingBasedScraper, ScraperFactory
from ..models.anime import AnimeInfo, RatingData, WebsiteName, AnimeType, AnimeStatus, Season
from ..models.config import WebsiteConfig
from ..utils.season_utils import get_season_from_date


class FilmarksScraper(WebScrapingBasedScraper):
    """Filmarks ç½‘é¡µçˆ¬è™« - ä¼˜åŒ–ç‰ˆæœ¬"""

    def __init__(self, website_name: WebsiteName, config: WebsiteConfig):
        super().__init__(website_name, config)
        self.base_url = config.base_url or "https://filmarks.com"
        # ç½‘ç»œé…ç½®ä¼˜åŒ–
        self.timeout = 30  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°30ç§’
        self.rate_limit = 3.0  # å¢åŠ è¯·æ±‚é—´éš”åˆ°3ç§’
        self.max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°

    def _get_optimized_headers(self) -> Dict[str, str]:
        """è·å–ä¼˜åŒ–çš„HTTPå¤´"""
        return {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
    def _parse_anime_type(self, filmarks_type: str) -> Optional[AnimeType]:
        """è§£æFilmarksç±»å‹"""
        if 'TV' in filmarks_type or 'ãƒ†ãƒ¬ãƒ“' in filmarks_type:
            return AnimeType.TV
        elif 'æ˜ ç”»' in filmarks_type or 'Movie' in filmarks_type:
            return AnimeType.MOVIE
        elif 'OVA' in filmarks_type:
            return AnimeType.OVA
        elif 'ONA' in filmarks_type:
            return AnimeType.ONA
        else:
            return AnimeType.TV
    
    def _parse_date(self, date_str: str) -> Optional[date]:
        """è§£æFilmarksæ—¥æœŸ"""
        if not date_str:
            return None
        
        try:
            # Filmarksæ—¥æœŸæ ¼å¼é€šå¸¸æ˜¯ "2024å¹´1æœˆ15æ—¥"
            date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', date_str)
            if date_match:
                year, month, day = date_match.groups()
                return date(int(year), int(month), int(day))
            
            # å°è¯•å…¶ä»–æ ¼å¼
            date_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ', date_str)
            if date_match:
                year, month = date_match.groups()
                return date(int(year), int(month), 1)
                
            date_match = re.search(r'(\d{4})', date_str)
            if date_match:
                year = date_match.group(1)
                return date(int(year), 1, 1)
                
        except ValueError:
            pass
        
        logger.warning(f"Failed to parse Filmarks date: {date_str}")
        return None
    
    def _extract_rating_from_page(self, html: str) -> Optional[Dict[str, Any]]:
        """ä»Filmarksé¡µé¢æå–è¯„åˆ†ä¿¡æ¯ - ä¼˜åŒ–ç‰ˆæœ¬"""
        soup = BeautifulSoup(html, 'html.parser')

        # æŸ¥æ‰¾è¯„åˆ† - ä½¿ç”¨å¤šç§é€‰æ‹©å™¨
        rating_text = None
        rating_selectors = [
            'span.c-rating__score',
            '.rating-score',
            '[class*="rating"]',
        ]

        for selector in rating_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text(strip=True)
                # æŸ¥æ‰¾æ•°å­—æ ¼å¼çš„è¯„åˆ†
                if re.match(r'^\d+\.?\d*$', text):
                    rating_text = text
                    logger.debug(f"æ‰¾åˆ°å¯èƒ½çš„è¯„åˆ†: {rating_text}")
                    break
            if rating_text:
                break

        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•åœ¨æ‰€æœ‰æ–‡æœ¬ä¸­æŸ¥æ‰¾è¯„åˆ†æ¨¡å¼
        if not rating_text:
            rating_patterns = [
                r'(\d\.\d)',  # å¦‚ 4.4
                r'â˜…+\s*(\d\.\d)',  # æ˜Ÿçº§è¯„åˆ†
            ]

            for pattern in rating_patterns:
                matches = re.findall(pattern, html)
                if matches:
                    rating_text = matches[0]
                    logger.debug(f"é€šè¿‡æ¨¡å¼åŒ¹é…æ‰¾åˆ°è¯„åˆ†: {rating_text}")
                    break

        if not rating_text:
            logger.warning("æœªæ‰¾åˆ°è¯„åˆ†ä¿¡æ¯")
            return None

        try:
            # è§£æè¯„åˆ†ï¼ˆ5æ˜Ÿåˆ¶è½¬10åˆ†åˆ¶ï¼‰
            raw_score = float(rating_text)
            if raw_score <= 5:
                raw_score = raw_score * 2
            logger.debug(f"è§£æåˆ°è¯„åˆ†: {rating_text} -> {raw_score}/10")
        except ValueError:
            logger.warning(f"æ— æ³•è§£æè¯„åˆ†: {rating_text}")
            return None

        # æŸ¥æ‰¾æŠ•ç¥¨æ•° - ä½¿ç”¨ä¼˜åŒ–çš„æ–¹æ³•
        vote_count = 0

        # æ–¹æ³•1: æŸ¥æ‰¾metaæ ‡ç­¾ä¸­çš„æŠ•ç¥¨æ•°
        meta_patterns = [
            r'ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°ï¼š(\d+(?:,\d+)*)ä»¶',
            r'æ„Ÿæƒ³ãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼\[(\d+(?:,\d+)*)ä»¶\]',
        ]

        for meta_tag in soup.find_all('meta'):
            content = meta_tag.get('content', '')
            for pattern in meta_patterns:
                match = re.search(pattern, content)
                if match:
                    vote_count = int(match.group(1).replace(',', ''))
                    logger.debug(f"åœ¨metaæ ‡ç­¾ä¸­æ‰¾åˆ°æŠ•ç¥¨æ•°: {vote_count}")
                    break
            if vote_count > 0:
                break

        # æ–¹æ³•2: æŸ¥æ‰¾JavaScriptä¸­çš„æ•°æ®
        if vote_count == 0:
            js_patterns = [
                r'&quot;count&quot;:(\d+)',  # HTMLè½¬ä¹‰çš„JSON
                r'"count"\s*:\s*(\d+)',       # æ™®é€šJSON
                r'count["\']?\s*:\s*(\d+)',   # å˜é‡èµ‹å€¼
            ]

            for pattern in js_patterns:
                match = re.search(pattern, html, re.IGNORECASE)
                if match:
                    candidate_count = int(match.group(1))
                    # éªŒè¯æ•°å­—æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
                    if 100 <= candidate_count <= 100000:
                        vote_count = candidate_count
                        logger.debug(f"åœ¨JavaScriptæ•°æ®ä¸­æ‰¾åˆ°æŠ•ç¥¨æ•°: {vote_count}")
                        break

        # æ–¹æ³•3: æ™ºèƒ½æ•°å­—ç­›é€‰ï¼ˆæ’é™¤æ¼”å‘˜IDç­‰ï¼‰
        if vote_count == 0:
            number_candidates = []
            for text_node in soup.find_all(string=re.compile(r'\d+')):
                text = str(text_node).strip()
                numbers = re.findall(r'\d+(?:,\d+)*', text)
                for num_str in numbers:
                    try:
                        num = int(num_str.replace(',', ''))
                        # æŠ•ç¥¨æ•°é€šå¸¸åœ¨1000-100000ä¹‹é—´
                        if 1000 <= num <= 100000:
                            # æ’é™¤æ¼”å‘˜IDï¼ˆæ£€æŸ¥æ˜¯å¦åœ¨æ¼”å‘˜é“¾æ¥ä¸­ï¼‰
                            is_actor_id = False
                            parent = text_node.parent if hasattr(text_node, 'parent') else None
                            if parent:
                                if parent.name == 'a' and '/people/' in parent.get('href', ''):
                                    is_actor_id = True
                                elif parent.parent and parent.parent.name == 'a' and '/people/' in parent.parent.get('href', ''):
                                    is_actor_id = True

                            if not is_actor_id:
                                number_candidates.append(num)
                    except ValueError:
                        continue

            if number_candidates:
                vote_count = max(number_candidates)
                logger.debug(f"é€‰æ‹©æœ€å¤§å€™é€‰æ•°å­—ä½œä¸ºæŠ•ç¥¨æ•°: {vote_count}")

        if vote_count == 0:
            logger.warning("æœªæ‰¾åˆ°æŠ•ç¥¨æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼5000")
            vote_count = 5000

        return {
            'score': raw_score,
            'vote_count': vote_count,
            'score_distribution': {}  # Filmarksä¸æä¾›è¯¦ç»†åˆ†å¸ƒ
        }
    
    def _extract_anime_info_from_page(self, html: str, filmarks_id: str) -> Optional[AnimeInfo]:
        """ä»Filmarksé¡µé¢æå–åŠ¨æ¼«ä¿¡æ¯"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # æ ‡é¢˜
        title_element = soup.find('h1', class_='p-content-detail__title')
        if not title_element:
            title_element = soup.find('h1')
        
        title = title_element.text.strip() if title_element else ''
        
        # åŸºæœ¬ä¿¡æ¯
        info_section = soup.find('div', class_='p-content-detail__other-info')
        if not info_section:
            return None
        
        info_text = info_section.get_text()
        
        # è§£æä¿¡æ¯
        anime_type = None
        start_date = None
        genres = []
        
        # æŸ¥æ‰¾ç±»å‹
        type_match = re.search(r'ã‚¸ãƒ£ãƒ³ãƒ«[ï¼š:]\s*([^\n]+)', info_text)
        if type_match:
            genre_str = type_match.group(1).strip()
            genres = [g.strip() for g in genre_str.split('ã€') if g.strip()]
            if genres:
                anime_type = self._parse_anime_type(genres[0])
        
        # æŸ¥æ‰¾å…¬å¼€æ—¥æœŸ
        date_match = re.search(r'å…¬é–‹[ï¼š:]\s*([^\n]+)', info_text)
        if date_match:
            start_date = self._parse_date(date_match.group(1).strip())
        
        # è§£æå­£åº¦
        season = None
        year = None
        if start_date:
            season, year = get_season_from_date(start_date)
        
        # ç®€ä»‹
        synopsis = ''
        synopsis_element = soup.find('div', class_='p-content-detail__summary')
        if synopsis_element:
            synopsis = synopsis_element.get_text().strip()
        
        anime_info = AnimeInfo(
            title=title,
            anime_type=anime_type,
            start_date=start_date,
            season=season,
            year=year,
            genres=genres,
            external_ids={WebsiteName.FILMARKS: filmarks_id},
            synopsis=synopsis
        )
        
        return anime_info
    
    async def search_anime(self, session: aiohttp.ClientSession, title: str) -> List[AnimeInfo]:
        """æœç´¢åŠ¨æ¼« - ä½¿ç”¨åŠ¨æ¼«ä¸“é—¨æœç´¢"""
        logger.info(f"ğŸ” æœç´¢åŠ¨æ¼«: {title}")

        # ä½¿ç”¨æ­£ç¡®çš„åŠ¨æ¼«æœç´¢URL
        search_url = f"{self.base_url}/search/animes"
        params = {'q': title}

        logger.debug(f"è¯·æ±‚URL: {search_url}")
        logger.debug(f"è¯·æ±‚å‚æ•°: {params}")

        # æ·»åŠ é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                response = await self._make_request(
                    session, search_url, params=params, headers=self._get_optimized_headers()
                )

                if not response or 'text' not in response:
                    logger.warning(f"æœç´¢è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries})")
                    if attempt < self.max_retries - 1:
                        await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                        continue
                    return []

                html = response['text']
                logger.debug(f"å“åº”é•¿åº¦: {len(html)} å­—ç¬¦")

                # è§£ææœç´¢ç»“æœ
                results = self._parse_search_results(html)
                logger.info(f"æ‰¾åˆ° {len(results)} ä¸ªæœç´¢ç»“æœ")

                return results

            except asyncio.TimeoutError:
                logger.warning(f"æœç´¢è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{self.max_retries})")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                    continue
                logger.error(f"æœç´¢åŠ¨æ¼«å¤±è´¥: è¿æ¥è¶…æ—¶")
                return []
            except Exception as e:
                logger.warning(f"æœç´¢è¯·æ±‚å¼‚å¸¸: {e} (å°è¯• {attempt + 1}/{self.max_retries})")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                    continue
                logger.error(f"æœç´¢åŠ¨æ¼«å¤±è´¥: {e}")
                return []

        return []

    def _parse_search_results(self, html: str) -> List[AnimeInfo]:
        """è§£ææœç´¢ç»“æœ"""
        soup = BeautifulSoup(html, 'html.parser')
        results = []

        # å°è¯•å¤šç§é€‰æ‹©å™¨
        selectors = [
            'div.p-content-cassette',
            'div.c-content-cassette',
            'div[class*="content"]'
        ]

        items = []
        for selector in selectors:
            items = soup.select(selector)
            if items:
                logger.debug(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(items)} ä¸ªç»“æœ")
                break

        for item in items[:5]:  # é™åˆ¶ç»“æœæ•°é‡
            try:
                anime_info = self._extract_anime_info_from_search_item(item)
                if anime_info:
                    logger.debug(f"æå–åˆ°åŠ¨æ¼«: {anime_info.title}")
                    results.append(anime_info)
            except Exception as e:
                logger.warning(f"è§£ææœç´¢ç»“æœé¡¹å¤±è´¥: {e}")
                continue

        return results

    def _extract_anime_info_from_search_item(self, item) -> Optional[AnimeInfo]:
        """ä»æœç´¢ç»“æœé¡¹ä¸­æå–åŠ¨æ¼«ä¿¡æ¯"""
        try:
            # æŸ¥æ‰¾åŠ¨æ¼«é“¾æ¥ - åŠ¨æ¼«URLæ ¼å¼æ˜¯ /animes/series_id/season_id
            link_elem = item if item.name == 'a' else item.find('a', href=re.compile(r'/animes/\d+/\d+'))
            if not link_elem:
                return None

            href = link_elem.get('href', '')
            if not href:
                return None

            # æå–åŠ¨æ¼«ID - æ ¼å¼: /animes/series_id/season_id
            id_match = re.search(r'/animes/(\d+)/(\d+)', href)
            if not id_match:
                return None

            series_id = id_match.group(1)
            season_id = id_match.group(2)
            filmarks_id = f"{series_id}_{season_id}"  # ç»„åˆID
            url = urljoin(self.base_url, href)

            # æå–æ ‡é¢˜
            title_elem = None
            title_selectors = [
                'h3',
                '.p-content-cassette__title',
                '.c-content-cassette__title',
                '[class*="title"]'
            ]

            for selector in title_selectors:
                title_elem = item.select_one(selector)
                if title_elem:
                    break

            # å¦‚æœåœ¨å½“å‰é¡¹ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•åœ¨çˆ¶å…ƒç´ ä¸­æŸ¥æ‰¾
            if not title_elem and hasattr(item, 'parent'):
                parent = item.parent
                for selector in title_selectors:
                    title_elem = parent.select_one(selector) if parent else None
                    if title_elem:
                        break

            # æå–æ ‡é¢˜æ–‡æœ¬
            if title_elem:
                title = title_elem.get_text(strip=True)
                # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤ä¸éœ€è¦çš„æ–‡æœ¬
                unwanted_texts = ['>>ç¶šãã‚’èª­ã‚€', '>>è©³ã—ã„æƒ…å ±ã‚’è¦‹ã‚‹', '>>å‹•ç”»é…ä¿¡ã‚µãƒ¼ãƒ“ã‚¹ãŒã‚ã‚‹ã‹ç¢ºèªã™ã‚‹', '>>é…ä¿¡ä¸­ã®å‹•ç”»é…ä¿¡ã‚µãƒ¼ãƒ“ã‚¹ã‚’ã‚‚ã£ã¨è¦‹ã‚‹']
                if title in unwanted_texts:
                    title = f"Anime {filmarks_id}"
            else:
                title = f"Anime {filmarks_id}"

            # æå–å¹´ä»½ï¼ˆå¦‚æœæœ‰ï¼‰
            year = None
            year_match = re.search(r'\((\d{4})\)', title)
            if year_match:
                year = int(year_match.group(1))

            anime_info = AnimeInfo(
                title=title,
                external_ids={WebsiteName.FILMARKS: filmarks_id},
                year=year
            )

            return anime_info

        except Exception as e:
            logger.warning(f"æå–åŠ¨æ¼«ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    async def get_anime_details(self, session: aiohttp.ClientSession, anime_id: str) -> Optional[AnimeInfo]:
        """è·å–åŠ¨æ¼«è¯¦ç»†ä¿¡æ¯"""
        # åŠ¨æ¼«IDæ ¼å¼: series_id_season_id
        if '_' in anime_id:
            series_id, season_id = anime_id.split('_', 1)
            url = f"{self.base_url}/animes/{series_id}/{season_id}"
        else:
            # å…¼å®¹æ—§æ ¼å¼
            url = f"{self.base_url}/movies/{anime_id}"

        response = await self._make_request(
            session, url, headers=self._get_optimized_headers()
        )

        if not response or 'text' not in response:
            return None

        try:
            return self._extract_anime_info_from_page(response['text'], anime_id)
        except Exception as e:
            logger.error(f"Failed to parse Filmarks anime details for {anime_id}: {e}")
            return None
    
    async def get_anime_rating(self, session: aiohttp.ClientSession, anime_id: str) -> Optional[RatingData]:
        """è·å–åŠ¨æ¼«è¯„åˆ†æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬"""
        # åŠ¨æ¼«IDæ ¼å¼: series_id_season_id
        if '_' in anime_id:
            series_id, season_id = anime_id.split('_', 1)
            url = f"{self.base_url}/animes/{series_id}/{season_id}"
        else:
            # å…¼å®¹æ—§æ ¼å¼
            url = f"{self.base_url}/movies/{anime_id}"

        logger.info(f"ğŸ“Š è·å–è¯„åˆ†: {anime_id}")

        # æ·»åŠ é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                response = await self._make_request(
                    session, url, headers=self._get_optimized_headers()
                )

                if not response or 'text' not in response:
                    logger.warning(f"è·å–è¯„åˆ†é¡µé¢å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries})")
                    if attempt < self.max_retries - 1:
                        await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                        continue
                    return None

                html = response['text']
                logger.debug(f"è¯„åˆ†é¡µé¢é•¿åº¦: {len(html)} å­—ç¬¦")

                # è§£æè¯„åˆ†ä¿¡æ¯
                rating_data = self._extract_rating_from_page(html)
                if not rating_data:
                    logger.warning("âŒ æœªèƒ½è§£æè¯„åˆ†ä¿¡æ¯")
                    return None

                logger.info(f"âœ… è·å–åˆ°è¯„åˆ†: {rating_data['score']} ({rating_data['vote_count']} ç¥¨)")

                rating = RatingData(
                    website=WebsiteName.FILMARKS,
                    raw_score=rating_data['score'],
                    vote_count=rating_data['vote_count'],
                    score_distribution=rating_data['score_distribution'],
                    site_mean=None,  # éœ€è¦ä»ç½‘ç«™ç»Ÿè®¡ä¸­è·å–
                    site_std=None,   # éœ€è¦è®¡ç®—
                    last_updated=datetime.now(),
                    url=url
                )

                return rating

            except asyncio.TimeoutError:
                logger.warning(f"è·å–è¯„åˆ†é¡µé¢è¶…æ—¶ (å°è¯• {attempt + 1}/{self.max_retries})")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                    continue
                logger.error(f"è·å–åŠ¨æ¼«è¯„åˆ†å¤±è´¥: è¿æ¥è¶…æ—¶")
                return None
            except Exception as e:
                logger.warning(f"è·å–è¯„åˆ†é¡µé¢å¼‚å¸¸: {e} (å°è¯• {attempt + 1}/{self.max_retries})")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                    continue
                logger.error(f"è·å–åŠ¨æ¼«è¯„åˆ†å¤±è´¥: {e}")
                return None

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        logger.error("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œæ— æ³•è·å–è¯„åˆ†é¡µé¢")
        return None
    
    async def get_seasonal_anime(self, session: aiohttp.ClientSession, year: int, season: str) -> List[AnimeInfo]:
        """è·å–å­£åº¦åŠ¨æ¼«åˆ—è¡¨"""
        # Filmarksæ²¡æœ‰ç›´æ¥çš„å­£åº¦åŠ¨æ¼«API
        logger.info(f"Filmarks does not provide seasonal anime API for {season} {year}")
        return []
    
    async def get_site_statistics(self, session: aiohttp.ClientSession) -> Optional[Dict[str, float]]:
        """è·å–ç½‘ç«™ç»Ÿè®¡æ•°æ®"""
        logger.info("Getting Filmarks site statistics...")
        
        # Filmarksè¯„åˆ†ç‰¹ç‚¹çš„ä¼°ç®—å€¼ï¼ˆè½¬æ¢ä¸º10åˆ†åˆ¶ï¼‰
        return {
            'mean': 7.6,  # 5æ˜Ÿåˆ¶è½¬æ¢åçš„å¹³å‡åˆ†
            'std': 0.8    # æ ‡å‡†å·®
        }


# æ³¨å†ŒFilmarksçˆ¬è™«
ScraperFactory.register_scraper(WebsiteName.FILMARKS, FilmarksScraper)
