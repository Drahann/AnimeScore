"""
åŠ¨æ¼«è¯„åˆ†åˆ†æå™¨ - æ•´åˆå„ç½‘ç«™æ•°æ®å¹¶è®¡ç®—ç»¼åˆè¯„åˆ†
"""
import asyncio
import aiohttp
from datetime import datetime, date
from typing import List, Dict, Optional, Tuple
from loguru import logger

from ..models.anime import AnimeScore, AnimeInfo, RatingData, SeasonalAnalysis, Season, WebsiteName
from ..models.config import Config
from ..scrapers.base import ScraperFactory
from ..utils.season_utils import get_current_season, get_season_date_range, is_anime_in_season
from ..utils.anime_filter import create_default_filter
from .scoring import ScoringEngine
from .data_completion import DataCompletion


class AnimeAnalyzer:
    """åŠ¨æ¼«è¯„åˆ†åˆ†æå™¨"""
    
    def __init__(self, config: Config):
        self.config = config
        self.scoring_engine = ScoringEngine(config)
        self.anime_filter = create_default_filter(config)
        self.scrapers = {}
        self._initialize_scrapers()
        self.data_completion = DataCompletion(config, self.scrapers)
    
    def _initialize_scrapers(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„çˆ¬è™«"""
        for website_name in self.config.get_enabled_websites():
            try:
                website_enum = WebsiteName(website_name)
                website_config = self.config.get_website_config(website_name)
                api_keys = self.config.api_keys.__dict__.get(website_name, {})
                
                scraper = ScraperFactory.create_scraper(
                    website_enum, website_config, api_keys
                )
                
                if scraper:
                    self.scrapers[website_enum] = scraper
                    logger.info(f"Initialized scraper for {website_name}")
                else:
                    logger.warning(f"Failed to initialize scraper for {website_name}")
                    
            except ValueError:
                logger.error(f"Unknown website: {website_name}")
            except Exception as e:
                logger.error(f"Error initializing scraper for {website_name}: {e}")
    
    async def get_seasonal_anime_list(self, season: Season, year: int) -> List[AnimeInfo]:
        """è·å–æŒ‡å®šå­£åº¦çš„åŠ¨æ¼«åˆ—è¡¨"""
        all_anime = []
        anime_dict = {}  # ç”¨äºå»é‡ï¼Œkeyä¸ºæ ‡é¢˜ï¼Œvalueä¸ºAnimeInfo
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            
            for website_name, scraper in self.scrapers.items():
                if scraper.is_enabled():
                    task = self._get_seasonal_anime_from_scraper(
                        session, scraper, year, season.value
                    )
                    tasks.append((website_name, task))
            
            # å¹¶å‘è·å–å„ç½‘ç«™æ•°æ®
            for website_name, task in tasks:
                try:
                    anime_list = await task
                    logger.info(f"Got {len(anime_list)} anime from {website_name}")
                    
                    for anime in anime_list:
                        # ç®€å•çš„å»é‡é€»è¾‘ï¼ŒåŸºäºæ ‡é¢˜
                        key = anime.title.lower().strip()
                        if key not in anime_dict:
                            anime_dict[key] = anime
                        else:
                            # åˆå¹¶å¤–éƒ¨IDå’Œå…¶ä»–ä¿¡æ¯
                            existing = anime_dict[key]
                            logger.debug(f"ğŸ”„ åˆå¹¶é‡å¤åŠ¨æ¼«: {anime.title} (æ¥è‡ª {website_name})")
                            existing.external_ids.update(anime.external_ids)

                            # åˆå¹¶æ ‡é¢˜ä¿¡æ¯
                            if anime.title_english and not existing.title_english:
                                logger.debug(f"   ğŸ“ æ·»åŠ è‹±æ–‡æ ‡é¢˜: {anime.title_english}")
                                existing.title_english = anime.title_english
                            if anime.title_japanese and not existing.title_japanese:
                                logger.debug(f"   ğŸ“ æ·»åŠ æ—¥æ–‡æ ‡é¢˜: {anime.title_japanese}")
                                existing.title_japanese = anime.title_japanese
                            if anime.title_chinese and not existing.title_chinese:
                                logger.info(f"   ğŸ‡¨ğŸ‡³ æ·»åŠ ä¸­æ–‡æ ‡é¢˜: '{existing.title}' -> '{anime.title_chinese}' (æ¥è‡ª {website_name})")
                                existing.title_chinese = anime.title_chinese
                            elif anime.title_chinese and existing.title_chinese:
                                logger.debug(f"   ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ ‡é¢˜å·²å­˜åœ¨: {existing.title_chinese}")
                            elif not anime.title_chinese:
                                logger.debug(f"   ğŸ‡¨ğŸ‡³ {website_name} æ— ä¸­æ–‡æ ‡é¢˜")

                            # åˆå¹¶å›¾ç‰‡ä¿¡æ¯
                            if anime.poster_image and not existing.poster_image:
                                logger.debug(f"   ğŸ–¼ï¸ æ·»åŠ æµ·æŠ¥å›¾ç‰‡: {anime.poster_image[:50]}...")
                                existing.poster_image = anime.poster_image
                            if anime.cover_image and not existing.cover_image:
                                existing.cover_image = anime.cover_image
                            if anime.banner_image and not existing.banner_image:
                                logger.debug(f"   ğŸ–¼ï¸ æ·»åŠ æ¨ªå¹…å›¾ç‰‡: {anime.banner_image[:50]}...")
                                existing.banner_image = anime.banner_image
                            
                except Exception as e:
                    logger.error(f"Error getting seasonal anime from {website_name}: {e}")
        
        all_anime = list(anime_dict.values())

        # å»é‡
        deduplicated_anime = self.anime_filter.deduplicate_anime(all_anime)

        # ç­›é€‰å­£åº¦æ–°ç•ª
        filtered_anime = self.anime_filter.filter_seasonal_anime(
            deduplicated_anime, season, year, self.config.seasonal.season_buffer_days
        )

        # æŒ‰å¼€å§‹æ—¥æœŸæ’åº
        sorted_anime = self.anime_filter.sort_by_start_date(filtered_anime)

        logger.info(f"Found {len(sorted_anime)} anime for {season.value} {year} "
                   f"after filtering and sorting")

        return sorted_anime
    
    async def _get_seasonal_anime_from_scraper(self, session: aiohttp.ClientSession,
                                             scraper, year: int, season: str) -> List[AnimeInfo]:
        """ä»å•ä¸ªçˆ¬è™«è·å–å­£åº¦åŠ¨æ¼«"""
        try:
            return await scraper.get_seasonal_anime(session, year, season)
        except Exception as e:
            logger.error(f"Error in scraper {scraper.website_name}: {e}")
            return []

    def _build_search_terms(self, anime: AnimeInfo) -> List[str]:
        """æ„å»ºæœç´¢è¯åˆ—è¡¨ï¼ˆä¼˜å…ˆæ—¥æ–‡æ ‡é¢˜ï¼‰- é€šç”¨æ–¹æ³•"""
        search_terms = []

        # 1. æ—¥æ–‡æ ‡é¢˜ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if anime.title_japanese:
            search_terms.append(anime.title_japanese)
            # å»æ‰ä¸€äº›å¯èƒ½çš„ä¿®é¥°è¯
            simplified_japanese = anime.title_japanese.replace('ç¬¬2æœŸ', '').replace('ç¬¬ï¼’æœŸ', '').replace('2nd Season', '').strip()
            if simplified_japanese != anime.title_japanese:
                search_terms.append(simplified_japanese)

        # 2. è‹±æ–‡æ ‡é¢˜
        if anime.title_english:
            search_terms.append(anime.title_english)

        # 3. åˆ«å
        if anime.alternative_titles:
            search_terms.extend(anime.alternative_titles[:2])  # åªå–å‰2ä¸ªåˆ«å

        # 4. åŸå§‹æ ‡é¢˜ï¼ˆç½—é©¬éŸ³ï¼‰
        search_terms.append(anime.title)

        # å»é‡å¹¶ä¿æŒé¡ºåº
        unique_search_terms = []
        for term in search_terms:
            if term and term not in unique_search_terms:
                unique_search_terms.append(term)

        return unique_search_terms

    async def improved_bangumi_search(self, session: aiohttp.ClientSession, anime_title: str, anilist_anime=None):
        """æ”¹è¿›çš„Bangumiæœç´¢ - ä¼˜å…ˆä½¿ç”¨æ—¥æ–‡æ ‡é¢˜"""
        bangumi_scraper = self.scrapers.get(WebsiteName.BANGUMI)
        if not bangumi_scraper:
            return None

        # å¦‚æœæ²¡æœ‰AniListæ•°æ®ï¼Œå…ˆè·å–
        if not anilist_anime:
            anilist_scraper = self.scrapers.get(WebsiteName.ANILIST)
            if anilist_scraper:
                anilist_results = await anilist_scraper.search_anime(session, anime_title)
                if anilist_results:
                    anilist_anime = anilist_results[0]

        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰AniListæ•°æ®ï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„AnimeInfoå¯¹è±¡
        if not anilist_anime:
            anilist_anime = AnimeInfo(title=anime_title)

        # ä½¿ç”¨é€šç”¨æœç´¢ç­–ç•¥
        search_terms = self._build_search_terms(anilist_anime)

        # é€ä¸ªå°è¯•æœç´¢Bangumi
        for search_term in search_terms:
            try:
                logger.debug(f"Trying Bangumi search with term: '{search_term}'")
                bangumi_results = await bangumi_scraper.search_anime(session, search_term)

                if bangumi_results:
                    logger.info(f"âœ… Bangumi search successful with term: '{search_term}'")
                    return bangumi_results[0]

            except Exception as e:
                logger.warning(f"Bangumi search error with term '{search_term}': {e}")

            # ç­‰å¾…é—´éš”
            await asyncio.sleep(0.5)

        logger.debug(f"Bangumi search failed for all terms: {search_terms}")
        return None
    
    async def collect_anime_ratings(self, anime_list: List[AnimeInfo]) -> List[AnimeScore]:
        """æ”¶é›†åŠ¨æ¼«è¯„åˆ†æ•°æ®"""
        anime_scores = []

        async with aiohttp.ClientSession() as session:
            for anime in anime_list:
                logger.info(f"Collecting ratings for: {anime.title}")

                anime_score = AnimeScore(anime_info=anime)

                # ä»å„ä¸ªç½‘ç«™æ”¶é›†è¯„åˆ†
                for website_name, scraper in self.scrapers.items():
                    if not scraper.is_enabled():
                        continue

                    # è·å–è¯¥ç½‘ç«™çš„åŠ¨æ¼«ID
                    anime_id = anime.external_ids.get(website_name)
                    if not anime_id:
                        # å¦‚æœæ²¡æœ‰ç›´æ¥çš„IDï¼Œå°è¯•æœç´¢ï¼ˆä½¿ç”¨ä¼˜åŒ–çš„æœç´¢ç­–ç•¥ï¼‰
                        anime_id = await self._search_anime_id_with_fallback(
                            session, scraper, anime
                        )
                        if anime_id:
                            anime.external_ids[website_name] = anime_id

                    if anime_id:
                        rating_data = await self._get_rating_from_scraper(
                            session, scraper, anime_id
                        )
                        if rating_data:
                            anime_score.add_or_update_rating(rating_data)

                anime_scores.append(anime_score)

        return anime_scores
    
    async def _search_anime_id(self, session: aiohttp.ClientSession,
                             scraper, title: str) -> Optional[str]:
        """æœç´¢åŠ¨æ¼«ID"""
        try:
            search_results = await scraper.search_anime(session, title)
            if search_results:
                # è¿”å›ç¬¬ä¸€ä¸ªç»“æœçš„ID
                first_result = search_results[0]
                return first_result.external_ids.get(scraper.website_name)
        except Exception as e:
            logger.warning(f"Search failed for '{title}' on {scraper.website_name}: {e}")

        return None

    async def _search_anime_id_with_fallback(self, session: aiohttp.ClientSession,
                                           scraper, anime: AnimeInfo) -> Optional[str]:
        """ä½¿ç”¨å¤šç§æœç´¢ç­–ç•¥æœç´¢åŠ¨æ¼«IDï¼ˆä¼˜å…ˆæ—¥æ–‡æ ‡é¢˜ï¼‰"""
        # ä½¿ç”¨é€šç”¨æœç´¢ç­–ç•¥
        search_terms = self._build_search_terms(anime)

        # é€ä¸ªå°è¯•æœç´¢
        for search_term in search_terms:
            try:
                logger.debug(f"Trying search term '{search_term}' on {scraper.website_name}")
                search_results = await scraper.search_anime(session, search_term)
                if search_results:
                    # è¿”å›ç¬¬ä¸€ä¸ªç»“æœçš„ID
                    first_result = search_results[0]
                    anime_id = first_result.external_ids.get(scraper.website_name)
                    if anime_id:
                        logger.info(f"âœ… Found anime ID '{anime_id}' using search term '{search_term}' on {scraper.website_name}")

                        # åˆå¹¶æœç´¢ç»“æœä¸­çš„åŠ¨æ¼«ä¿¡æ¯åˆ°åŸå§‹åŠ¨æ¼«å¯¹è±¡
                        self._merge_search_result_info(anime, first_result, scraper.website_name)

                        return anime_id
            except Exception as e:
                logger.warning(f"Search failed for '{search_term}' on {scraper.website_name}: {e}")
                continue

            # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
            await asyncio.sleep(0.5)

        logger.debug(f"Search failed for all terms on {scraper.website_name}: {search_terms}")
        return None

    def _merge_search_result_info(self, original_anime: AnimeInfo, search_result: AnimeInfo, website_name: WebsiteName):
        """åˆå¹¶æœç´¢ç»“æœä¸­çš„åŠ¨æ¼«ä¿¡æ¯åˆ°åŸå§‹åŠ¨æ¼«å¯¹è±¡"""
        # åˆå¹¶æ ‡é¢˜ä¿¡æ¯
        if search_result.title_english and not original_anime.title_english:
            logger.info(f"   ğŸ“ æ·»åŠ è‹±æ–‡æ ‡é¢˜: '{original_anime.title}' -> '{search_result.title_english}' (æ¥è‡ª {website_name.value})")
            original_anime.title_english = search_result.title_english

        if search_result.title_japanese and not original_anime.title_japanese:
            logger.info(f"   ğŸ“ æ·»åŠ æ—¥æ–‡æ ‡é¢˜: '{original_anime.title}' -> '{search_result.title_japanese}' (æ¥è‡ª {website_name.value})")
            original_anime.title_japanese = search_result.title_japanese

        if search_result.title_chinese and not original_anime.title_chinese:
            logger.info(f"   ğŸ‡¨ğŸ‡³ æ·»åŠ ä¸­æ–‡æ ‡é¢˜: '{original_anime.title}' -> '{search_result.title_chinese}' (æ¥è‡ª {website_name.value})")
            original_anime.title_chinese = search_result.title_chinese

        # åˆå¹¶å›¾ç‰‡ä¿¡æ¯
        if search_result.poster_image and not original_anime.poster_image:
            logger.debug(f"   ğŸ–¼ï¸ æ·»åŠ æµ·æŠ¥å›¾ç‰‡: {search_result.poster_image[:50]}...")
            original_anime.poster_image = search_result.poster_image

        if search_result.cover_image and not original_anime.cover_image:
            original_anime.cover_image = search_result.cover_image

        if search_result.banner_image and not original_anime.banner_image:
            logger.debug(f"   ğŸ–¼ï¸ æ·»åŠ æ¨ªå¹…å›¾ç‰‡: {search_result.banner_image[:50]}...")
            original_anime.banner_image = search_result.banner_image

        # åˆå¹¶å¤–éƒ¨ID
        for ext_website, ext_id in search_result.external_ids.items():
            if ext_website not in original_anime.external_ids:
                original_anime.external_ids[ext_website] = ext_id
                logger.debug(f"   ğŸ”— æ·»åŠ å¤–éƒ¨ID: {ext_website.value} -> {ext_id}")
    
    async def _get_rating_from_scraper(self, session: aiohttp.ClientSession,
                                     scraper, anime_id: str) -> Optional[RatingData]:
        """ä»çˆ¬è™«è·å–è¯„åˆ†æ•°æ®"""
        try:
            rating_data = await scraper.get_anime_rating(session, anime_id)
            # æ³¨æ„ï¼šsite_meanå’Œsite_stdå°†åœ¨åç»­çš„_calculate_seasonal_site_statisticsä¸­è®¾ç½®
            return rating_data
        except Exception as e:
            logger.error(f"Error getting rating from {scraper.website_name} "
                        f"for anime {anime_id}: {e}")
            return None
    
    def calculate_composite_scores(self, anime_scores: List[AnimeScore]) -> List[AnimeScore]:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        # é¦–å…ˆè®¡ç®—å­£åº¦å†…å„ç½‘ç«™çš„ç»Ÿè®¡æ•°æ®
        self._calculate_seasonal_site_statistics(anime_scores)

        valid_scores = []

        min_websites = self.config.model.weights.min_websites
        for anime_score in anime_scores:
            if anime_score.has_sufficient_data(min_websites):
                composite_score = self.scoring_engine.calculate_composite_score(anime_score)
                if composite_score:
                    anime_score.composite_score = composite_score
                    valid_scores.append(anime_score)
                else:
                    logger.warning(f"Failed to calculate composite score for "
                                 f"{anime_score.anime_info.title}")
            else:
                logger.info(f"Insufficient data for {anime_score.anime_info.title} "
                           f"(need {min_websites} websites)")

        # è®¡ç®—ç½‘ç«™æ’å
        self.scoring_engine.calculate_site_rankings(anime_scores)

        # æ’å
        ranked_scores = self.scoring_engine.rank_anime_list(valid_scores)

        logger.info(f"Successfully calculated composite scores for "
                   f"{len(ranked_scores)} out of {len(anime_scores)} anime")

        return ranked_scores

    def _calculate_seasonal_site_statistics(self, anime_scores: List[AnimeScore]):
        """
        è®¡ç®—å­£åº¦å†…å„ç½‘ç«™çš„ç»Ÿè®¡æ•°æ®

        æ ¹æ®é…ç½®å†³å®šä½¿ç”¨å­£åº¦åŠ¨æ€ç»Ÿè®¡è¿˜æ˜¯å›ºå®šç»Ÿè®¡æ•°æ®ã€‚
        """
        # æ£€æŸ¥é…ç½®çš„ç»Ÿè®¡æ–¹æ³•
        stats_config = self.config.model.site_statistics

        if stats_config.method == "fixed":
            # ä½¿ç”¨å›ºå®šç»Ÿè®¡æ•°æ®
            logger.info("Using fixed site statistics as configured")
            for anime_score in anime_scores:
                for rating in anime_score.ratings:
                    self._apply_fallback_statistics(rating)
            return

        # ä½¿ç”¨å­£åº¦åŠ¨æ€ç»Ÿè®¡
        from collections import defaultdict

        # æ”¶é›†å„ç½‘ç«™çš„è¯„åˆ†æ•°æ®
        website_scores = defaultdict(list)

        for anime_score in anime_scores:
            for rating in anime_score.ratings:
                if rating.raw_score is not None:
                    website_scores[rating.website].append(rating.raw_score)

        # è®¡ç®—å„ç½‘ç«™çš„ç»Ÿè®¡æ•°æ®
        site_statistics = {}
        min_samples = stats_config.min_seasonal_samples

        for website, scores in website_scores.items():
            if len(scores) >= min_samples:
                mean, std = self.scoring_engine.calculate_site_statistics(scores)
                site_statistics[website] = {'mean': mean, 'std': std}
                logger.info(f"Calculated seasonal statistics for {website.value}: "
                           f"mean={mean:.3f}, std={std:.3f} (from {len(scores)} anime)")
            else:
                logger.warning(f"Insufficient data for {website.value}: only {len(scores)} anime "
                             f"(need {min_samples}), using fallback statistics")

        # æ›´æ–°æ‰€æœ‰è¯„åˆ†æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
        for anime_score in anime_scores:
            for rating in anime_score.ratings:
                if rating.website in site_statistics:
                    stats = site_statistics[rating.website]
                    rating.site_mean = stats['mean']
                    rating.site_std = stats['std']
                    logger.debug(f"Updated site stats for {rating.website.value}: "
                               f"mean={stats['mean']:.3f}, std={stats['std']:.3f}")
                else:
                    # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„å­£åº¦æ•°æ®ï¼Œä½¿ç”¨å›é€€ç»Ÿè®¡æ•°æ®
                    self._apply_fallback_statistics(rating)

    def _apply_fallback_statistics(self, rating: RatingData):
        """
        ä¸ºæ²¡æœ‰è¶³å¤Ÿå­£åº¦æ•°æ®çš„ç½‘ç«™åº”ç”¨å›é€€ç»Ÿè®¡æ•°æ®

        è¿™äº›æ•°æ®åŸºäºå†å²ç»éªŒå’Œç½‘ç«™ç‰¹ç‚¹ï¼Œä½œä¸ºå­£åº¦æ•°æ®ä¸è¶³æ—¶çš„å¤‡é€‰æ–¹æ¡ˆã€‚
        """
        fallback_stats = {
            WebsiteName.BANGUMI: {'mean': 7.2, 'std': 0.8},
            WebsiteName.MAL: {'mean': 7.8, 'std': 0.6},
            WebsiteName.ANILIST: {'mean': 7.5, 'std': 0.9},
            WebsiteName.DOUBAN: {'mean': 8.0, 'std': 0.7},
            WebsiteName.IMDB: {'mean': 7.0, 'std': 1.0},
            WebsiteName.FILMARKS: {'mean': 7.3, 'std': 0.8}
        }

        if rating.website in fallback_stats:
            stats = fallback_stats[rating.website]
            rating.site_mean = stats['mean']
            rating.site_std = stats['std']
            logger.info(f"Applied fallback statistics for {rating.website.value}: "
                       f"mean={stats['mean']}, std={stats['std']}")
        else:
            # é€šç”¨é»˜è®¤å€¼
            rating.site_mean = 7.5
            rating.site_std = 0.8
            logger.warning(f"Using generic fallback statistics for {rating.website.value}")
    
    async def analyze_season_with_completion(self, season: Optional[Season] = None,
                                           year: Optional[int] = None,
                                           enable_completion: bool = True) -> SeasonalAnalysis:
        """åˆ†ææŒ‡å®šå­£åº¦ï¼ˆåŒ…å«æ•°æ®è¡¥å…¨ï¼‰"""
        if season is None or year is None:
            season, year = get_current_season()

        logger.info(f"ğŸŒ å¼€å§‹åˆ†æ {season.value} {year} (æ•°æ®è¡¥å…¨: {'å¯ç”¨' if enable_completion else 'ç¦ç”¨'})")

        # 1. è·å–å­£åº¦åŠ¨æ¼«åˆ—è¡¨
        anime_list = await self.get_seasonal_anime_list(season, year)

        # 2. æ”¶é›†è¯„åˆ†æ•°æ®ï¼ˆç¬¬ä¸€è½®ï¼‰
        anime_scores = await self.collect_anime_ratings(anime_list)
        logger.info(f"ğŸ“Š ç¬¬ä¸€è½®æ”¶é›†å®Œæˆï¼Œè·å¾— {len(anime_scores)} ä¸ªåŠ¨æ¼«æ•°æ®")

        # 3. æ•°æ®è¡¥å…¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if enable_completion:
            logger.info(f"ğŸ”§ å‡†å¤‡å¯åŠ¨æ•°æ®è¡¥å…¨ï¼Œenable_completion={enable_completion}")
            anime_scores = await self._perform_data_completion(anime_scores)
        else:
            logger.info(f"â­ï¸ è·³è¿‡æ•°æ®è¡¥å…¨ï¼Œenable_completion={enable_completion}")

        # 4. è®¡ç®—ç»¼åˆè¯„åˆ†
        ranked_scores = self.calculate_composite_scores(anime_scores)

        # 5. åˆ›å»ºåˆ†æç»“æœ
        analysis = SeasonalAnalysis(
            season=season,
            year=year,
            anime_scores=ranked_scores,
            total_anime_count=len(anime_list),
            analyzed_anime_count=len(ranked_scores)
        )
        
        logger.info(f"ğŸ‰ åˆ†æå®Œæˆ: {len(ranked_scores)} ä¸ªåŠ¨æ¼«å®Œæˆæ’å")

        return analysis

    async def analyze_season(self, season: Optional[Season] = None,
                           year: Optional[int] = None) -> SeasonalAnalysis:
        """åˆ†ææŒ‡å®šå­£åº¦ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼Œé»˜è®¤å¯ç”¨æ•°æ®è¡¥å…¨ï¼‰"""
        return await self.analyze_season_with_completion(season, year, enable_completion=True)

    async def _perform_data_completion(self, anime_scores: List[AnimeScore]) -> List[AnimeScore]:
        """æ‰§è¡Œæ•°æ®è¡¥å…¨"""
        logger.info("ğŸ” å¼€å§‹æ•°æ®è¡¥å…¨æµç¨‹...")

        # 1. è¯†åˆ«ç¼ºå¤±æ•°æ®
        missing_records = self.data_completion.identify_missing_data(anime_scores)

        if not missing_records:
            logger.info("âœ… æ‰€æœ‰åŠ¨æ¼«æ•°æ®å®Œæ•´ï¼Œæ— éœ€è¡¥å…¨")
            return anime_scores

        # 2. è¡¥å…¨ç¼ºå¤±æ•°æ®
        completed_data, completed_anime_info = await self.data_completion.complete_missing_data(missing_records)

        if not completed_data and not completed_anime_info:
            logger.info("âš ï¸ æœªèƒ½è¡¥å…¨ä»»ä½•æ•°æ®")
            return anime_scores

        # 3. åˆå¹¶è¡¥å…¨æ•°æ®
        merged_scores = self.data_completion.merge_completed_data(anime_scores, completed_data, completed_anime_info)

        # 4. é‡æ–°è®¡ç®—ç½‘ç«™ç»Ÿè®¡ï¼ˆå› ä¸ºæœ‰æ–°æ•°æ®ï¼‰
        self._recalculate_site_statistics(merged_scores)

        # 5. è¾“å‡ºè¡¥å…¨æ‘˜è¦
        summary = self.data_completion.get_completion_summary()
        logger.info(f"ğŸ“ˆ æ•°æ®è¡¥å…¨æ‘˜è¦:")
        logger.info(f"   - éœ€è¦è¡¥å…¨çš„åŠ¨æ¼«: {summary.get('total_anime_with_missing_data', 0)}")
        logger.info(f"   - æœç´¢å°è¯•æ¬¡æ•°: {summary.get('total_search_attempts', 0)}")
        logger.info(f"   - æˆåŠŸè¡¥å…¨æ¬¡æ•°: {summary.get('successful_completions', 0)}")
        logger.info(f"   - è¡¥å…¨æˆåŠŸç‡: {summary.get('success_rate', 0):.1f}%")

        return merged_scores

    def _recalculate_site_statistics(self, anime_scores: List[AnimeScore]):
        """é‡æ–°è®¡ç®—ç½‘ç«™ç»Ÿè®¡æ•°æ®"""
        logger.debug("ğŸ”„ é‡æ–°è®¡ç®—ç½‘ç«™ç»Ÿè®¡æ•°æ®...")

        # æ”¶é›†æ‰€æœ‰è¯„åˆ†æ•°æ®æŒ‰ç½‘ç«™åˆ†ç»„
        website_scores = {}

        for anime_score in anime_scores:
            for rating in anime_score.ratings:
                if rating.website not in website_scores:
                    website_scores[rating.website] = []
                website_scores[rating.website].append(rating.raw_score)

        # é‡æ–°è®¡ç®—æ¯ä¸ªç½‘ç«™çš„ç»Ÿè®¡æ•°æ®
        for website, scores in website_scores.items():
            if len(scores) >= 5:  # è‡³å°‘5ä¸ªæ ·æœ¬æ‰é‡æ–°è®¡ç®—
                mean_score = sum(scores) / len(scores)
                variance = sum((s - mean_score) ** 2 for s in scores) / len(scores)
                std_score = variance ** 0.5

                logger.debug(f"ğŸ“Š {website.value}: æ›´æ–°ç»Ÿè®¡ (æ ·æœ¬: {len(scores)}, å‡å€¼: {mean_score:.3f}, æ ‡å‡†å·®: {std_score:.3f})")

                # æ›´æ–°æ‰€æœ‰è¯¥ç½‘ç«™çš„è¯„åˆ†æ•°æ®ç»Ÿè®¡
                for anime_score in anime_scores:
                    for rating in anime_score.ratings:
                        if rating.website == website:
                            rating.site_mean = mean_score
                            rating.site_std = std_score

    def get_scraper_status(self) -> Dict[str, bool]:
        """è·å–çˆ¬è™«çŠ¶æ€"""
        return {
            name.value: scraper.is_enabled() 
            for name, scraper in self.scrapers.items()
        }
