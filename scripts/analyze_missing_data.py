#!/usr/bin/env python3
"""
åˆ†æåŠ¨æ¼«è¯„åˆ†ç»“æœï¼Œæ‰¾å‡ºç¼ºå¤±ç½‘ç«™æ•°æ®çš„åŠ¨æ¼«
è¾“å‡ºç¼ºå¤±æ•°æ®çš„åŠ¨æ¼«åˆ—è¡¨ï¼Œæ–¹ä¾¿äººå·¥æŸ¥æ‰¾å’Œæ·»åŠ 
"""

import json
import csv
import argparse
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set, Any
from collections import defaultdict

def load_analysis_result(file_path: str) -> Dict[str, Any]:
    """åŠ è½½åˆ†æç»“æœæ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        if file_path.endswith('.json'):
            return json.load(f)
        else:
            raise ValueError("ç›®å‰åªæ”¯æŒJSONæ ¼å¼çš„ç»“æœæ–‡ä»¶")

def load_enabled_websites(config_path: str = "config/config.yaml") -> Set[str]:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½å¯ç”¨çš„ç½‘ç«™åˆ—è¡¨ï¼ˆæ’é™¤æ•°æ®è¡¥å…¨æ’é™¤åˆ—è¡¨ä¸­çš„ç½‘ç«™ï¼‰"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        enabled_websites = set()
        websites_config = config.get('websites', {})

        # è·å–æ•°æ®è¡¥å…¨æ’é™¤åˆ—è¡¨
        data_completion_config = config.get('data_completion', {})
        excluded_websites = set(data_completion_config.get('excluded_websites', []))

        for website_name, website_config in websites_config.items():
            if website_config.get('enabled', False) and website_name not in excluded_websites:
                enabled_websites.add(website_name)

        print(f"ğŸ“Š æ•°æ®åˆ†æå¯ç”¨çš„ç½‘ç«™: {sorted(enabled_websites)}")
        if excluded_websites:
            print(f"ğŸ“Š æ•°æ®åˆ†ææ’é™¤çš„ç½‘ç«™: {sorted(excluded_websites)}")

        return enabled_websites
    except Exception as e:
        print(f"âš ï¸  æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_path}: {e}")
        print("ä½¿ç”¨é»˜è®¤çš„ç½‘ç«™åˆ—è¡¨: anilist, bangumi, filmarks, mal")
        return {'anilist', 'bangumi', 'filmarks', 'mal'}

def analyze_missing_data(data: Dict[str, Any], enabled_websites: Set[str] = None) -> Dict[str, Any]:
    """åˆ†æç¼ºå¤±çš„ç½‘ç«™æ•°æ®"""

    # å¦‚æœæ²¡æœ‰æä¾›å¯ç”¨çš„ç½‘ç«™åˆ—è¡¨ï¼Œä»é…ç½®æ–‡ä»¶åŠ è½½
    if enabled_websites is None:
        enabled_websites = load_enabled_websites()

    print(f"ğŸ“‹ å¯ç”¨çš„ç½‘ç«™: {', '.join(sorted(enabled_websites))}")

    # åªåˆ†æå¯ç”¨çš„ç½‘ç«™
    all_websites = enabled_websites
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_anime': len(data['rankings']),
        'website_coverage': defaultdict(int),
        'missing_patterns': defaultdict(int),
        'missing_anime': []
    }
    
    for anime in data['rankings']:
        # è·å–å½“å‰åŠ¨æ¼«æœ‰æ•°æ®çš„ç½‘ç«™
        current_websites = set()
        for rating in anime.get('ratings', []):
            current_websites.add(rating['website'])
        
        # ç»Ÿè®¡ç½‘ç«™è¦†ç›–ç‡
        for website in all_websites:
            if website in current_websites:
                stats['website_coverage'][website] += 1
        
        # æ‰¾å‡ºç¼ºå¤±çš„ç½‘ç«™
        missing_websites = all_websites - current_websites
        
        if missing_websites:
            # ç»Ÿè®¡ç¼ºå¤±æ¨¡å¼
            missing_pattern = ','.join(sorted(missing_websites))
            stats['missing_patterns'][missing_pattern] += 1
            
            # è®°å½•ç¼ºå¤±æ•°æ®çš„åŠ¨æ¼«
            anime_info = {
                'rank': anime['rank'],
                'title': anime['title'],
                'title_english': anime.get('title_english', ''),
                'anime_type': anime.get('anime_type', ''),
                'episodes': anime.get('episodes', ''),
                'start_date': anime.get('start_date', ''),
                'studios': ', '.join(anime.get('studios', [])),
                'genres': ', '.join(anime.get('genres', [])),
                'composite_score': anime.get('composite_score', 0),
                'total_votes': anime.get('total_votes', 0),
                'website_count': anime.get('website_count', 0),
                'current_websites': ', '.join(sorted(current_websites)),
                'missing_websites': ', '.join(sorted(missing_websites)),
                'missing_count': len(missing_websites),
                'confidence': anime.get('confidence', 0),
                'percentile': anime.get('percentile', 0)
            }
            
            # æ·»åŠ å„å¯ç”¨ç½‘ç«™çš„å…·ä½“ä¿¡æ¯
            for website in sorted(all_websites):
                anime_info[f'{website}_score'] = ''
                anime_info[f'{website}_votes'] = ''
                anime_info[f'{website}_url'] = ''
            
            # å¡«å…¥å·²æœ‰çš„ç½‘ç«™æ•°æ®
            for rating in anime.get('ratings', []):
                website = rating['website']
                anime_info[f'{website}_score'] = rating.get('raw_score', '')
                anime_info[f'{website}_votes'] = rating.get('vote_count', '')
                anime_info[f'{website}_url'] = rating.get('url', '')
            
            stats['missing_anime'].append(anime_info)
    
    return stats

def save_missing_data_csv(missing_anime: List[Dict], output_path: str, enabled_websites: Set[str]):
    """ä¿å­˜ç¼ºå¤±æ•°æ®åˆ°CSVæ–‡ä»¶"""
    if not missing_anime:
        print("æ²¡æœ‰ç¼ºå¤±æ•°æ®çš„åŠ¨æ¼«")
        return

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)

    # åŠ¨æ€ç”Ÿæˆå­—æ®µåï¼ŒåªåŒ…å«å¯ç”¨çš„ç½‘ç«™
    base_fieldnames = [
        'rank', 'title', 'title_english', 'anime_type', 'episodes', 'start_date',
        'studios', 'genres', 'composite_score', 'total_votes', 'website_count',
        'current_websites', 'missing_websites', 'missing_count', 'confidence', 'percentile'
    ]

    # ä¸ºæ¯ä¸ªå¯ç”¨çš„ç½‘ç«™æ·»åŠ å­—æ®µ
    website_fieldnames = []
    for website in sorted(enabled_websites):
        website_fieldnames.extend([
            f'{website}_score', f'{website}_votes', f'{website}_url'
        ])

    fieldnames = base_fieldnames + website_fieldnames

    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(missing_anime)

    print(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {output_path}")

def save_missing_data_json(stats: Dict, output_path: str, enabled_websites: Set[str]):
    """ä¿å­˜ç¼ºå¤±æ•°æ®åˆ°JSONæ–‡ä»¶"""
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)

    output_data = {
        'analysis_info': {
            'analysis_date': datetime.now().isoformat(),
            'total_anime': stats['total_anime'],
            'missing_anime_count': len(stats['missing_anime']),
            'enabled_websites': sorted(enabled_websites)
        },
        'website_coverage': dict(stats['website_coverage']),
        'missing_patterns': dict(stats['missing_patterns']),
        'missing_anime': stats['missing_anime']
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… JSONæ–‡ä»¶å·²ä¿å­˜: {output_path}")

def print_summary(stats: Dict):
    """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
    print("\nğŸ“Š ç¼ºå¤±æ•°æ®åˆ†ææ‘˜è¦")
    print("=" * 60)
    
    print(f"æ€»åŠ¨æ¼«æ•°é‡: {stats['total_anime']}")
    print(f"æœ‰ç¼ºå¤±æ•°æ®çš„åŠ¨æ¼«: {len(stats['missing_anime'])}")
    print(f"ç¼ºå¤±æ•°æ®æ¯”ä¾‹: {len(stats['missing_anime'])/stats['total_anime']*100:.1f}%")
    
    print(f"\nğŸŒ ç½‘ç«™è¦†ç›–ç‡:")
    for website, count in sorted(stats['website_coverage'].items()):
        coverage = count / stats['total_anime'] * 100
        print(f"  {website:10}: {count:3d}/{stats['total_anime']} ({coverage:5.1f}%)")
    
    print(f"\nâŒ ç¼ºå¤±æ¨¡å¼ (å‰10ä¸ª):")
    sorted_patterns = sorted(stats['missing_patterns'].items(), key=lambda x: x[1], reverse=True)
    for pattern, count in sorted_patterns[:10]:
        print(f"  ç¼ºå¤± {pattern:30}: {count:3d} ä¸ªåŠ¨æ¼«")
    
    print(f"\nğŸ” ç¼ºå¤±æœ€å¤šæ•°æ®çš„åŠ¨æ¼« (å‰10ä¸ª):")
    sorted_missing = sorted(stats['missing_anime'], key=lambda x: x['missing_count'], reverse=True)
    for anime in sorted_missing[:10]:
        print(f"  {anime['title']:40} | ç¼ºå¤± {anime['missing_count']} ä¸ªç½‘ç«™ | æ’å #{anime['rank']}")

def main():
    parser = argparse.ArgumentParser(description='åˆ†æåŠ¨æ¼«è¯„åˆ†ç»“æœä¸­çš„ç¼ºå¤±æ•°æ®')
    parser.add_argument('input_file', help='è¾“å…¥çš„ç»“æœæ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶å‰ç¼€ (é»˜è®¤: missing_data)')
    parser.add_argument('--csv-only', action='store_true', help='åªè¾“å‡ºCSVæ ¼å¼')
    parser.add_argument('--json-only', action='store_true', help='åªè¾“å‡ºJSONæ ¼å¼')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    input_path = Path(args.input_file)
    if not input_path.exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    print(f"ğŸ“‚ åŠ è½½åˆ†æç»“æœ: {input_path}")
    
    try:
        # åŠ è½½å¯ç”¨çš„ç½‘ç«™åˆ—è¡¨
        enabled_websites = load_enabled_websites()

        # åŠ è½½æ•°æ®
        data = load_analysis_result(str(input_path))

        # åˆ†æç¼ºå¤±æ•°æ®
        print("ğŸ” åˆ†æç¼ºå¤±æ•°æ®...")
        stats = analyze_missing_data(data, enabled_websites)
        
        # æ‰“å°æ‘˜è¦
        print_summary(stats)
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åå’Œè·¯å¾„
        if args.output:
            output_prefix = args.output
        else:
            # ä»è¾“å…¥æ–‡ä»¶åç”Ÿæˆè¾“å‡ºå‰ç¼€
            season_info = data.get('analysis_info', {})
            season = season_info.get('season', 'Unknown')
            year = season_info.get('year', 'Unknown')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_prefix = f"missing_data_{season}_{year}_{timestamp}"

        # åˆ›å»ºsearchlistç›®å½•
        searchlist_dir = Path("data/searchlist")
        searchlist_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ç»“æœåˆ°searchlistç›®å½•
        if not args.json_only:
            csv_path = searchlist_dir / f"{output_prefix}.csv"
            save_missing_data_csv(stats['missing_anime'], str(csv_path), enabled_websites)

        if not args.csv_only:
            json_path = searchlist_dir / f"{output_prefix}.json"
            save_missing_data_json(stats, str(json_path), enabled_websites)
        
        print(f"\nğŸ‰ åˆ†æå®Œæˆ! æ‰¾åˆ° {len(stats['missing_anime'])} ä¸ªæœ‰ç¼ºå¤±æ•°æ®çš„åŠ¨æ¼«")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
