#!/usr/bin/env python3
"""
ç”Ÿæˆäººå·¥æŸ¥æ‰¾æ¸…å•
æ ¹æ®ç¼ºå¤±æ•°æ®åˆ†æç»“æœï¼Œç”Ÿæˆä¾¿äºäººå·¥æŸ¥æ‰¾çš„åŠ¨æ¼«æ¸…å•
æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œæä¾›æœç´¢å…³é”®è¯å’Œç½‘ç«™é“¾æ¥
"""

import json
import csv
import argparse
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Set
from urllib.parse import quote

def generate_search_keywords(anime: Dict[str, Any]) -> List[str]:
    """ç”Ÿæˆæœç´¢å…³é”®è¯"""
    keywords = []
    
    # ä¸»æ ‡é¢˜
    if anime['title']:
        keywords.append(anime['title'])
    
    # è‹±æ–‡æ ‡é¢˜
    if anime['title_english'] and anime['title_english'] != anime['title']:
        keywords.append(anime['title_english'])
    
    # ç®€åŒ–æ ‡é¢˜ï¼ˆå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
    title_simple = anime['title'].replace(':', '').replace('-', ' ').replace('  ', ' ').strip()
    if title_simple != anime['title']:
        keywords.append(title_simple)
    
    # è‹±æ–‡æ ‡é¢˜ç®€åŒ–
    if anime['title_english']:
        english_simple = anime['title_english'].replace(':', '').replace('-', ' ').replace('  ', ' ').strip()
        if english_simple != anime['title_english'] and english_simple not in keywords:
            keywords.append(english_simple)
    
    return keywords

def load_enabled_websites(config_path: str = "config/config.yaml") -> Set[str]:
    """ä»é…ç½®æ–‡ä»¶åŠ è½½å¯ç”¨çš„ç½‘ç«™åˆ—è¡¨"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        enabled_websites = set()
        websites_config = config.get('websites', {})

        for website_name, website_config in websites_config.items():
            if website_config.get('enabled', False):
                enabled_websites.add(website_name)

        return enabled_websites
    except Exception as e:
        print(f"âš ï¸  æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_path}: {e}")
        print("ä½¿ç”¨é»˜è®¤çš„ç½‘ç«™åˆ—è¡¨: anilist, bangumi, filmarks, mal")
        return {'anilist', 'bangumi', 'filmarks', 'mal'}

def generate_search_urls(keywords: List[str], missing_websites: List[str]) -> Dict[str, str]:
    """ç”Ÿæˆæœç´¢URL"""
    urls = {}

    primary_keyword = keywords[0] if keywords else ""
    encoded_keyword = quote(primary_keyword)

    for website in missing_websites:
        if website == 'imdb':
            urls['imdb'] = f"https://www.imdb.com/find/?q={encoded_keyword}&s=tt&ttype=tv"
        elif website == 'douban':
            urls['douban'] = f"https://search.douban.com/movie/subject_search?search_text={encoded_keyword}"
        elif website == 'anilist':
            urls['anilist'] = f"https://anilist.co/search/anime?search={encoded_keyword}"
        elif website == 'bangumi':
            urls['bangumi'] = f"https://bgm.tv/subject_search/{encoded_keyword}?cat=2"
        elif website == 'filmarks':
            urls['filmarks'] = f"https://filmarks.com/search/animes?q={encoded_keyword}"
        elif website == 'mal':
            urls['mal'] = f"https://myanimelist.net/anime.php?q={encoded_keyword}"

    return urls

def calculate_priority(anime: Dict[str, Any]) -> float:
    """è®¡ç®—æŸ¥æ‰¾ä¼˜å…ˆçº§"""
    # åŸºç¡€åˆ†æ•°ï¼šæ’åè¶Šé«˜ä¼˜å…ˆçº§è¶Šé«˜
    rank_score = (100 - anime['rank']) / 100 * 50
    
    # æŠ•ç¥¨æ•°åˆ†æ•°ï¼šæŠ•ç¥¨è¶Šå¤šä¼˜å…ˆçº§è¶Šé«˜
    vote_score = min(anime['total_votes'] / 100000 * 30, 30)
    
    # ç½®ä¿¡åº¦åˆ†æ•°ï¼šç½®ä¿¡åº¦è¶Šé«˜ä¼˜å…ˆçº§è¶Šé«˜
    confidence_score = anime['confidence'] * 20
    
    # ç¼ºå¤±ç½‘ç«™æ•°æƒ©ç½šï¼šç¼ºå¤±è¶Šå¤šä¼˜å…ˆçº§è¶Šä½
    missing_penalty = anime['missing_count'] * 5
    
    return rank_score + vote_score + confidence_score - missing_penalty

def generate_search_list(missing_data_file: str, output_prefix: str = None):
    """ç”Ÿæˆäººå·¥æŸ¥æ‰¾æ¸…å•"""

    # åŠ è½½å¯ç”¨çš„ç½‘ç«™åˆ—è¡¨
    enabled_websites = load_enabled_websites()
    print(f"ğŸ“‹ å¯ç”¨çš„ç½‘ç«™: {', '.join(sorted(enabled_websites))}")

    # åŠ è½½ç¼ºå¤±æ•°æ®
    with open(missing_data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    missing_anime = data['missing_anime']
    
    # è®¡ç®—ä¼˜å…ˆçº§å¹¶æ’åº
    for anime in missing_anime:
        anime['priority'] = calculate_priority(anime)
        anime['search_keywords'] = generate_search_keywords(anime)
        missing_websites = anime['missing_websites'].split(', ')
        anime['search_urls'] = generate_search_urls(anime['search_keywords'], missing_websites)
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    missing_anime.sort(key=lambda x: x['priority'], reverse=True)
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    if not output_prefix:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_prefix = f"search_list_{timestamp}"

    # åˆ›å»ºsearchlistç›®å½•
    searchlist_dir = Path("data/searchlist")
    searchlist_dir.mkdir(parents=True, exist_ok=True)

    # ç”ŸæˆCSVæ¸…å•
    csv_path = searchlist_dir / f"{output_prefix}.csv"
    generate_csv_search_list(missing_anime, str(csv_path), enabled_websites)

    # ç”ŸæˆJSONæ¸…å•
    json_path = searchlist_dir / f"{output_prefix}.json"
    generate_json_search_list(missing_anime, data['analysis_info'], str(json_path))

    # ç”ŸæˆæŒ‰ç½‘ç«™åˆ†ç»„çš„æ¸…å•
    website_csv_path = searchlist_dir / f"{output_prefix}_by_website.csv"
    generate_website_grouped_list(missing_anime, str(website_csv_path), enabled_websites)
    
    print(f"\nğŸ‰ æŸ¥æ‰¾æ¸…å•ç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“‹ ä¸»è¦æ¸…å•: {csv_path}")
    print(f"ğŸ“‹ è¯¦ç»†æ•°æ®: {json_path}")
    print(f"ğŸ“‹ æŒ‰ç½‘ç«™åˆ†ç»„: {website_csv_path}")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print_search_statistics(missing_anime)

def generate_csv_search_list(missing_anime: List[Dict], output_path: str, enabled_websites: Set[str]):
    """ç”ŸæˆCSVæ ¼å¼çš„æŸ¥æ‰¾æ¸…å•"""
    base_fieldnames = [
        'priority', 'rank', 'title', 'title_english', 'anime_type', 'start_date',
        'missing_websites', 'missing_count', 'composite_score', 'total_votes', 'confidence',
        'search_keyword_1', 'search_keyword_2', 'search_keyword_3'
    ]

    # ä¸ºæ¯ä¸ªå¯ç”¨çš„ç½‘ç«™æ·»åŠ æœç´¢URLå­—æ®µ
    search_url_fieldnames = []
    for website in sorted(enabled_websites):
        search_url_fieldnames.append(f'{website}_search_url')

    end_fieldnames = ['current_websites', 'studios', 'genres']
    fieldnames = base_fieldnames + search_url_fieldnames + end_fieldnames
    
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for anime in missing_anime:
            row = {
                'priority': f"{anime['priority']:.1f}",
                'rank': anime['rank'],
                'title': anime['title'],
                'title_english': anime['title_english'],
                'anime_type': anime['anime_type'],
                'start_date': anime['start_date'],
                'missing_websites': anime['missing_websites'],
                'missing_count': anime['missing_count'],
                'composite_score': f"{anime['composite_score']:.3f}",
                'total_votes': anime['total_votes'],
                'confidence': f"{anime['confidence']:.3f}",
                'current_websites': anime['current_websites'],
                'studios': anime['studios'],
                'genres': anime['genres']
            }
            
            # æ·»åŠ æœç´¢å…³é”®è¯
            keywords = anime['search_keywords']
            for i in range(3):
                key = f'search_keyword_{i+1}'
                row[key] = keywords[i] if i < len(keywords) else ''
            
            # æ·»åŠ æœç´¢URLï¼ˆåªåŒ…å«å¯ç”¨çš„ç½‘ç«™ï¼‰
            search_urls = anime['search_urls']
            for website in sorted(enabled_websites):
                row[f'{website}_search_url'] = search_urls.get(website, '')
            
            writer.writerow(row)

def generate_json_search_list(missing_anime: List[Dict], analysis_info: Dict, output_path: str):
    """ç”ŸæˆJSONæ ¼å¼çš„æŸ¥æ‰¾æ¸…å•"""
    output_data = {
        'analysis_info': analysis_info,
        'generation_date': datetime.now().isoformat(),
        'total_missing_anime': len(missing_anime),
        'search_list': missing_anime
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

def generate_website_grouped_list(missing_anime: List[Dict], output_path: str, enabled_websites: Set[str]):
    """ç”ŸæˆæŒ‰ç½‘ç«™åˆ†ç»„çš„æŸ¥æ‰¾æ¸…å•"""
    # æŒ‰ç¼ºå¤±ç½‘ç«™åˆ†ç»„
    website_groups = {}
    for anime in missing_anime:
        missing_websites = anime['missing_websites']
        if missing_websites not in website_groups:
            website_groups[missing_websites] = []
        website_groups[missing_websites].append(anime)
    
    fieldnames = [
        'missing_websites', 'priority', 'rank', 'title', 'title_english', 
        'search_keyword_1', 'search_keyword_2', 'composite_score', 'total_votes'
    ]
    
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        # æŒ‰ç¼ºå¤±ç½‘ç«™æ•°é‡æ’åº
        sorted_groups = sorted(website_groups.items(), key=lambda x: len(x[1]), reverse=True)
        
        for missing_websites, anime_list in sorted_groups:
            for anime in anime_list:
                keywords = anime['search_keywords']
                row = {
                    'missing_websites': missing_websites,
                    'priority': f"{anime['priority']:.1f}",
                    'rank': anime['rank'],
                    'title': anime['title'],
                    'title_english': anime['title_english'],
                    'search_keyword_1': keywords[0] if len(keywords) > 0 else '',
                    'search_keyword_2': keywords[1] if len(keywords) > 1 else '',
                    'composite_score': f"{anime['composite_score']:.3f}",
                    'total_votes': anime['total_votes']
                }
                writer.writerow(row)

def print_search_statistics(missing_anime: List[Dict]):
    """æ‰“å°æŸ¥æ‰¾ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\nğŸ“Š æŸ¥æ‰¾æ¸…å•ç»Ÿè®¡")
    print("=" * 50)
    
    # æŒ‰ç¼ºå¤±ç½‘ç«™åˆ†ç»„ç»Ÿè®¡
    website_stats = {}
    for anime in missing_anime:
        missing_websites = anime['missing_websites']
        if missing_websites not in website_stats:
            website_stats[missing_websites] = 0
        website_stats[missing_websites] += 1
    
    print("ğŸ” æŒ‰ç¼ºå¤±ç½‘ç«™åˆ†ç»„:")
    for missing_websites, count in sorted(website_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"  ç¼ºå¤± {missing_websites:30}: {count:3d} ä¸ªåŠ¨æ¼«")
    
    # é«˜ä¼˜å…ˆçº§åŠ¨æ¼«
    high_priority = [a for a in missing_anime if a['priority'] >= 70]
    print(f"\nâ­ é«˜ä¼˜å…ˆçº§åŠ¨æ¼« (ä¼˜å…ˆçº§ >= 70): {len(high_priority)} ä¸ª")
    for anime in high_priority[:5]:
        print(f"  #{anime['rank']:2d} {anime['title']:40} (ä¼˜å…ˆçº§: {anime['priority']:.1f})")

def main():
    parser = argparse.ArgumentParser(description='ç”Ÿæˆäººå·¥æŸ¥æ‰¾æ¸…å•')
    parser.add_argument('missing_data_file', help='ç¼ºå¤±æ•°æ®JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶å‰ç¼€')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not Path(args.missing_data_file).exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.missing_data_file}")
        return
    
    print(f"ğŸ“‚ åŠ è½½ç¼ºå¤±æ•°æ®: {args.missing_data_file}")
    
    try:
        generate_search_list(args.missing_data_file, args.output)
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
