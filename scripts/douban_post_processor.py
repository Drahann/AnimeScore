#!/usr/bin/env python3
"""
è±†ç“£æ•°æ®åå¤„ç†å™¨
åœ¨ä¸»è¦åˆ†æå®Œæˆåï¼Œç‹¬ç«‹è¿è¡Œè±†ç“£çˆ¬è™«è¡¥å……æ•°æ®
"""
import asyncio
import json
import sys
import csv
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Any
import click
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.scrapers.douban_enhanced import DoubanEnhancedScraper
from src.models.anime import WebsiteName, RatingData
from src.models.config import Config, WebsiteConfig
from src.core.scoring import ScoringEngine
import aiohttp


class DoubanPostProcessor:
    """è±†ç“£æ•°æ®åå¤„ç†å™¨"""
    
    def __init__(self, config: Config):
        self.config = config
        self.scoring_engine = ScoringEngine(config)
        
        # åˆ›å»ºè±†ç“£çˆ¬è™«é…ç½® - æ›´ä¿å®ˆçš„è®¾ç½®
        douban_config = WebsiteConfig(
            enabled=True,
            base_url="https://movie.douban.com",
            rate_limit=15.0,  # 15ç§’é—´éš”ï¼Œéå¸¸ä¿å®ˆ
            timeout=60        # æ›´é•¿çš„è¶…æ—¶æ—¶é—´
        )
        
        self.douban_scraper = DoubanEnhancedScraper(WebsiteName.DOUBAN, douban_config)
        
    def find_latest_results(self, results_dir: Path) -> Optional[Path]:
        """æŸ¥æ‰¾æœ€æ–°çš„åˆ†æç»“æœæ–‡ä»¶"""
        # ä¼˜å…ˆæŸ¥æ‰¾JSONæ–‡ä»¶
        json_files = list(results_dir.glob("anime_ranking_*.json"))

        if json_files:
            latest_file = max(json_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"æ‰¾åˆ°æœ€æ–°JSONç»“æœæ–‡ä»¶: {latest_file}")
            return latest_file

        # å¦‚æœæ²¡æœ‰JSONæ–‡ä»¶ï¼ŒæŸ¥æ‰¾CSVæ–‡ä»¶
        csv_files = list(results_dir.glob("anime_ranking_*simple.csv"))

        if csv_files:
            latest_file = max(csv_files, key=lambda x: x.stat().st_mtime)
            logger.warning(f"æœªæ‰¾åˆ°JSONæ–‡ä»¶ï¼Œä½¿ç”¨CSVæ–‡ä»¶: {latest_file}")
            logger.warning("æ³¨æ„: CSVæ–‡ä»¶åŠŸèƒ½æœ‰é™ï¼Œå»ºè®®å…ˆè¿è¡Œå®Œæ•´åˆ†æç”ŸæˆJSONæ–‡ä»¶")
            return latest_file

        logger.error("æœªæ‰¾åˆ°åˆ†æç»“æœæ–‡ä»¶ (JSONæˆ–CSV)")
        return None
    
    def load_analysis_results(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """åŠ è½½åˆ†æç»“æœ"""
        try:
            if file_path.suffix.lower() == '.json':
                return self._load_json_results(file_path)
            elif file_path.suffix.lower() == '.csv':
                return self._load_csv_results(file_path)
            else:
                logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
                return None

        except Exception as e:
            logger.error(f"åŠ è½½åˆ†æç»“æœå¤±è´¥: {e}")
            return None

    def _load_json_results(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """åŠ è½½JSONæ ¼å¼çš„åˆ†æç»“æœ"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        logger.info(f"æˆåŠŸåŠ è½½JSONæ–‡ä»¶: {len(data.get('rankings', []))} æ¡åŠ¨æ¼«æ•°æ®")
        return data

    def _load_csv_results(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """åŠ è½½CSVæ ¼å¼çš„åˆ†æç»“æœå¹¶è½¬æ¢ä¸ºJSONæ ¼å¼"""
        logger.info("æ­£åœ¨ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®...")

        rankings = []

        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)

            for row in reader:
                try:
                    # åŸºæœ¬ä¿¡æ¯
                    anime = {
                        'rank': int(row.get('æ’å', 0)),
                        'title': row.get('ä¸­æ–‡å', ''),
                        'title_english': row.get('æ—¥æ–‡å', ''),
                        'composite_score': float(row.get('ç»¼åˆè¯„åˆ†', 0)),
                        'ratings': []
                    }

                    # è§£æå„ç½‘ç«™è¯„åˆ†æ•°æ®
                    websites = ['ANILIST', 'FILMARKS', 'IMDB', 'MAL', 'DOUBAN']

                    for website in websites:
                        score_key = f"{website}_è¯„åˆ†"
                        votes_key = f"{website}_æŠ•ç¥¨æ•°"
                        rank_key = f"{website}_æ’å"

                        if score_key in row and row[score_key]:
                            try:
                                rating = {
                                    'website': website.lower(),
                                    'raw_score': float(row[score_key]),
                                    'vote_count': int(row.get(votes_key, 0)) if row.get(votes_key) else 0,
                                    'site_rank': int(row[rank_key]) if row.get(rank_key) else None,
                                    'bayesian_score': float(row[score_key]),
                                    'z_score': 0.0,
                                    'weight': 1.0,
                                    'site_percentile': None,
                                    'score_distribution': {},
                                    'last_updated': datetime.now().isoformat(),
                                    'url': ''
                                }
                                anime['ratings'].append(rating)
                            except (ValueError, TypeError):
                                continue

                    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                    anime['website_count'] = len(anime['ratings'])
                    anime['total_votes'] = sum(r.get('vote_count', 0) for r in anime['ratings'])
                    anime['percentile'] = 0.0  # ä¼šé‡æ–°è®¡ç®—

                    rankings.append(anime)

                except Exception as e:
                    logger.warning(f"è§£æCSVè¡Œæ—¶å‡ºé”™: {e}")
                    continue

        # æ„å»ºå®Œæ•´çš„æ•°æ®ç»“æ„
        data = {
            'rankings': rankings,
            'analysis_info': {
                'analyzed_anime_count': len(rankings),
                'timestamp': datetime.now().isoformat(),
                'source': 'csv_import'
            }
        }

        logger.info(f"æˆåŠŸä»CSVåŠ è½½ {len(rankings)} æ¡åŠ¨æ¼«æ•°æ®")
        return data
    
    def identify_missing_douban_data(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯†åˆ«ç¼ºå°‘è±†ç“£æ•°æ®çš„åŠ¨æ¼«"""
        missing_douban = []
        
        for anime in data.get('rankings', []):
            has_douban = False
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è±†ç“£è¯„åˆ†
            for rating in anime.get('ratings', []):
                if rating.get('website') == 'douban':
                    has_douban = True
                    break
            
            if not has_douban:
                missing_douban.append(anime)
        
        logger.info(f"å‘ç° {len(missing_douban)} ä¸ªåŠ¨æ¼«ç¼ºå°‘è±†ç“£æ•°æ®")
        return missing_douban
    
    async def search_douban_with_patience(self, session: aiohttp.ClientSession, 
                                        anime_title: str, max_attempts: int = 3) -> Optional[str]:
        """è€å¿ƒæœç´¢è±†ç“£ID"""
        logger.info(f"ğŸ” æœç´¢è±†ç“£æ•°æ®: {anime_title}")
        
        for attempt in range(max_attempts):
            try:
                # å°è¯•å¤šç§æœç´¢ç­–ç•¥
                strategies = [
                    lambda: self.douban_scraper.search_anime_alternative_sites(session, anime_title),
                    lambda: self.douban_scraper.search_anime_with_mobile_api(session, anime_title),
                    lambda: self.douban_scraper._try_original_search(session, anime_title)
                ]
                
                for i, strategy in enumerate(strategies):
                    logger.info(f"   å°è¯•ç­–ç•¥ {i+1}: {strategy.__name__ if hasattr(strategy, '__name__') else f'ç­–ç•¥{i+1}'}")
                    
                    try:
                        results = await strategy()
                        if results:
                            douban_id = results[0].external_ids.get(WebsiteName.DOUBAN)
                            if douban_id:
                                logger.info(f"   âœ… æ‰¾åˆ°è±†ç“£ID: {douban_id}")
                                return douban_id
                    except Exception as e:
                        logger.warning(f"   ç­–ç•¥ {i+1} å¤±è´¥: {e}")
                        continue
                
                # å¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´å†é‡è¯•
                if attempt < max_attempts - 1:
                    wait_time = 30 * (attempt + 1)  # 30ç§’, 60ç§’, 90ç§’
                    logger.info(f"   ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                
            except Exception as e:
                logger.error(f"æœç´¢è±†ç“£IDæ—¶å‡ºé”™: {e}")
                if attempt < max_attempts - 1:
                    await asyncio.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿ
        
        logger.warning(f"âŒ æ— æ³•æ‰¾åˆ°è±†ç“£æ•°æ®: {anime_title}")
        return None
    
    async def get_douban_rating_with_patience(self, session: aiohttp.ClientSession, 
                                            douban_id: str) -> Optional[RatingData]:
        """è€å¿ƒè·å–è±†ç“£è¯„åˆ†"""
        logger.info(f"ğŸ“Š è·å–è±†ç“£è¯„åˆ†: {douban_id}")
        
        max_attempts = 5
        for attempt in range(max_attempts):
            try:
                rating = await self.douban_scraper.get_anime_rating(session, douban_id)
                if rating:
                    logger.info(f"   âœ… è·å–æˆåŠŸ: {rating.raw_score} ({rating.vote_count} ç¥¨)")
                    return rating
                else:
                    logger.warning(f"   å°è¯• {attempt+1}/{max_attempts} å¤±è´¥")
                    
            except Exception as e:
                logger.error(f"è·å–è±†ç“£è¯„åˆ†æ—¶å‡ºé”™: {e}")
            
            # é€æ¸å¢åŠ ç­‰å¾…æ—¶é—´
            if attempt < max_attempts - 1:
                wait_time = 20 * (attempt + 1)  # 20ç§’, 40ç§’, 60ç§’, 80ç§’
                logger.info(f"   ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                await asyncio.sleep(wait_time)
        
        logger.error(f"âŒ æ— æ³•è·å–è±†ç“£è¯„åˆ†: {douban_id}")
        return None
    
    async def process_douban_data(self, data: Dict[str, Any], 
                                max_anime: Optional[int] = None) -> Dict[str, Any]:
        """å¤„ç†è±†ç“£æ•°æ®è¡¥å…¨"""
        missing_douban = self.identify_missing_douban_data(data)
        
        if not missing_douban:
            logger.info("æ‰€æœ‰åŠ¨æ¼«éƒ½å·²æœ‰è±†ç“£æ•°æ®")
            return data
        
        # é™åˆ¶å¤„ç†æ•°é‡
        if max_anime:
            missing_douban = missing_douban[:max_anime]
            logger.info(f"é™åˆ¶å¤„ç†æ•°é‡ä¸º {max_anime} ä¸ªåŠ¨æ¼«")
        
        successful_additions = 0
        
        async with aiohttp.ClientSession() as session:
            logger.info("ğŸš€ å¼€å§‹è±†ç“£æ•°æ®è¡¥å…¨...")
            
            for i, anime in enumerate(missing_douban, 1):
                anime_title = anime.get('title', 'æœªçŸ¥')
                logger.info(f"\nğŸ“º å¤„ç† {i}/{len(missing_douban)}: {anime_title}")
                
                try:
                    # æœç´¢è±†ç“£ID
                    douban_id = await self.search_douban_with_patience(session, anime_title)
                    
                    if douban_id:
                        # è·å–è¯„åˆ†æ•°æ®
                        rating_data = await self.get_douban_rating_with_patience(session, douban_id)
                        
                        if rating_data:
                            # æ·»åŠ åˆ°åŠ¨æ¼«æ•°æ®ä¸­
                            if 'ratings' not in anime:
                                anime['ratings'] = []
                            
                            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                            rating_dict = {
                                'website': 'douban',
                                'raw_score': rating_data.raw_score,
                                'vote_count': rating_data.vote_count,
                                'score_distribution': rating_data.score_distribution or {},
                                'bayesian_score': rating_data.raw_score,  # ä¸´æ—¶å€¼ï¼Œä¼šé‡æ–°è®¡ç®—
                                'z_score': 0.0,  # ä¼šé‡æ–°è®¡ç®—
                                'weight': 1.0,   # ä¼šé‡æ–°è®¡ç®—
                                'site_rank': None,  # ä¼šé‡æ–°è®¡ç®—
                                'site_percentile': None,
                                'last_updated': datetime.now().isoformat(),
                                'url': rating_data.url
                            }
                            
                            anime['ratings'].append(rating_dict)
                            anime['website_count'] = len(anime['ratings'])
                            anime['total_votes'] = sum(r.get('vote_count', 0) for r in anime['ratings'])
                            
                            successful_additions += 1
                            logger.info(f"   âœ… æˆåŠŸæ·»åŠ è±†ç“£æ•°æ®")
                        
                    # åœ¨æ¯ä¸ªåŠ¨æ¼«ä¹‹é—´æ·»åŠ é•¿æ—¶é—´å»¶è¿Ÿ
                    if i < len(missing_douban):
                        wait_time = 45  # 45ç§’é—´éš”
                        logger.info(f"â³ ç­‰å¾… {wait_time} ç§’...")
                        await asyncio.sleep(wait_time)
                        
                except Exception as e:
                    logger.error(f"å¤„ç†åŠ¨æ¼« {anime_title} æ—¶å‡ºé”™: {e}")
                    continue
        
        logger.info(f"ğŸ‰ è±†ç“£æ•°æ®è¡¥å…¨å®Œæˆ: æˆåŠŸæ·»åŠ  {successful_additions} æ¡æ•°æ®")
        
        # é‡æ–°è®¡ç®—è¯„åˆ†å’Œæ’å
        if successful_additions > 0:
            logger.info("ğŸ”„ é‡æ–°è®¡ç®—è¯„åˆ†å’Œæ’å...")
            self.recalculate_scores_and_rankings(data)
        
        return data
    
    def recalculate_scores_and_rankings(self, data: Dict[str, Any]):
        """é‡æ–°è®¡ç®—è¯„åˆ†å’Œæ’å"""
        try:
            # é‡æ–°è®¡ç®—ç»¼åˆè¯„åˆ†
            for anime in data['rankings']:
                if anime.get('ratings'):
                    # ä½¿ç”¨è¯„åˆ†å¼•æ“é‡æ–°è®¡ç®—
                    ratings = []
                    for rating_dict in anime['ratings']:
                        rating_data = RatingData(
                            website=WebsiteName(rating_dict['website']),
                            raw_score=rating_dict['raw_score'],
                            vote_count=rating_dict['vote_count'],
                            score_distribution=rating_dict.get('score_distribution', {}),
                            last_updated=datetime.now(),
                            url=rating_dict.get('url', '')
                        )
                        ratings.append(rating_data)
                    
                    # é‡æ–°è®¡ç®—ç»¼åˆè¯„åˆ†
                    composite_score = self.scoring_engine.calculate_composite_score(ratings)
                    anime['composite_score'] = composite_score
            
            # é‡æ–°æ’åº
            data['rankings'].sort(key=lambda x: x.get('composite_score', 0), reverse=True)
            
            # é‡æ–°åˆ†é…æ’å
            for i, anime in enumerate(data['rankings'], 1):
                anime['rank'] = i
                anime['percentile'] = (len(data['rankings']) - i + 1) / len(data['rankings']) * 100
            
            # é‡æ–°è®¡ç®—ç½‘ç«™æ’å
            self.recalculate_site_rankings(data)
            
            logger.info("âœ… è¯„åˆ†å’Œæ’åé‡æ–°è®¡ç®—å®Œæˆ")
            
        except Exception as e:
            logger.error(f"é‡æ–°è®¡ç®—è¯„åˆ†æ—¶å‡ºé”™: {e}")
    
    def recalculate_site_rankings(self, data: Dict[str, Any]):
        """é‡æ–°è®¡ç®—ç½‘ç«™æ’å"""
        from collections import defaultdict
        
        # æŒ‰ç½‘ç«™åˆ†ç»„æ”¶é›†è¯„åˆ†æ•°æ®
        website_anime_scores = defaultdict(list)
        
        for anime in data['rankings']:
            for rating in anime.get('ratings', []):
                if rating.get('raw_score') is not None:
                    website_anime_scores[rating['website']].append({
                        'anime': anime,
                        'rating': rating,
                        'score': rating['raw_score']
                    })
        
        # ä¸ºæ¯ä¸ªç½‘ç«™è®¡ç®—æ’å
        for website, anime_ratings in website_anime_scores.items():
            if len(anime_ratings) < 2:
                continue
            
            # æŒ‰è¯„åˆ†é™åºæ’åº
            sorted_ratings = sorted(anime_ratings, key=lambda x: x['score'], reverse=True)
            total_count = len(sorted_ratings)
            
            # åˆ†é…æ’å
            for i, item in enumerate(sorted_ratings, 1):
                rank = i
                percentile = (total_count - rank + 1) / total_count * 100
                
                # æ›´æ–°è¯„åˆ†æ•°æ®ä¸­çš„æ’åä¿¡æ¯
                item['rating']['site_rank'] = rank
                item['rating']['site_percentile'] = percentile
    
    def save_updated_results(self, data: Dict[str, Any], original_file: Path) -> List[Path]:
        """ä¿å­˜æ›´æ–°åçš„ç»“æœ"""
        saved_files = []
        
        # ç”Ÿæˆæ–°çš„æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = original_file.stem.replace("_simple", "").replace("_detailed", "")
        
        # ä¿å­˜JSON
        json_file = original_file.parent / f"{base_name}_douban_{timestamp}.json"
        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            saved_files.append(json_file)
            logger.info(f"âœ… ä¿å­˜JSON: {json_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜JSONå¤±è´¥: {e}")
        
        # ä¿å­˜ç®€åŒ–CSV
        csv_file = original_file.parent / f"{base_name}_douban_{timestamp}_simple.csv"
        try:
            self.save_simple_csv(data, csv_file)
            saved_files.append(csv_file)
            logger.info(f"âœ… ä¿å­˜CSV: {csv_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜CSVå¤±è´¥: {e}")
        
        return saved_files

    def save_simple_csv(self, data: Dict[str, Any], csv_path: Path):
        """ä¿å­˜ç®€åŒ–ç‰ˆCSV"""
        # è·å–æ‰€æœ‰å¯ç”¨çš„ç½‘ç«™
        enabled_websites = set()
        for anime in data['rankings']:
            for rating in anime.get('ratings', []):
                enabled_websites.add(rating['website'])

        enabled_websites = sorted(list(enabled_websites))

        with open(csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)

            # æ„å»ºè¡¨å¤´
            headers = ['æ’å', 'ä¸­æ–‡å', 'æ—¥æ–‡å', 'ç»¼åˆè¯„åˆ†']

            # æ·»åŠ å„ç½‘ç«™çš„è¯„åˆ†ã€æŠ•ç¥¨æ•°å’Œæ’ååˆ—
            for website in enabled_websites:
                headers.extend([
                    f"{website.upper()}_è¯„åˆ†",
                    f"{website.upper()}_æŠ•ç¥¨æ•°",
                    f"{website.upper()}_æ’å"
                ])

            writer.writerow(headers)

            # å†™å…¥æ•°æ®è¡Œ
            for anime in data['rankings']:
                row = [
                    anime['rank'],
                    anime['title'],
                    anime.get('title_english', anime['title']),
                    anime['composite_score']
                ]

                # åˆ›å»ºç½‘ç«™è¯„åˆ†å­—å…¸
                website_ratings = {}
                for rating in anime.get('ratings', []):
                    website_ratings[rating['website']] = {
                        'score': rating.get('raw_score'),
                        'votes': rating.get('vote_count', 0),
                        'rank': rating.get('site_rank', '')
                    }

                # æ·»åŠ å„ç½‘ç«™çš„è¯„åˆ†ã€æŠ•ç¥¨æ•°å’Œæ’å
                for website in enabled_websites:
                    if website in website_ratings:
                        row.extend([
                            website_ratings[website]['score'],
                            website_ratings[website]['votes'],
                            website_ratings[website]['rank']
                        ])
                    else:
                        row.extend(["", "", ""])

                writer.writerow(row)


@click.command()
@click.option('--config', '-c', default='config/config.yaml',
              help='é…ç½®æ–‡ä»¶è·¯å¾„')
@click.option('--input', '-i', default=None,
              help='è¾“å…¥çš„åˆ†æç»“æœæ–‡ä»¶è·¯å¾„ (é»˜è®¤: è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ–‡ä»¶)')
@click.option('--max-anime', '-m', type=int, default=None,
              help='æœ€å¤§å¤„ç†åŠ¨æ¼«æ•°é‡ (ç”¨äºæµ‹è¯•)')
@click.option('--verbose', '-v', is_flag=True,
              help='å¯ç”¨è¯¦ç»†æ—¥å¿—')
def main(config, input, max_anime, verbose):
    """è±†ç“£æ•°æ®åå¤„ç†å™¨ - ä¸ºç°æœ‰åˆ†æç»“æœè¡¥å……è±†ç“£æ•°æ®"""

    # è®¾ç½®æ—¥å¿—
    logger.remove()
    level = "DEBUG" if verbose else "INFO"
    logger.add(
        sys.stdout,
        level=level,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>"
    )

    logger.info("ğŸ­ è±†ç“£æ•°æ®åå¤„ç†å™¨å¯åŠ¨")
    logger.info("=" * 60)

    try:
        # åŠ è½½é…ç½®
        app_config = Config.load_from_file(config)

        # åˆ›å»ºåå¤„ç†å™¨
        processor = DoubanPostProcessor(app_config)

        # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
        if input:
            input_file = Path(input)
            if not input_file.exists():
                logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
                return
        else:
            results_dir = Path(app_config.storage.results_dir)
            input_file = processor.find_latest_results(results_dir)
            if not input_file:
                return

        # åŠ è½½åˆ†æç»“æœ
        data = processor.load_analysis_results(input_file)
        if not data:
            return

        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
        total_anime = len(data.get('rankings', []))
        missing_douban = processor.identify_missing_douban_data(data)

        logger.info(f"ğŸ“Š å½“å‰çŠ¶æ€:")
        logger.info(f"   æ€»åŠ¨æ¼«æ•°: {total_anime}")
        logger.info(f"   ç¼ºå°‘è±†ç“£æ•°æ®: {len(missing_douban)}")

        if not missing_douban:
            logger.info("ğŸ‰ æ‰€æœ‰åŠ¨æ¼«éƒ½å·²æœ‰è±†ç“£æ•°æ®ï¼Œæ— éœ€å¤„ç†")
            return

        # ç¡®è®¤å¤„ç†
        if max_anime:
            process_count = min(max_anime, len(missing_douban))
        else:
            process_count = len(missing_douban)

        logger.info(f"âš ï¸ å³å°†å¤„ç† {process_count} ä¸ªåŠ¨æ¼«çš„è±†ç“£æ•°æ®")
        logger.info(f"â±ï¸ é¢„è®¡è€—æ—¶: {process_count * 1.5:.1f} åˆ†é’Ÿ (æ¯ä¸ªåŠ¨æ¼«çº¦1.5åˆ†é’Ÿ)")

        if not max_anime:  # åªæœ‰åœ¨éæµ‹è¯•æ¨¡å¼ä¸‹æ‰è¯¢é—®ç¡®è®¤
            confirm = input("\næ˜¯å¦ç»§ç»­? (y/N): ").strip().lower()
            if confirm != 'y':
                logger.info("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
                return

        # å¼€å§‹å¤„ç†
        logger.info(f"\nğŸš€ å¼€å§‹è±†ç“£æ•°æ®è¡¥å…¨...")

        # è¿è¡Œå¼‚æ­¥å¤„ç†
        updated_data = asyncio.run(processor.process_douban_data(data, max_anime))

        # ä¿å­˜ç»“æœ
        logger.info(f"\nğŸ’¾ ä¿å­˜æ›´æ–°åçš„ç»“æœ...")
        saved_files = processor.save_updated_results(updated_data, input_file)

        logger.info(f"\nğŸ‰ è±†ç“£æ•°æ®åå¤„ç†å®Œæˆ!")
        logger.info(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶:")
        for file_path in saved_files:
            logger.info(f"   - {file_path}")

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        final_missing = processor.identify_missing_douban_data(updated_data)
        added_count = len(missing_douban) - len(final_missing)

        logger.info(f"\nğŸ“ˆ å¤„ç†ç»Ÿè®¡:")
        logger.info(f"   æˆåŠŸæ·»åŠ è±†ç“£æ•°æ®: {added_count}")
        logger.info(f"   ä»ç¼ºå°‘è±†ç“£æ•°æ®: {len(final_missing)}")
        logger.info(f"   æˆåŠŸç‡: {added_count/len(missing_douban)*100:.1f}%")

    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        logger.error(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
