#!/usr/bin/env python3
"""
URLæå–è„šæœ¬
ä»JSONç»“æœæ–‡ä»¶ä¸­æå–æ‰€æœ‰è¯„åˆ†ç½‘ç«™çš„URLï¼Œè¾“å‡ºåˆ°txtæ–‡ä»¶ä¸­ä¾¿äºæ‰‹åŠ¨æ£€æŸ¥
"""
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set
import click
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.config import Config

class URLExtractor:
    """URLæå–å™¨"""
    
    def __init__(self, config: Config):
        self.config = config
    
    def find_latest_results_file(self) -> str:
        """æŸ¥æ‰¾æœ€æ–°çš„ç»“æœæ–‡ä»¶ï¼ˆä¼˜å…ˆä» final_results ç›®å½•ï¼‰"""
        # é¦–å…ˆæ£€æŸ¥ final_results ç›®å½•
        final_results_dir = Path(self.config.storage.final_results_dir)
        if final_results_dir.exists():
            json_files = list(final_results_dir.glob("anime_ranking_*.json"))
            if json_files:
                latest_file = max(json_files, key=lambda x: x.stat().st_mtime)
                logger.info(f"ğŸ“‚ è‡ªåŠ¨é€‰æ‹© final_results ä¸­çš„æœ€æ–°æ–‡ä»¶: {latest_file.name}")
                return str(latest_file)
        
        # å¦‚æœ final_results æ²¡æœ‰æ–‡ä»¶ï¼Œåˆ™ä»æ™®é€š results ç›®å½•æŸ¥æ‰¾
        results_dir = Path(self.config.storage.results_dir)
        if not results_dir.exists():
            raise FileNotFoundError("ç»“æœç›®å½•ä¸å­˜åœ¨")
        
        json_files = list(results_dir.glob("anime_ranking_*.json"))
        if not json_files:
            raise FileNotFoundError("æ²¡æœ‰æ‰¾åˆ°åˆ†æç»“æœæ–‡ä»¶")
        
        latest_file = max(json_files, key=lambda x: x.stat().st_mtime)
        logger.info(f"ğŸ“‚ è‡ªåŠ¨é€‰æ‹© results ä¸­çš„æœ€æ–°æ–‡ä»¶: {latest_file.name}")
        return str(latest_file)
    
    def extract_urls_from_json(self, file_path: str) -> Dict[str, List[Dict]]:
        """ä»JSONæ–‡ä»¶ä¸­æå–æ‰€æœ‰URL"""
        logger.info(f"ğŸ“‚ åŠ è½½JSONæ–‡ä»¶: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æŒ‰ç½‘ç«™åˆ†ç±»å­˜å‚¨URLä¿¡æ¯
        urls_by_website = {}
        total_urls = 0
        
        for ranking in data.get('rankings', []):
            anime_title = ranking.get('title', 'æœªçŸ¥åŠ¨æ¼«')
            
            for rating in ranking.get('ratings', []):
                website = rating.get('website', 'æœªçŸ¥ç½‘ç«™')
                url = rating.get('url', '')
                raw_score = rating.get('raw_score', 0)
                vote_count = rating.get('vote_count', 0)
                
                if url:  # åªå¤„ç†æœ‰URLçš„è¯„åˆ†
                    if website not in urls_by_website:
                        urls_by_website[website] = []
                    
                    urls_by_website[website].append({
                        'anime_title': anime_title,
                        'url': url,
                        'raw_score': raw_score,
                        'vote_count': vote_count
                    })
                    total_urls += 1
        
        logger.info(f"âœ… æˆåŠŸæå– {total_urls} ä¸ªURLï¼Œæ¶µç›– {len(urls_by_website)} ä¸ªç½‘ç«™")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        for website, urls in urls_by_website.items():
            logger.info(f"   {website}: {len(urls)} ä¸ªURL")
        
        return urls_by_website
    
    def save_urls_to_txt(self, urls_by_website: Dict[str, List[Dict]], output_path: str, format_type: str = "grouped"):
        """ä¿å­˜URLåˆ°txtæ–‡ä»¶"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            # å†™å…¥æ–‡ä»¶å¤´
            f.write("=" * 80 + "\n")
            f.write("åŠ¨æ¼«è¯„åˆ†ç½‘ç«™URLåˆ—è¡¨\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")
            
            if format_type == "grouped":
                self._write_grouped_format(f, urls_by_website)
            elif format_type == "simple":
                self._write_simple_format(f, urls_by_website)
            elif format_type == "detailed":
                self._write_detailed_format(f, urls_by_website)
            
            # å†™å…¥ç»Ÿè®¡ä¿¡æ¯
            total_urls = sum(len(urls) for urls in urls_by_website.values())
            f.write("\n" + "=" * 80 + "\n")
            f.write("ç»Ÿè®¡ä¿¡æ¯:\n")
            f.write(f"æ€»URLæ•°: {total_urls}\n")
            f.write(f"ç½‘ç«™æ•°: {len(urls_by_website)}\n")
            for website, urls in sorted(urls_by_website.items()):
                f.write(f"  {website}: {len(urls)} ä¸ª\n")
        
        logger.success(f"âœ… URLåˆ—è¡¨å·²ä¿å­˜åˆ°: {output_file}")
    
    def _write_grouped_format(self, f, urls_by_website: Dict[str, List[Dict]]):
        """æŒ‰ç½‘ç«™åˆ†ç»„çš„æ ¼å¼"""
        for website, urls in sorted(urls_by_website.items()):
            f.write(f"\n{'='*20} {website.upper()} ({'='*20}\n")
            f.write(f"å…± {len(urls)} ä¸ªURL\n\n")
            
            for i, url_info in enumerate(urls, 1):
                f.write(f"{i:3d}. {url_info['anime_title']}\n")
                f.write(f"     è¯„åˆ†: {url_info['raw_score']} ({url_info['vote_count']:,} ç¥¨)\n")
                f.write(f"     URL: {url_info['url']}\n\n")
    
    def _write_simple_format(self, f, urls_by_website: Dict[str, List[Dict]]):
        """ç®€å•æ ¼å¼ï¼ˆåªæœ‰URLï¼‰"""
        f.write("æ‰€æœ‰URLåˆ—è¡¨:\n\n")
        
        url_count = 1
        for website, urls in sorted(urls_by_website.items()):
            for url_info in urls:
                f.write(f"{url_count:3d}. {url_info['url']}\n")
                url_count += 1
    
    def _write_detailed_format(self, f, urls_by_website: Dict[str, List[Dict]]):
        """è¯¦ç»†æ ¼å¼ï¼ˆåŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼‰"""
        url_count = 1
        for website, urls in sorted(urls_by_website.items()):
            for url_info in urls:
                f.write(f"{url_count:3d}. ã€{website.upper()}ã€‘{url_info['anime_title']}\n")
                f.write(f"     è¯„åˆ†: {url_info['raw_score']} | æŠ•ç¥¨: {url_info['vote_count']:,} | ç½‘ç«™: {website}\n")
                f.write(f"     URL: {url_info['url']}\n")
                f.write(f"     æ£€æŸ¥: [ ] æ­£ç¡® [ ] é”™è¯¯ [ ] éœ€è¦ä¿®æ­£\n\n")
                url_count += 1
    
    def save_urls_by_website(self, urls_by_website: Dict[str, List[Dict]], output_dir: str):
        """ä¸ºæ¯ä¸ªç½‘ç«™å•ç‹¬ä¿å­˜URLæ–‡ä»¶"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for website, urls in urls_by_website.items():
            website_file = output_path / f"urls_{website}_{timestamp}.txt"
            
            with open(website_file, 'w', encoding='utf-8') as f:
                f.write(f"{website.upper()} ç½‘ç«™URLåˆ—è¡¨\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"å…± {len(urls)} ä¸ªURL\n")
                f.write("=" * 60 + "\n\n")
                
                for i, url_info in enumerate(urls, 1):
                    f.write(f"{i:3d}. {url_info['anime_title']}\n")
                    f.write(f"     è¯„åˆ†: {url_info['raw_score']} ({url_info['vote_count']:,} ç¥¨)\n")
                    f.write(f"     URL: {url_info['url']}\n")
                    f.write(f"     æ£€æŸ¥: [ ] æ­£ç¡® [ ] é”™è¯¯\n\n")
            
            logger.info(f"âœ… {website} URLåˆ—è¡¨å·²ä¿å­˜åˆ°: {website_file}")
    
    def create_url_validation_checklist(self, urls_by_website: Dict[str, List[Dict]], output_path: str):
        """åˆ›å»ºURLéªŒè¯æ£€æŸ¥æ¸…å•"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("åŠ¨æ¼«è¯„åˆ†ç½‘ç«™URLéªŒè¯æ£€æŸ¥æ¸…å•\n")
            f.write("=" * 50 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("ä½¿ç”¨è¯´æ˜: è®¿é—®æ¯ä¸ªURLï¼Œæ£€æŸ¥æ˜¯å¦æŒ‡å‘æ­£ç¡®çš„åŠ¨æ¼«é¡µé¢\n")
            f.write("æ ‡è®°: âœ“ = æ­£ç¡®, âœ— = é”™è¯¯, ? = éœ€è¦è¿›ä¸€æ­¥ç¡®è®¤\n")
            f.write("=" * 50 + "\n\n")
            
            url_count = 1
            for website, urls in sorted(urls_by_website.items()):
                f.write(f"\nã€{website.upper()}ã€‘ç½‘ç«™ ({len(urls)} ä¸ªURL)\n")
                f.write("-" * 40 + "\n")
                
                for url_info in urls:
                    f.write(f"{url_count:3d}. [ ] {url_info['anime_title']}\n")
                    f.write(f"        è¯„åˆ†: {url_info['raw_score']} | æŠ•ç¥¨: {url_info['vote_count']:,}\n")
                    f.write(f"        URL: {url_info['url']}\n")
                    f.write(f"        å¤‡æ³¨: ________________________\n\n")
                    url_count += 1
        
        logger.success(f"âœ… URLéªŒè¯æ£€æŸ¥æ¸…å•å·²ä¿å­˜åˆ°: {output_file}")


@click.command()
@click.option('--config', '-c', default='config/config.yaml',
              help='é…ç½®æ–‡ä»¶è·¯å¾„')
@click.option('--input', '-i', default=None,
              help='è¾“å…¥çš„JSONæ–‡ä»¶è·¯å¾„ (é»˜è®¤: è‡ªåŠ¨é€‰æ‹©æœ€æ–°æ–‡ä»¶)')
@click.option('--output', '-o', default=None,
              help='è¾“å‡ºç›®å½• (é»˜è®¤: final_resultsç›®å½•)')
@click.option('--format', '-f', 'format_type', 
              type=click.Choice(['grouped', 'simple', 'detailed']), 
              default='grouped',
              help='è¾“å‡ºæ ¼å¼: grouped=æŒ‰ç½‘ç«™åˆ†ç»„, simple=åªæœ‰URL, detailed=è¯¦ç»†ä¿¡æ¯')
@click.option('--separate', '-s', is_flag=True,
              help='ä¸ºæ¯ä¸ªç½‘ç«™å•ç‹¬ç”Ÿæˆæ–‡ä»¶')
@click.option('--checklist', is_flag=True,
              help='ç”ŸæˆURLéªŒè¯æ£€æŸ¥æ¸…å•')
@click.option('--verbose', '-v', is_flag=True,
              help='å¯ç”¨è¯¦ç»†æ—¥å¿—')
def main(config, input, output, format_type, separate, checklist, verbose):
    """URLæå–ç¨‹åº - ä»JSONç»“æœæ–‡ä»¶ä¸­æå–æ‰€æœ‰è¯„åˆ†ç½‘ç«™çš„URL"""
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if verbose:
        logger.remove()
        logger.add(sys.stderr, level="DEBUG")
    
    logger.info("ğŸ”— å¯åŠ¨URLæå–ç¨‹åº")
    
    try:
        # 1. åŠ è½½é…ç½®
        app_config = Config.load_from_file(config)
        logger.info(f"âœ… é…ç½®åŠ è½½æˆåŠŸ: {config}")
        
        # 2. åˆ›å»ºURLæå–å™¨
        extractor = URLExtractor(app_config)
        
        # 3. ç¡®å®šè¾“å…¥æ–‡ä»¶
        if input is None:
            logger.info("ğŸ” æœªæŒ‡å®šè¾“å…¥æ–‡ä»¶ï¼Œè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°ç»“æœ...")
            input_file = extractor.find_latest_results_file()
        else:
            input_file = input
            logger.info(f"ğŸ“‚ ä½¿ç”¨æŒ‡å®šçš„è¾“å…¥æ–‡ä»¶: {input_file}")
        
        # 4. æå–URL
        urls_by_website = extractor.extract_urls_from_json(input_file)
        
        if not urls_by_website:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•URL")
            return
        
        # 5. ç¡®å®šè¾“å‡ºç›®å½•
        output_dir = output or app_config.storage.final_results_dir
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # 6. ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        input_filename = Path(input_file).stem
        
        # 7. ä¿å­˜URLåˆ—è¡¨
        if separate:
            # ä¸ºæ¯ä¸ªç½‘ç«™å•ç‹¬ç”Ÿæˆæ–‡ä»¶
            extractor.save_urls_by_website(urls_by_website, output_dir)
        else:
            # ç”Ÿæˆç»Ÿä¸€çš„URLåˆ—è¡¨
            output_file = output_path / f"urls_list_{format_type}_{timestamp}.txt"
            extractor.save_urls_to_txt(urls_by_website, str(output_file), format_type)
        
        # 8. ç”Ÿæˆæ£€æŸ¥æ¸…å•
        if checklist:
            checklist_file = output_path / f"url_validation_checklist_{timestamp}.txt"
            extractor.create_url_validation_checklist(urls_by_website, str(checklist_file))
        
        logger.success("ğŸ‰ URLæå–å®Œæˆï¼")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        # æ˜¾ç¤ºä½¿ç”¨å»ºè®®
        logger.info("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        logger.info("   â€¢ ä½¿ç”¨æµè§ˆå™¨æ‰“å¼€URLåˆ—è¡¨æ–‡ä»¶")
        logger.info("   â€¢ é€ä¸ªè®¿é—®URLæ£€æŸ¥é¡µé¢æ­£ç¡®æ€§")
        logger.info("   â€¢ è®°å½•éœ€è¦ä¿®æ­£çš„URL")
        logger.info("   â€¢ ä½¿ç”¨ manual_data_correction.py ä¿®æ­£é”™è¯¯æ•°æ®")
        
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
